[
  {
    "objectID": "projects/steel-nylon-classifier.html",
    "href": "projects/steel-nylon-classifier.html",
    "title": "Simple MLP model for Guitar Steel vs Nylon strings classification",
    "section": "",
    "text": "Can a 2-hidden-layer MLP do a good job classifying musical instrument sounds? Letâ€™s find out!"
  },
  {
    "objectID": "projects/steel-nylon-classifier.html#download-audio-from-youtube",
    "href": "projects/steel-nylon-classifier.html#download-audio-from-youtube",
    "title": "Simple MLP model for Guitar Steel vs Nylon strings classification",
    "section": "4.1 Download audio from YouTube",
    "text": "4.1 Download audio from YouTube\nThese are the clips that I handpicked from YouTube. They are solo guitar recordings and were recorded in a professional studio. To watch any of them, just add the youtube url prefix. For example: â€œfoIPN-T7RGoâ€ â¡ï¸ â€œyoutube.com/watch?v=foIPN-T7RGoâ€\n\nsteel_clips = [\"foIPN-T7RGo\",\"10ATKnZLg9c\",\"IP8vBL5Q8Ac\"]\nnylon_clips = [\"qgb-bdEEI-M\",\"qXwvz-nTiog\",\"6jQ34uTmA9s\"]\n\nNext, we define our function to download and extract audio from a YouTube url:\n\nfrom pytube import YouTube\n\ndef download_youtube_mp3(link, output_dir):\n    \"\"\"\n    Download and extract audio from a clip from youtube \n    \"\"\"\n    yt=YouTube(f\"youtube.com/watch?v={link}\")\n    t=yt.streams.filter(only_audio=True).first().download(output_dir, link + \".mp3\")\n    print(f\"Downloaded YouTube Audio from: {link}\")\n\nEach clip is over 60 minutes long, which could take a long time to download. To accelerate, we will create a downloading thread for each clip and download all clips simultaneously.\n\ndownload_thread_list = []\n\nfor link in steel_clips:\n  new_thread = threading.Thread(target=download_youtube_mp3, args=(link, RAW_CLIP_PATH + \"steel\"))\n  download_thread_list.append(new_thread)\n\nfor link in nylon_clips:\n  new_thread = threading.Thread(target=download_youtube_mp3, args=(link, RAW_CLIP_PATH + \"nylon\"))\n  download_thread_list.append(new_thread)\n\n\nprint(\"Download Raw Clips starting...\")\n# start each thread\nfor thread in download_thread_list:\n  thread.start()\n\n# wait for all to finish\nfor thread in download_thread_list:\n  thread.join()\n\n# successfully excecuted\nprint(\"Download Raw Clips finished!\")\n\nDownload Raw Clips starting...\nDownloaded YouTube Audio from: foIPN-T7RGo\nDownloaded YouTube Audio from: 6jQ34uTmA9s\nDownloaded YouTube Audio from: qgb-bdEEI-M\nDownloaded YouTube Audio from: qXwvz-nTiog\nDownloaded YouTube Audio from: 10ATKnZLg9c\nDownloaded YouTube Audio from: IP8vBL5Q8Ac\nDownload Raw Clips finished!"
  },
  {
    "objectID": "projects/steel-nylon-classifier.html#segmentize-into-5-second-clips",
    "href": "projects/steel-nylon-classifier.html#segmentize-into-5-second-clips",
    "title": "Simple MLP model for Guitar Steel vs Nylon strings classification",
    "section": "4.2 Segmentize into 5-second clips",
    "text": "4.2 Segmentize into 5-second clips\nNow, letâ€™s create some function to segment each audio clip into segments of 5 second long.\n\ndef segmentize_signal(signal, sr, dur):\n    \"\"\"\n    Segmentize the 1-d signal (mono) to a list of clips with custom duration (dur).\n    \"\"\"\n    seg_len = dur * sr\n\n    # calculate number of segments\n    no_segs = len(signal) // seg_len\n\n\n    # truncate input signal to have length divisiable by seg_len\n    trunc_len = int(no_segs * seg_len)\n\n    # split equally\n    return np.split(signal[:trunc_len], no_segs)\n\ndef save_audio(signal, sr, output_dir, filename):\n    output_path = os.path.join(output_dir, filename)\n    # torchaudio.save(output_path, signal, sr)\n    # print(output_path, sr)\n    sf.write(output_path, signal, sr)\n\ndef segment_audio_file(audio_path, output_dir,  target_sr=TARGET_SR, segment_duration=SEGMENT_DURATION):\n    print(f\"Processing raw clip: {audio_path}\")\n    signal, _ = librosa.load(audio_path, sr=target_sr, mono=True)\n    # signal, target_sr = librosa.load(audio_path,sr=None,  mono=True)\n    print(f\"\\tLoaded clip from disk\")\n    segments_list = segmentize_signal(signal, target_sr, segment_duration)\n    print(f\"\\tSegmented clip into {len(segments_list)} segments\")\n    for seg_idx, seg in enumerate(segments_list):\n        seg_name = f\"{audio_path.split('/')[-1][:-4]}_{seg_idx}.wav\"\n        save_audio(seg, target_sr, output_dir, seg_name)\n    print(f\"\\tSegments are saved completely\")\n\nNext, we use threading to segmentize all clips at the same time. Beware that if your system has less than 32GB of RAM, this could cause the system to freeze and run out of memory. In such case, please modify the code before do it sequentially (i.e.Â without threading)\n\nthread_list = []\n\nfor cls in CLASSES:\n# get all raw files from subfolders\n    raw_audio_paths = glob(f\"{RAW_CLIP_PATH}{cls}/*mp3\")\n    for audio_path in raw_audio_paths:\n        output_dir = f\"{SEGMENT_DIR}{cls}\"\n        new_thread = threading.Thread(target=segment_audio_file, args=(audio_path, output_dir))\n        thread_list.append(new_thread)\n        \nprint(\"Segmentation starting...\")\n# start each thread\nfor thread in thread_list:\n  thread.start()\n\n# wait for all to finish\nfor thread in thread_list:\n  thread.join()\n\n# successfully excecuted\nprint(\"Segmentation finished!\")\n\nSegmentation starting...\nProcessing raw clip: /workspace/data/raw/nylon/qXwvz-nTiog.mp3\nProcessing raw clip: /workspace/data/raw/nylon/6jQ34uTmA9s.mp3\nProcessing raw clip: /workspace/data/raw/nylon/qgb-bdEEI-M.mp3\nProcessing raw clip: /workspace/data/raw/steel/IP8vBL5Q8Ac.mp3\nProcessing raw clip: /workspace/data/raw/steel/foIPN-T7RGo.mp3\n\n\n/opt/conda/lib/python3.7/site-packages/librosa/util/decorators.py:88: UserWarning: PySoundFile failed. Trying audioread instead.\n  return f(*args, **kwargs)\n/opt/conda/lib/python3.7/site-packages/librosa/util/decorators.py:88: UserWarning: PySoundFile failed. Trying audioread instead.\n  return f(*args, **kwargs)\n\n\nProcessing raw clip: /workspace/data/raw/steel/10ATKnZLg9c.mp3\n\n\n/opt/conda/lib/python3.7/site-packages/librosa/util/decorators.py:88: UserWarning: PySoundFile failed. Trying audioread instead.\n  return f(*args, **kwargs)\n\n\n    Loaded clip from disk\n    Segmented clip into 648 segments\n    Segments are saved completely\n    Loaded clip from disk\n    Segmented clip into 742 segments\n    Segments are saved completely\n    Loaded clip from disk\n    Segmented clip into 1230 segments\n    Segments are saved completely\n    Loaded clip from disk\n    Segmented clip into 1251 segments\n    Segments are saved completely\n    Loaded clip from disk\n    Segmented clip into 1427 segments\n    Segments are saved completely\n    Loaded clip from disk\n    Segmented clip into 2647 segments\n    Segments are saved completely\nSegmentation finished!"
  },
  {
    "objectID": "projects/steel-nylon-classifier.html#create-annotations",
    "href": "projects/steel-nylon-classifier.html#create-annotations",
    "title": "Simple MLP model for Guitar Steel vs Nylon strings classification",
    "section": "5.1 Create annotations",
    "text": "5.1 Create annotations\nBefore creating our own dataset class, we need to have a csv file to describe our training / val / test sets.\nThis annotation dataframe stores the paths to each audio sample and its label:\n\nannotation_dict = {\"audio_path\": [], \"label\": []}\n\n\nfor label, cls in enumerate(CLASSES):\n  wav_dirs = f\"{SEGMENT_DIR}{cls}/*wav\"\n  audio_path_list = glob(wav_dirs)\n  count_audio_files = len(audio_path_list)\n  label_list = [label] * count_audio_files\n\n  annotation_dict[\"audio_path\"] += audio_path_list\n  annotation_dict[\"label\"]      += label_list\n\n\nannotation_df = pd.DataFrame.from_dict(annotation_dict)\nannotation_df.tail()\n\n\n\n\n\n  \n    \n      \n      audio_path\n      label\n    \n  \n  \n    \n      7940\n      ./data/segments/steel/foIPN-T7RGo_575.wav\n      1\n    \n    \n      7941\n      ./data/segments/steel/10ATKnZLg9c_545.wav\n      1\n    \n    \n      7942\n      ./data/segments/steel/10ATKnZLg9c_1035.wav\n      1\n    \n    \n      7943\n      ./data/segments/steel/10ATKnZLg9c_602.wav\n      1\n    \n    \n      7944\n      ./data/segments/steel/IP8vBL5Q8Ac_1146.wav\n      1\n    \n  \n\n\n\n\nThe data is quite enormously for an average system. Thatâ€™s why I seperated the training data set to full, half, quarter, and one eighth. This allows me to build and test model fast (by using a smaller training dataset). When I find something that works well, I can then use a larger training dataset to improve the training.\n\ntrain_df_full = annotation_df.sample(frac=TRAIN_SIZE, random_state=RANDOM_SEED)\nval_df = annotation_df.drop(train_df_full.index, axis=0)\n\n# make smaller train datasets for quick experimentations\ntrain_df_half = train_df_full.sample(frac=1/2, random_state=RANDOM_SEED)\ntrain_df_quarter = train_df_full.sample(frac=1/4, random_state=RANDOM_SEED)\ntrain_df_1eight = train_df_full.sample(frac=1/8, random_state=RANDOM_SEED)\n\nWe have 4816 samples of NYLON, and 3129 of STEEL\n\nannotation_df[\"label\"].value_counts()\n\n0    4816\n1    3129\nName: label, dtype: int64\n\n\nFinally, letâ€™s write them to CSV files for later use.\n\ndf_list = [train_df_full, train_df_half, train_df_quarter, train_df_1eight, val_df]\n\ndf_names = [\"train_df_full\", \"train_df_half\", \"train_df_quarter\", \"train_df_1eight\", \"val_df\"]\n\nfor df_name, df_content in zip(df_names, df_list):\n    df_content.to_csv(f\"{DATA_DIR}{df_name}.csv\", index=False)"
  },
  {
    "objectID": "projects/steel-nylon-classifier.html#dataset-class",
    "href": "projects/steel-nylon-classifier.html#dataset-class",
    "title": "Simple MLP model for Guitar Steel vs Nylon strings classification",
    "section": "5.2 Dataset class",
    "text": "5.2 Dataset class\nWe create GuitarSoundDataset which inherets Dataset from PyTorch. This class holds the annotation that we created earlier and helps us access and preprocess each individual input and label.\nTo create this class, I took inspiration from this awesome Deep Learning for Audio channel: https://www.youtube.com/watch?v=iCwMQJnKk2c&t=1s&ab_channel=ValerioVelardo-TheSoundofAI\n\nfrom torch.utils.data import Dataset\n\nclass GuitarSoundDataset(Dataset):\n\n    def __init__(self,\n                 annotations_file,\n                 transformation,\n                 target_sample_rate,\n                 num_samples,\n                 device,\n                 audio_col=\"audio_path\",\n                 label_col=\"label\"):\n        self.annotations = pd.read_csv(annotations_file)\n        self.device = device\n        if transformation:\n          self.transformation = transformation.to(self.device)\n        else:\n          self.transformation = None\n        self.target_sample_rate = target_sample_rate\n        self.num_samples = num_samples\n        self.audio_col = audio_col\n        self.label_col = label_col\n\n    def __len__(self):\n        return len(self.annotations)\n\n    def __getitem__(self, index):\n        audio_sample_path = self.__get_audio_sample_path(index)\n        label = self.__get_audio_sample_label(index)\n        signal, sr = torchaudio.load(audio_sample_path)\n        if signal.dim() < 2:\n          signal = signal[None, :]\n        signal = signal.to(self.device)\n        signal, sr = self.preprocess_signal(signal, sr)\n        if self.transformation:\n          signal = self.transformation(signal)\n        return signal, label\n\n    def preprocess_signal(self, signal, sr):\n        signal = self.__resample_if_necessary(signal, sr)\n        signal = self.__mix_down_if_necessary(signal)\n        signal = self.__cut_if_necessary(signal)\n        signal = self.__right_pad_if_necessary(signal)\n        return signal, sr\n\n    def __cut_if_necessary(self, signal):\n        if signal.shape[1] > self.num_samples:\n            signal = signal[:, :self.num_samples]\n        return signal\n\n    def __right_pad_if_necessary(self, signal):\n        length_signal = signal.shape[1]\n        if length_signal < self.num_samples:\n            num_missing_samples = self.num_samples - length_signal\n            last_dim_padding = (0, num_missing_samples)\n            signal = torch.nn.functional.pad(signal, last_dim_padding)\n        return signal\n\n    def __resample_if_necessary(self, signal, sr):\n        if sr != self.target_sample_rate:\n            resampler = torchaudio.transforms.Resample(sr, self.target_sample_rate).to(self.device)\n            signal = resampler(signal)\n        return signal\n\n    def __mix_down_if_necessary(self, signal):\n        if signal.shape[0] > 1:\n            signal = torch.mean(signal, dim=0, keepdim=True)\n        return signal\n\n    def __get_audio_sample_path(self, index):\n        path = self.annotations.iloc[index, :][self.audio_col]\n        return path\n\n    def __get_audio_sample_label(self, index):\n        label =  self.annotations.iloc[index, :][self.label_col]\n        return torch.tensor(label, dtype=torch.float)"
  },
  {
    "objectID": "projects/steel-nylon-classifier.html#dataloader",
    "href": "projects/steel-nylon-classifier.html#dataloader",
    "title": "Simple MLP model for Guitar Steel vs Nylon strings classification",
    "section": "5.3 DataLoader",
    "text": "5.3 DataLoader\n\nfrom torch.utils.data import DataLoader\n\ndef create_data_loader(dataset, batch_size):\n    dataset_loader = DataLoader(dataset, batch_size=batch_size)\n    return dataset_loader\n\nMel Spectrogram transforms our signal from time-domain into frequency-domain, which helps not only human but also computers to understand the characteristic of sound input better. Thus, we need to transform each audio input into mel spec before feeding it into the neural network.\n\nmel_spectrogram = torchaudio.transforms.MelSpectrogram(\n      sample_rate=TARGET_SR,\n      n_fft=1024,\n      hop_length=512,\n      n_mels=64\n  )\n\ntrain_dataset = GuitarSoundDataset(\n                      annotations_file =f\"{DATA_DIR}train_df_half.csv\",\n                      transformation = mel_spectrogram,\n                      target_sample_rate = TARGET_SR,\n                      num_samples = TARGET_SR * SEGMENT_DURATION,\n                      device = device)\nprint(f\"There are {len(train_dataset)} samples in the TRAIN dataset.\")\n\nval_dataset = GuitarSoundDataset(f\"{DATA_DIR}val_df.csv\",\n                      transformation = mel_spectrogram,\n                      target_sample_rate = TARGET_SR,\n                      num_samples = TARGET_SR * SEGMENT_DURATION,\n                      device = device)\nprint(f\"There are {len(val_dataset)} samples in the VAL dataset.\")\n\nThere are 3774 samples in the TRAIN dataset.\nThere are 397 samples in the VAL dataset.\n\n\nWe will take one sample out to find out the exact input shape for our neural network\n\nsignal_sample, _ = val_dataset[0]\nsignal_sample.shape\n\ntorch.Size([1, 64, 79])"
  },
  {
    "objectID": "projects/steel-nylon-classifier.html#training-loop",
    "href": "projects/steel-nylon-classifier.html#training-loop",
    "title": "Simple MLP model for Guitar Steel vs Nylon strings classification",
    "section": "6.1 Training Loop",
    "text": "6.1 Training Loop\nBecause the training and validating loops are pretty basic, I donâ€™t delve into these code too much. The official tutorial is where I took inspiration from: https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n\ndef compute_accuracy(preds, target):\n  _preds = preds.detach().cpu().numpy()\n  _target = target.detach().cpu().numpy()\n  return np.mean(_preds.squeeze().round() == _target.squeeze())\n\ndef train_single_epoch(model, data_loader, loss_fn, optimiser, device):\n  size = len(data_loader.dataset)\n  train_losses = []\n  train_accs = []\n\n  model.train(True)\n  for batch, (input, target) in enumerate(data_loader):\n      input, target = input.to(device), target.to(device)\n\n      # calculate loss\n      preds = model(input)\n      loss = loss_fn(preds.squeeze(), target.squeeze())\n      train_losses.append(loss.item())\n\n\n      # backpropagate error and update weights\n      optimiser.zero_grad()\n      loss.backward()\n      optimiser.step()\n\n      # calculate accuracy\n      acc = compute_accuracy(preds, target)\n      train_accs.append(acc)\n\n  return np.mean(train_losses), np.mean(train_accs)\n\ndef validate(model, data_loader, loss_fn, device):\n  # model.train(False)\n  val_losses = []\n  val_accs = []\n  with torch.inference_mode():\n    for input, target in data_loader:\n      input, target = input.to(device), target.to(device)\n\n      # calculate loss\n      preds = model(input)\n      loss = loss_fn(preds.squeeze(), target.squeeze())\n      val_losses.append(loss.item())\n\n      # calculate acc\n      acc = compute_accuracy(preds, target)\n      val_accs.append(acc)\n\n    return np.mean(val_losses), np.mean(val_accs)\n\ndef save_model(model, model_dir):\n  torch.save(model.state_dict(), model_dir)\n\ndef train(model, train_dataloader, test_dataloader, loss_fn, optimiser, device, epochs, save_best=True, model_dir=\"bestmodel.pth\"):\n  train_losses = []\n  train_accs = []\n  val_losses = []\n  val_accs = []\n  for i in range(epochs):\n      # training\n      train_loss, train_acc = train_single_epoch(model, train_dataloader, loss_fn, optimiser, device)\n      # val\n      val_loss, val_acc = validate(model, test_dataloader, loss_fn, device)\n      print(f\"Epoch {i+1} | train loss: {train_loss:.5f}, train acc: {train_acc:.3%} | val loss: {val_loss:.5f}, val acc: {val_acc:.3%}\")\n\n      # save best val acc\n      if save_best and len(val_losses) > 0 and val_acc > np.max(val_accs):\n        # save model\n        print(\"-> Best Model found! Saving to disk...\")\n        save_model(model, model_dir)\n\n      # update losses\n      train_losses.append(train_loss)\n      val_losses.append(val_loss)\n      train_accs.append(train_acc)\n      val_accs.append(val_acc)\n  print(\"Finished training\")\n  return train_losses, train_accs, val_losses, val_accs\n\n\ndef plot_model(model_history):\n    train_losses, train_accs, val_losses, val_accs = model_history\n    # Plot Loss\n    plt.plot(range(len(train_losses)), train_losses, label='Training Loss')\n    plt.plot(range(len(train_losses)), val_losses, label='Validation Loss')\n    \n    # Add in a title and axes labels\n    plt.title('Training and Validation Loss')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend(loc=\"upper left\")\n    plt.show()    \n    \n    # Plot Acc\n    plt.plot(range(len(train_accs)), train_accs, label='Training Acc')\n    plt.plot(range(len(train_accs)), val_accs, label='Validation Acc')\n    \n    # Add in a title and axes labels\n    plt.title('Training and Validation Acc')\n    plt.xlabel('Epochs')\n    plt.ylabel('Accuracy')\n    plt.legend(loc=\"upper left\")\n    plt.show()\n    \ndef describe_model_stats(model_history):\n    train_losses, train_accs, val_losses, val_accs = model_history\n    history = {\"train_losses\": train_losses, \"train_accs\": train_accs, \"val_losses\": val_losses, \"val_accs\": val_accs}\n    print(pd.DataFrame.from_dict(history).describe())"
  },
  {
    "objectID": "projects/steel-nylon-classifier.html#mlp-model-building-2-hidden-layers-with-relu-activation",
    "href": "projects/steel-nylon-classifier.html#mlp-model-building-2-hidden-layers-with-relu-activation",
    "title": "Simple MLP model for Guitar Steel vs Nylon strings classification",
    "section": "6.2 MLP Model Building: 2 hidden layers with ReLu Activation",
    "text": "6.2 MLP Model Building: 2 hidden layers with ReLu Activation\nI define a simple MLP with 2 hidden fully connected layers with relu activation. The final output is then taken by sigmoid to produce probabily prediction.\n\nfrom torch import nn\nfrom torchsummary import summary\n\n\nclass MLPNetwork(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        self.flatten = nn.Flatten()\n        self.linear = nn.Sequential(\n            nn.Linear(1 * 64 * 79, 256), # I got the number (1 * 64 * 79) as input size from the code above\n            nn.ReLU(),\n            nn.Linear(256, 128),\n            nn.ReLU(),\n            nn.Linear(128, 1),\n        )\n\n    def forward(self, input_data):\n        x = self.flatten(input_data)\n        logits = self.linear(x)\n        predictions = torch.sigmoid(logits)\n        return predictions\n        # return x\n\n\nif __name__ == \"__main__\":\n    model2 = MLPNetwork()\n    summary(model2.to(device), (1, 64, 79))\n\n----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n           Flatten-1                 [-1, 5056]               0\n            Linear-2                  [-1, 256]       1,294,592\n              ReLU-3                  [-1, 256]               0\n            Linear-4                  [-1, 128]          32,896\n              ReLU-5                  [-1, 128]               0\n            Linear-6                    [-1, 1]             129\n================================================================\nTotal params: 1,327,617\nTrainable params: 1,327,617\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.02\nForward/backward pass size (MB): 0.04\nParams size (MB): 5.06\nEstimated Total Size (MB): 5.13\n----------------------------------------------------------------\n\n\nAudio input is complex, with an audio sample of 5-second long at 8000 Hz sampling rate, we have an input of 5056 already.\nAnd, this simple MLP model already has 1.3+ millions params.\nNow, letâ€™s create a folder to store our trained params.\n\nMODEL_DIR = f\"{ROOT_DIR}weights/\"\n\nif not os.path.exists(MODEL_DIR):\n    os.makedirs(MODEL_DIR)\n\nThen, define some hyper params for training and create dataloader for each training and validation dataset\n\nBATCH_SIZE = 128\nEPOCHS = 15\nLEARNING_RATE = 0.001\n    \ntrain_dataloader = create_data_loader(train_dataset, BATCH_SIZE)\nval_dataloader = create_data_loader(val_dataset, BATCH_SIZE)\n\nNow, letâ€™s train our model!\n\nMODEL_SAVE_PATH = f\"{MODEL_DIR}model_mlp1.pth\"\nprint(f\"Best models will saved to: {MODEL_DIR} (based on val acc)\")\n\nmodel1 = MLPNetwork()\n\n\nif os.path.exists(MODEL_SAVE_PATH):\n  model1.load_state_dict(torch.load(MODEL_SAVE_PATH, map_location=torch.device(device)))\n\nmodel1 = model1.to(device)\n\n# initialise loss funtion + optimiser\nloss_fn = nn.BCELoss()\n\noptimiser = torch.optim.Adam(model1.parameters(),\n                                lr=LEARNING_RATE)\n\n# train model\nhistory_model1 = train(model1, train_dataloader, val_dataloader, loss_fn, optimiser, device, EPOCHS, save_best=True, model_dir=MODEL_SAVE_PATH)\n\nBest models will saved to: ./weights/ (based on val acc)\nEpoch 1 | train loss: 18.60054, train acc: 74.353% | val loss: 18.41207, val acc: 79.943%\nEpoch 2 | train loss: 16.17105, train acc: 80.607% | val loss: 14.15756, val acc: 82.287%\n-> Best Model found! Saving to disk...\nEpoch 3 | train loss: 16.04773, train acc: 79.716% | val loss: 22.44626, val acc: 72.251%\nEpoch 4 | train loss: 15.91658, train acc: 80.841% | val loss: 24.57104, val acc: 72.446%\nEpoch 5 | train loss: 15.68584, train acc: 81.720% | val loss: 26.59615, val acc: 71.274%\nEpoch 6 | train loss: 17.09794, train acc: 80.188% | val loss: 15.84533, val acc: 81.671%\nEpoch 7 | train loss: 15.53885, train acc: 82.014% | val loss: 15.38519, val acc: 80.364%\nEpoch 8 | train loss: 12.86597, train acc: 84.409% | val loss: 19.71215, val acc: 75.931%\nEpoch 9 | train loss: 12.15642, train acc: 85.247% | val loss: 13.57315, val acc: 81.926%\nEpoch 10 | train loss: 11.81812, train acc: 84.438% | val loss: 12.96833, val acc: 81.145%\nEpoch 11 | train loss: 10.44457, train acc: 86.577% | val loss: 10.41160, val acc: 82.677%\n-> Best Model found! Saving to disk...\nEpoch 12 | train loss: 8.54932, train acc: 88.063% | val loss: 7.47481, val acc: 84.826%\n-> Best Model found! Saving to disk...\nEpoch 13 | train loss: 7.18438, train acc: 88.478% | val loss: 6.82821, val acc: 83.263%\nEpoch 14 | train loss: 5.54964, train acc: 89.264% | val loss: 4.69765, val acc: 84.405%\nEpoch 15 | train loss: 2.19403, train acc: 89.026% | val loss: 1.49113, val acc: 84.075%\nFinished training\n\n\n\ndescribe_model_stats(history_model1)\nplot_model(history_model1)\n\n       train_losses  train_accs  val_losses   val_accs\ncount     15.000000   15.000000   15.000000  15.000000\nmean      12.388064    0.836627   14.304709   0.798988\nstd        4.770562    0.042507    7.314085   0.046287\nmin        2.194033    0.743532    1.491129   0.712740\n25%        9.496942    0.807237    8.943201   0.779372\n50%       12.865967    0.844086   14.157557   0.816707\n75%       15.982155    0.873198   19.062112   0.829703\nmax       18.600537    0.892641   26.596150   0.848257"
  },
  {
    "objectID": "projects/vietnamese-lyrics-classifier.html",
    "href": "projects/vietnamese-lyrics-classifier.html",
    "title": "Vietnamese Lyrics Classification",
    "section": "",
    "text": "Liá»‡u ráº±ng pháº§n lá»i cá»§a má»™t bÃ i hÃ¡t cÃ³ Ä‘áº§y Ä‘á»§ thÃ´ng tin Ä‘á»ƒ chÃºng ta phÃ¢n loáº¡i chá»§ Ä‘á» bÃ i hÃ¡t Ä‘Ã³? Trong bÃ i viáº¿t nÃ y, chÃºng ta sáº½ dÃ¹ng machine learning Ä‘á»ƒ tráº£ lá»i cÃ¢u há»i nÃ y vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p Logistic Regression (PyTorch) / Naive Bayes / Genetic Algorithm / Decision Tree nhÃ©!"
  },
  {
    "objectID": "projects/vietnamese-lyrics-classifier.html#trÃ­ch-xuáº¥t-dá»¯-liá»‡u",
    "href": "projects/vietnamese-lyrics-classifier.html#trÃ­ch-xuáº¥t-dá»¯-liá»‡u",
    "title": "Vietnamese Lyrics Classification",
    "section": "2.1 TrÃ­ch xuáº¥t dá»¯ liá»‡u",
    "text": "2.1 TrÃ­ch xuáº¥t dá»¯ liá»‡u\nCáº£m Æ¡n loibaihathot.com Ä‘Ã£ cÃ³ má»™t kho lá»i bÃ i hÃ¡t khÃ¡ nhiá»u vÃ  dá»… trÃ­ch xuáº¥t. Táº¥t cáº£ nhá»¯ng gÃ¬ chÃºng ta cáº§n chá»‰ lÃ  thÆ° viá»‡n requests Ä‘á»ƒ láº¥y ná»™i dung html cá»§a trang web. VÃ , BeautifulSoup Ä‘á»ƒ parse ná»™i dung html ra element cho dá»… trÃ­ch xuáº¥t.\n\n# Import thÆ° viá»‡n cáº§n thiáº¿t\n\nimport requests\nimport time\nimport csv\nimport re\nfrom bs4 import BeautifulSoup\n\nXÃ¡c Ä‘á»‹nh 1 sá»‘ chá»§ Ä‘á» cÃ³ sáºµn trÃªn trang web Ä‘á»ƒ tÃ i vá». Base URL sáº½ lÃ  url gá»‘c, tá»« Ä‘Ã¢y chÃºng ta replace {genre} Ä‘á»ƒ táº£i lá»i cho bÃ i hÃ¡t thuá»™c chá»§ Ä‘á» tÆ°Æ¡ng á»©ng.\nLÆ°u Ã½: á» Ä‘Ã¢y mÃ¬nh dÃ¹ng tá»« genre khÃ´ng sÃ¡t nghÄ©a â€œchá»§ Ä‘á»â€ Ä‘Ã¢u nghen.\n\ngenre_list = ['cach-mang', 'que-huong', 'thieu-nhi', 'tre']\n\nbase_url = \"https://loibaihathot.com/{genre}\"\n\nTiáº¿p theo, chÃºng ta táº£i háº¿t táº¥t cáº£ cÃ¡c URL cá»§a cÃ¡c bÃ i hÃ¡t, phÃ¢n loáº¡i theo chá»§ Ä‘á»\n\nlyric_url_list = {}\n\nfor genre in genre_list:\n  url = base_url.format(genre=genre)\n  html_text = requests.get(url).text\n  soup = BeautifulSoup(html_text, 'html.parser')\n  all_links = list(map(lambda e: e['href'], soup.find_all('a')))\n  lyric_url_list[genre] = list(set(filter(lambda url: '/20' in url, all_links)))\n\nTáº¡o thÆ° má»¥c data Ä‘á»ƒ lÆ°u trá»¯ dá»¯ liá»‡u, trÃ¡nh viá»‡c pháº£i cháº¡y TrÃ­ch xuáº¥t láº¡i láº§n ná»¯a:\n\nimport os\n\nDATA_FOLDER = 'data/'\n\ndef create_path_if_nonexist(path):\n  if not os.path.exists(path):\n    os.mkdir(path)\n\ncreate_path_if_nonexist(DATA_FOLDER)\n\nViáº¿t hÃ m Ä‘á»ƒ táº£i ná»™i dung lá»i bÃ i hÃ¡t cho má»™t bÃ i hÃ¡t, vá»›i input lÃ  Ä‘Æ°á»ng dáº«n Ä‘áº¿n bÃ i hÃ¡t Ä‘Ã³:\n\nimport re\n\ndef download_lyric(song_url):\n  html_text = requests.get(song_url).text\n  soup = BeautifulSoup(html_text, 'html.parser')\n  lyric = soup.find('div', class_=\"entry-content content mt-6\").get_text(separator = '\\n', strip = True)\n  lyric = re.sub('\\n.+','',lyric, count=3).strip()\n  return lyric\n\nCÃ²n Ä‘Ã¢y lÃ  hÃ m Ä‘á»ƒ táº£i lá»i nhiá»u bÃ i cÃ¹ng 1 lÃºc, hÃ m nÃ y sáº½ dÃ¹ng Ä‘á»ƒ phÃ¢n luá»“ng threading -> tÄƒng tá»‘c download:\n\ndef download_songs(song_urls, genre):\n  full_path = os.path.join(DATA_FOLDER, genre)\n  create_path_if_nonexist(full_path)\n  for url in song_urls:\n    lyric = download_lyric(url)\n    if len(lyric) < 10:\n      continue\n    file_name =re.findall('/[^\\/]+$', url)[0][1:-5]\n    with open(f'{full_path}/{file_name}.txt', 'w') as f:\n      f.write(lyric)\n\nVá»›i má»—i chá»§ Ä‘á», chÃºng ta sáº½ dÃ¹ng threading Ä‘á»ƒ táº£i háº¿t lá»i bÃ i hÃ¡t vá». Äá»ƒ táº£i nhanh hÆ¡n, chÃºng ta cÃ³ thá»ƒ chia thÃªm nhiá»u threads con ná»¯a. NhÆ°ng mÃ¬nh muá»‘n má»i thá»© Ä‘Æ¡n giáº£n trÆ°á»›c Ä‘Ã£:\n\nimport threading\n\nthread_list = []\n\n# create list of threads\nfor genre in genre_list:\n  thread = threading.Thread(target=download_songs, args=(lyric_url_list[genre],genre))\n  thread_list.append(thread)\n\nprint(\"Download starting...\")\n# start each thread\nfor thread in thread_list:\n  thread.start()\n\n# wait for all to finish\nfor thread in thread_list:\n  thread.join()\n\n# successfully excecuted\nprint(\"Download finished!\")\n\nDownload starting...\nDownload finished!"
  },
  {
    "objectID": "projects/vietnamese-lyrics-classifier.html#tiá»n-xá»­-lÃ½-pre-processing",
    "href": "projects/vietnamese-lyrics-classifier.html#tiá»n-xá»­-lÃ½-pre-processing",
    "title": "Vietnamese Lyrics Classification",
    "section": "2.2 Tiá»n xá»­ lÃ½ (Pre-processing)",
    "text": "2.2 Tiá»n xá»­ lÃ½ (Pre-processing)\nSau khi mÃ¬nh vÃ  train thá»­ cho bá»™ dá»¯ liá»‡u ban Ä‘áº§u thÃ¬ tháº¥y Ä‘á»™ chÃ­nh xÃ¡c Ä‘á»u dÆ°á»›i 20%. Kiá»ƒm tra láº¡i thÃ¬ tháº¥y cÃ³ nhiá»u bÃ i download vá» vÃ o sai thÆ° má»¥c chá»§ Ä‘á». Äiá»u nÃ y cÃ³ thá»ƒ gÃ¢y nhiá»…u dá»¯ liá»‡u, nÃªn chÃºng ta sáº½ xÃ³a thá»§ xÃ´ng nhá»¯ng files sau Ä‘á»ƒ tÄƒng Ä‘á»™ chÃ­nh xÃ¡c khi huáº¥n luyá»‡n:\n\n# Clean-up data\n\nto_delete = {\n    \"thieu-nhi\":[\n        \"cam-on-nguoi-da-roi-xa-toi.txt\",\n        \"chac-ai-do-se-ve.txt\",\n        \"dung-tin-em-manh-me.txt\",\n        \"hay-ra-khoi-nguoi-do-di.txt\",\n        \"khuon-mat-dang-thuong.txt\",\n    ],\n    \n    \"cach-mang\":[\n        \"cam-on-nguoi-da-roi-xa-toi.txt\",\n        \"chac-ai-do-se-ve.txt\",\n        \"dung-tin-em-manh-me.txt\",\n        \"hay-ra-khoi-nguoi-do-di.txt\",\n        \"khuon-mat-dang-thuong.txt\",\n    ],\n    \n    \"que-huong\":[\n        \"cam-on-nguoi-da-roi-xa-toi.txt\",\n        \"chac-ai-do-se-ve.txt\",\n        \"dung-tin-em-manh-me.txt\",\n        \"hay-ra-khoi-nguoi-do-di.txt\",\n        \"khuon-mat-dang-thuong.txt\",\n    ],\n\n}\n\nfor subfolder, filenames in to_delete.items():\n    for filename in filenames:\n      try:\n        filepath = f\"{DATA_FOLDER}{subfolder}/{filename}\"\n        os.remove(filepath)\n      except Exception as e:\n        print(e)\n\nCÃ i thÃªm thÆ° viá»‡n underthesea Ä‘á»ƒ há»— trá»£ tokenize ngÃ´n ngá»¯ Viá»‡t:\n\n!pip install -q underthesea==1.3.5a3\n\nTiá»n xá»­ lÃ½ lá»i bÃ i hÃ¡t, chá»§ yáº¿u lÃ  xÃ³a kÃ½ tá»± Ä‘áº·c biá»‡t:\n\ndef preprocess_lyric(text):\n  # remove special characters\n  import re\n  text = re.sub('[^\\w\\s]','', text).lower()\n  return text\n\npreprocess_lyric('NgÃ y mai??!! 13 em Ä‘i!')\n\n'ngÃ y mai 13 em Ä‘i'"
  },
  {
    "objectID": "projects/vietnamese-lyrics-classifier.html#xÃ¢y-dá»±ng-mÃ´-hÃ¬nh",
    "href": "projects/vietnamese-lyrics-classifier.html#xÃ¢y-dá»±ng-mÃ´-hÃ¬nh",
    "title": "Vietnamese Lyrics Classification",
    "section": "3.1 XÃ¢y dá»±ng mÃ´ hÃ¬nh",
    "text": "3.1 XÃ¢y dá»±ng mÃ´ hÃ¬nh\nChÃºng ta import nhá»¯ng library cáº§n thiáº¿t cá»§a PyTorch\n\nimport torch\nimport matplotlib.pyplot as plt\nfrom torch import nn\nfrom torch import optim\nfrom torchvision import datasets, transforms\n\nseed = 69\ntorch.manual_seed(seed)\n\n<torch._C.Generator at 0x7f4a9c335130>\n\n\nKhai bÃ¡o mÃ´ hÃ¬nh gá»“m 2 lá»›p: 1. Linear: Nháº­n input lÃ  tensor cá»§a bÃ i hÃ¡t Ä‘Æ°á»£c tokenize thÃ nh tensor cÃ³ n_features vÃ  output lÃ  tensor cÃ³ sá»‘ chiá»u lÃ  n_labels tÆ°Æ¡ng á»©ng vá»›i sá»‘ lÆ°á»£ng chá»§ Ä‘á» cáº§n phÃ¢n loáº¡i 2. LogSoftMax: ÄÃ¢y lÃ  dáº¡ng activation function cho bÃ i toÃ¡n phÃ¢n loáº¡i Ä‘a lá»›p\n\nmodel = nn.Sequential(nn.Linear(n_features, n_labels),nn.LogSoftmax(dim=1))\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(), lr=0.001)\n\nHÃ m tÃ­nh Ä‘á»™ chÃ­nh xÃ¡c cá»§a mÃ´ hÃ¬nh:\n\ndef calculate_accuracy(model, X, labels):\n  y_hat = model(torch.as_tensor(X).float())\n  preds = y_hat.max(axis=1, keepdim=True)[1].numpy().squeeze()\n  correct = np.sum(preds == labels)\n  accuracy = correct / len(labels)\n  return accuracy\n\nTiáº¿n hÃ nh train model\n\nepochs = 300\n\nlosses = []\naccs = []\nfor e in range(epochs+1):\n  optimizer.zero_grad()\n  # tÃ­nh y_hat\n  output = model(X_train_tensor)\n\n  # tÃ­nh loss\n  loss = criterion(output, y_train_onehot_tensor)\n\n  # tÃ­nh gradient\n  loss.backward()\n\n  # tá»‘i Æ°u gradient\n  optimizer.step()\n\n  # cáº­p nháº­t loss\n  running_loss = loss.item()\n  losses.append(running_loss)\n\n  # tÃ­nh accuracy\n  acc = calculate_accuracy(model, X_test, y_test)\n  accs.append(acc)\n\n  # in thÃ´ng sá»‘ training\n  if e % 100 == 0:\n    print(f\"Training epoch {e} : loss: {running_loss:.3f}; accuracy: {acc:.2%}\")\n\nTraining epoch 0 : loss: 9201.797; accuracy: 40.00%\nTraining epoch 100 : loss: 16386.709; accuracy: 76.67%\nTraining epoch 200 : loss: 6421.723; accuracy: 56.67%\nTraining epoch 300 : loss: 906.633; accuracy: 86.67%\n\n\nMÃ´ hÃ¬nh chÃºng ta sau khi train 300 epoch cÃ³ accuracy 86.67%.\n\nplt.plot(losses)\nplt.xlabel('epoch')\nplt.ylabel('Loss')\nplt.title('Model losses over time (epoch)')\nplt.show()\n\n\n\n\n\nplt.plot(accs)\nplt.xlabel('epochs')\nplt.ylabel('Loss')\nplt.title('Model accuracy over time (epoch)')\nplt.show()\n\n\n\n\nHÃ m sau dÃ¹ng Ä‘á»ƒ phÃ¢n loáº¡i lá»i bÃ i hÃ¡t báº¥t ká»³.\n\ndef predict_label(model, lyric):\n  x = lyric_to_features(lyric, n_labels)\n  y_hat = model(torch.as_tensor(np.array([x])).float())\n  pred = y_hat.max(axis=1, keepdim=True)[1].numpy().squeeze()\n  return genre_list[pred]"
  },
  {
    "objectID": "projects/vietnamese-lyrics-classifier.html#predict",
    "href": "projects/vietnamese-lyrics-classifier.html#predict",
    "title": "Vietnamese Lyrics Classification",
    "section": "3.2 Predict",
    "text": "3.2 Predict\ná» Ä‘Ã¢y chÃºng ta sáº½ phÃ¢n loáº¡i thá»­ má»™t sá»‘ bÃ i hÃ¡t má»™t Ã¡ch cÃ¡ch thá»§ cÃ´ng nhÃ©.\nÄÃ¢y lÃ  bÃ i â€œHÃ¡t vá» anhâ€, thuá»™c thá»ƒ loáº¡i cÃ¡ch máº¡ng. MÃ´ hÃ¬nh Ä‘Ã£ phÃ¢n loáº¡i chÃ­nh xÃ¡c trÆ°á»ng há»£p nÃ y.\n\nlyric = '''HÃ¡t Vá» Anh\nMá»™t ba lÃ´, cÃ¢y sÃºng trÃªn vai,\nNgÆ°á»i chiáº¿n sÄ© quen vá»›i gian lao,\nNgÃ y dÃ i Ä‘Ãªm thÃ¢u váº«n cÃ³ nhá»¯ng ngÆ°á»i lÃ­nh tráº»,\nNáº·ng tÃ¬nh quÃª hÆ°Æ¡ng canh giá»¯ trÃªn miá»n Ä‘áº¥t máº¹.\nRá»«ng Ã¢m u, mÃ¢y nÃºi mÃªnh mÃ´ng\nNgÃ y náº¯ng chÃ¡y, Ä‘Ãªm giÃ¡ láº¡nh Ä‘áº§y.\nRá»«ng má» sÆ°Æ¡ng khuya bÃ³ng tá»‘i quÃ¢n thÃ¹ trÆ°á»›c máº·t,\nNáº·ng tÃ¬nh non sÃ´ng anh dÃ¢ng trá»n tuá»•i Ä‘á»i thanh xuÃ¢n.\nCho em thÆ¡ ngá»§ ngon vÃ  vui bÆ°á»›c sá»›m hÃ´m Ä‘áº¿n trÆ°á»ng\nCho yÃªn vui mÃ¹a xuÃ¢n Ä‘Ã´i lá»©a cÃ²n háº¹n hÃ² Æ°á»›c mÆ¡\nÃÃ£ cÃ³ nhá»¯ng hy sinh khÃ³ nÃ³i háº¿t báº±ng lá»i\nNÃªn Ä‘á»ng láº¡i trong tÃ´i nhá»¯ng nghÄ© suy.\nCho tÃ´i bÃ i ca vá» ngÆ°á»i chiáº¿n sÄ© nÆ¡i tuyáº¿n Ä‘áº§u.\nNÆ¡i biÃªn cÆ°á»ng rá»«ng sÃ¢u, anh Ã¢m tháº§m chá»‹u Ä‘á»±ng giÃ³ sÆ°Æ¡ng.\nÄÃ£ cÃ³ nhá»¯ng gian lao, Ä‘Ã£ cÃ³ nhá»¯ng nhá»c nháº±n\nMang trong trÃ¡i tim anh trá»n niá»m tin.\nXin hÃ¡t mÃ£i vá» anh ngÆ°á»i chiáº¿n sÄ© biÃªn cÆ°Æ¡ng\nXin hÃ¡t mÃ£i vá» anh ngÆ°á»i chiáº¿n sÄ© biÃªn cÆ°Æ¡ng\nNghe lá»i bÃ i hÃ¡t HÃ¡t Vá» Anh\nHÃ¡t Vá» Anh '''\n\npredict_label(model, lyric)\n\n'cach-mang'\n\n\nBÃ i hÃ¡t DÃ¢y Äá»§ng Äá»‰nh Buá»“n Ä‘Æ°á»£c phÃ¢n loáº¡i QuÃª HÆ°Æ¡ng trong dá»¯ liá»‡u. ÄÃ¢y cÅ©ng lÃ  má»™t dá»± Ä‘oÃ¡n chÃ­nh xÃ¡c.\n\nlyric = '''DÃ¢y Äá»§ng Äá»‰nh Buá»“n (Remix)\nEm Ä‘i theo chá»“ng xa thÃ´n lÃ ng cÃ¡ch biá»‡t dÃ²ng sÃ´ng.\nEm Ä‘i theo chá»“ng anh nÆ¡i nÃ y má»i mon` Ä‘á»£i trÃ´ng\nNhÆ° dÃ¢y Ä‘á»§ng Ä‘á»‰nh nuÃ´i trÃ¡i tÃ¬nh bÃ o thÃ¡ng ngÃ y qua.\nTÃ¬nh Ä‘Ã£ trá»ng xanh rá»“i ngÆ°á»i ná»¡ Ä‘em Ä‘i hÃ¡i cho Ä‘Ã nh.\nAi xuÃ´i chá»‰ mÃ¬nh Ã´m ná»—i buá»“n cho ngÆ°á»i ta vui.\nAi xuÃ´i chá»‰ mÃ¬nh xÃ¢y duyÃªn tÃ¬nh giá» Ä‘Ã¢y láº» loi.\nNhÃ¬n con nÆ°á»›c cháº£y theo con thuyá»n láº¡c báº¿n Ä‘á»i nhau.\nLá»i Æ°á»›c háº¹n xÆ°a giá» thÃ¬ cÅ©ng xa xa cuá»‘i chÃ¢n trá»i.\nÄK:\nÄau thÆ°Æ¡ng thui thá»§i Ä‘Ãªm trÆ°Æ¡ng, giÃ³ láº¡nh tá»«ng Ä‘Ãªm láº» bÃ³ng Ä‘Æ¡n cÃ´i.\nBuá»“n miÃªn man tháº§m trÃ¡ch cho Ä‘á»i lÆ¡ lá»­ng chi rá»“i bá» báº¡n mÃ¬nh Ãªn.\nYÃªu thÆ°Æ¡ng xin tráº£ cho ngÆ°á»i nuá»‘t lá»‡ nhÃ¬n theo Ä‘Ã¡m cÆ°á»›i ngÆ°á»i ta.\nÄá»ƒ bÃªn Ä‘Ã¢y Ä‘á»§ng Ä‘á»‰nh u buá»“n, sao mang Ã¢n tÃ¬nh trao táº·ng ngÆ°á»i ta.\nNghe lá»i bÃ i hÃ¡t DÃ¢y Äá»§ng Äá»‰nh Buá»“n (Remix)\n\nÂ \nDÃ¢y Äá»§ng Äá»‰nh Buá»“n (Remix)'''\n\npredict_label(model, lyric)\n\n'que-huong'\n\n\nBÃ i hÃ¡t thiáº¿u nhi nÃ y cÅ©ng Ä‘Æ°á»£c phÃ¢n loáº¡i Ä‘Ãºng:\n\nlyric = '''Ai yÃªu bÃ¡c Há»“ ChÃ­ Minh hÆ¡n thiáº¿u nhi Viá»‡t Nam\nBÃ¡c chÃºng em dÃ¡ng cao cao, ngÆ°á»i thanh thanh\nBÃ¡c chÃºng em máº¯t nhÆ° sao, rÃ¢u hÆ¡i dÃ i\nBÃ¡c chÃºng em nÆ°á»›c da nÃ¢u vÃ¬ sÆ°Æ¡ng giÃ³\nBÃ¡c chÃºng em thá» cÆ°Æ¡ng quyáº¿t tráº£ thÃ¹ nhÃ \nHá»“ ChÃ­ Minh kÃ­nh yÃªu, chÃºng em kÃ­nh yÃªu BÃ¡c Há»“ ChÃ­ Minh trá»n má»™t Ä‘á»i\nHá»“ ChÃ­ Minh kÃ­nh yÃªu BÃ¡c Ä‘Ã£ bao phen bÃ´n ba nÆ°á»›c ngoÃ i vÃ¬ giá»‘ng nÃ²i\nBÃ¡c nay tuy Ä‘Ã£ giÃ  rá»“i\nGiÃ  rá»“i nhÆ°ng váº«n vui tÆ°Æ¡i\nNgÃ y ngÃ y chÃºng chÃ¡u Æ°á»›c mÆ¡\nMong sao BÃ¡c sá»‘ng muÃ´n Ä‘á»i Ä‘á»ƒ dáº«n dáº¯t nhi Ä‘á»“ng thÃ nh ngÆ°á»i vÃ  kiáº¿n thiáº¿t nÆ°á»›c nhÃ  báº±ng NgÆ°á»i\nHá»“ ChÃ­ Minh kÃ­nh yÃªu, chÃºng em kÃ­nh yÃªu BÃ¡c Há»“ ChÃ­ Minh trá»n má»™t Ä‘á»i\nHá»“ ChÃ­ Minh kÃ­nh yÃªu, chÃºng em Æ°á»›c sao BÃ¡c Há»“ ChÃ­ Minh sá»‘ng muÃ´n Ä‘á»i\nAi YÃªu BÃ¡c Há»“ ChÃ­ Minh HÆ¡n ChÃºng Em Nhi Äá»“ng'''\n\npredict_label(model, lyric)\n\n'thieu-nhi'\n\n\nBÃ i hÃ¡t â€œÃnh náº¯ng cá»§a anhâ€ Ä‘Æ°á»£c phÃ¢n loáº¡i chÃ­nh xÃ¡c cho thá»ƒ loáº¡i â€œTráº»â€\n\nlyric = '''Nhá»¯ng phÃºt giÃ¢Ñƒ trÃ´i qua táº§m taÑƒ\nÏ¹há» má»™t ai Ä‘Ã³ Ä‘áº¿n bÃªn anh\nLáº·ng nghe nhá»¯ng tÃ¢m tÆ° nÃ Ñƒ\nLÃ  tia náº¯ng áº¥m\nLÃ  em Ä‘áº¿n bÃªn anh cho vÆ¡i Ä‘i Æ°u phiá»n ngÃ Ñƒ hÃ´m qua\nNháº¹ nhÃ ng xÃ³a Ä‘i bao mÃ¢Ñƒ Ä‘en vÃ¢Ñƒ quanh cuá»™c Ä‘á»i nÆ¡i anh\nPhÃºt giÃ¢Ñƒ anh mong Ä‘áº¿n tÃ¬nh ÑƒÃªu áº¥Ñƒ\nGiá» Ä‘Ã¢Ñƒ lÃ  em, ngÆ°á»i anh mÆ¡ Æ°á»›c bao Ä‘Ãªm\nĞ…áº½ luÃ´n tháº­t gáº§n bÃªn em\nĞ…áº½ luÃ´n lÃ  vÃ²ng taÑƒ áº¥m Ãªm\nĞ…áº½ luÃ´n lÃ  ngÆ°á»i ÑƒÃªu em\nÏ¹Ã¹ng em Ä‘i Ä‘áº¿n chÃ¢n trá»i\nLáº¯ng nghe tá»«ng nhá»‹p tim anh\nLáº¯ng nghe tá»«ng lá»i anh muá»‘n nÃ³i\nVÃ¬ em luÃ´n Ä‘áº¹p nháº¥t khi em cÆ°á»i'''\n\npredict_label(model, lyric)\n\n'tre'\n\n\nBÃ¢y giá» mÃ¬nh sáº½ láº¥y 2 bÃ i hÃ¡t khÃ´ng trong táº­p dá»¯ liá»‡u\nBÃ i thá»© nháº¥t lÃ  má»™t bÃ i nháº¡c tráº» má»›i ná»•i gáº§n Ä‘Ã¢y â€œAnh ChÆ°a ThÆ°Æ¡ng Em Äáº¿n Váº­y ÄÃ¢uâ€:\n\nlyric = '''Sao mÃ¬nh khÃ´ng gáº¡t bá» Ä‘i háº¿t nhá»¯ng lá»i nÃ³i ngoÃ i kia\nVÃ  sao mÃ¬nh khÃ´ng gáº¡t bá» Ä‘i háº¿t nhá»¯ng Ä‘á»‹nh kiáº¿n ngoÃ i kia\n\nGiá»¯a ngÃ¢n hÃ  em biáº¿t Ä‘Ã¢u lÃ \nBiáº¿t Ä‘Ã¢u lÃ  tháº¿ gian nÃ y mÃ \nMÃ¬nh bÃªn nhau, Ä‘Æ°á»£c yÃªu nhau, Ä‘Æ°á»£c trao nhau, tÃ¬nh yÃªu sÃ¢u trÃ¡i tim Ä‘áº­m sÃ¢u\n\nGiá»¯a ngÃ¢n hÃ  em biáº¿t Ä‘Ã¢u lÃ \nBiáº¿t Ä‘Ã¢u má»™t sá»›m mai khi mÃ \nCáº§n bao lÃ¢u, chá» bao lÃ¢u, Ä‘á»£i bao lÃ¢u, tÃ¬nh trao nhau mÃ£i thÃ´i Ä‘áº­m sÃ¢u\n\nGiá»¯a ngÃ¢n hÃ , giá»¯a ngÃ¢n hÃ , giá»¯a ngÃ¢n hÃ \nBiáº¿t Ä‘Ã¢u lÃ , biáº¿t Ä‘Ã¢u lÃ , biáº¿t Ä‘Ã¢u lÃ \nHÃ nh tinh cá»§a hai chÃºng ta\nMá»™t nÆ¡i cá»§a riÃªng chÃºng ta\n\nGiá»¯a ngÃ¢n hÃ , giá»¯a ngÃ¢n hÃ , giá»¯a ngÃ¢n hÃ \nBiáº¿t Ä‘Ã¢u lÃ , biáº¿t Ä‘Ã¢u lÃ , biáº¿t Ä‘Ã¢u lÃ \nHÃ nh tinh cá»§a hai chÃºng ta\ná» 1 tháº¿ giá»›i cÃ²n ráº¥t xa'''\n\npredict_label(model, lyric)\n\n'tre'\n\n\nVá»›i bÃ i hÃ¡t â€œBÆ°á»›c qua mÃ¹a cÃ´ Ä‘Æ¡nâ€:\n\nlyric = '''MÃ¹a thu rÆ¡i vÃ o em, vÃ o trong giáº¥c mÆ¡ hÃ´m qua\nMÃ¹a thu Ã´m mÃ¬nh em, cháº¡y xa vÃ²ng tay vá»™i vÃ£\nLá»i em nÃ³i ngÃ y xÆ°a Ä‘Ã¢u Ä‘Ã¢y\nVáº«n Ã¢m tháº§m chÃ¬m vÃ o trong mÃ¢y\nÄáº¿n bao giá», dáº·n lÃ²ng anh khÃ´ng mong nhá»›\nMÃ¹a thu rÆ¡i vÃ o em, vÃ o trong chiáº¿c hÃ´n ngÃ¢y thÆ¡\nMÃ¹a thu khÃ´ng cáº§n anh, vÃ¬ em giá» Ä‘Ã¢y cÃ²n mÃ£i há»¯ng há»\nNgÃ y mai kia náº¿u cÃ³ phÃºt giÃ¢y vÃ´ tÃ¬nh tháº¥y nhau sáº½ nÃ³i cÃ¢u gÃ¬...\nHay ta chá»‰ nhÃ¬n\nLáº·ng láº½\nÄi qua\nChÃ o cÆ¡n mÆ°a\nLÃ m sao cá»© kÃ©o ta quay láº¡i\nNhá»¯ng rung Ä‘á»™ng con tim\nLáº§n Ä‘áº§u hai ta gáº·p gá»¡'''\n\npredict_label(model, lyric)\n\n'tre'"
  },
  {
    "objectID": "articles/translate-batch-normalization-in-3-levels-of-understanding.html",
    "href": "articles/translate-batch-normalization-in-3-levels-of-understanding.html",
    "title": "3 Cáº¥p Ä‘á»™ hiá»ƒu vá» Batch Normalization (BÃ i dá»‹ch)",
    "section": "",
    "text": "Trong series bÃ i dá»‹ch nÃ y, mÃ¬nh sÆ°u táº§m nhá»¯ng bÃ i trÃªn nguá»“n nhÆ° Medium vÃ  dá»‹ch láº¡i vá»›i má»¥c Ä‘Ã­ch:\nTÃ¬m Ä‘á»c bÃ i gá»‘c á»Ÿ Ä‘Ã¢y"
  },
  {
    "objectID": "articles/translate-batch-normalization-in-3-levels-of-understanding.html#cÃ¡ch-hiá»ƒu-trong-30-giÃ¢y",
    "href": "articles/translate-batch-normalization-in-3-levels-of-understanding.html#cÃ¡ch-hiá»ƒu-trong-30-giÃ¢y",
    "title": "3 Cáº¥p Ä‘á»™ hiá»ƒu vá» Batch Normalization (BÃ i dá»‹ch)",
    "section": "2.1 CÃ¡ch hiá»ƒu trong 30 giÃ¢y",
    "text": "2.1 CÃ¡ch hiá»ƒu trong 30 giÃ¢y\nBatch-Normalization (BN) lÃ  phÆ°Æ¡ng phÃ¡p khiáº¿n cho viá»‡c huáº¥n luyá»‡n máº¡ng nÆ¡ rÃ´ng sÃ¢u (Deep Nearon Network, DNN) nhanh vÃ  á»•n Ä‘á»‹nh hÆ¡n.\nNÃ³ bao gá»“m chuáº©n hoÃ¡ cÃ¡c vectors cá»§a lá»›p áº©n (hidden layers) sá»­ dá»¥ng trung bÃ¬nh vÃ  phÆ°Æ¡ng sai (mean vÃ  variance) cá»§a batch hiá»‡n táº¡i. BÆ°á»›c chuáº©n hoÃ¡ cÃ³ thá»ƒ Ä‘Æ°á»£c Ã¡p dá»¥ng ngay trÆ°á»›c hoáº·c ngay sau má»™t hÃ m phi tuyáº¿n tÃ­nh.\n\n\n\n\nMultilayer Perceptron (MLP) khÃ´ng batch normalization (BN) | Nguá»“n : author - Design : Lou HD\n\n\n\n\n\nMultilayer Perceptron (MLP) cÃ³ batch normalization (BN) | Nguá»“n : author - Design : Lou HD\n\nTáº¥t cáº£ cÃ¡c ná»n táº£ng há»c sÃ¢u Ä‘á»u Ä‘Ã£ há»— trá»£ Batch Normalization. ThÆ°á»ng báº¡n sáº½ sá»­ dá»¥ng BN nhÆ° má»™t lá»›p trong máº¡ng DNN.\nVá»›i nhá»¯ng ai thÃ­ch Ä‘á»c code hÆ¡n chá»¯ thÃ¬ tÃ¡c giáº£ cÃ³ triá»ƒn khai BN dáº¡ng Jupyter Notebook á»Ÿ Ä‘Ã¢y."
  },
  {
    "objectID": "articles/translate-batch-normalization-in-3-levels-of-understanding.html#cÃ¡ch-hiá»ƒu-trong-3-phÃºt",
    "href": "articles/translate-batch-normalization-in-3-levels-of-understanding.html#cÃ¡ch-hiá»ƒu-trong-3-phÃºt",
    "title": "3 Cáº¥p Ä‘á»™ hiá»ƒu vá» Batch Normalization (BÃ i dá»‹ch)",
    "section": "2.2 CÃ¡ch hiá»ƒu trong 3 phÃºt",
    "text": "2.2 CÃ¡ch hiá»ƒu trong 3 phÃºt\nCÃ¡ch tÃ­nh toÃ¡n BN lÃ  khÃ¡c nhau cho traing vs testing.\n\n2.2.1 Training\nVá»›i má»—i lá»›p áº©n (hidden layer), BN chuyá»ƒn Ä‘á»•i tÃ­n hiá»‡u nhÆ° sau:\n\n\n\nLá»›p BN Ä‘áº§u tiÃªn xÃ¡c Ä‘á»‹nh trung bÃ¬nh ğœ‡ vÃ  phÆ°Æ¡ng sai ÏƒÂ² cá»§a cÃ¡c kÃ­ch hoáº¡t (activation) trong batch, sá»­ dá»¥ng cÃ´ng thá»©c (1) vÃ  (2). Tiáº¿p theo, nÃ³ chuáº©n hoÃ¡ vector kÃ­ch hoáº¡t \\(Z^{(i)}\\) vá»›i cÃ´ng thá»©c (3). Tháº¿ lÃ , táº¥t cáº£ cÃ¡c output Ä‘á»u Ä‘Æ°á»£c tuÃ¢n theo phÃ¢n phá»‘i chuáº©n trong batch Ä‘Ã³. (ğœ€ lÃ  má»™t háº±ng sá»‘ giÃºp duy trÃ¬ numerical stablity)\n\n\n\n\nBÆ°á»›c Ä‘áº§u cá»§a Batch Norm. VÃ­ dá»¥ má»™t hidden layer vá»›i 3 neurons, kÃ­ch cá»¡ batch lÃ  b. Má»—i neuron sau Ä‘Ã³ Ä‘á»u tuÃ¢n theo phÃ¢n phá»‘i chuáº©n | Nguá»“n : author - Design : Lou HD\n\n\n\n\n\nBatch Norm trÃªn dá»¯ liá»‡u 1 chiá»u. Mean vÃ  variance Ä‘Æ°á»£c trÃ­nh trÃªn toÃ n bá»™ cÃ¡c features cá»§a batch | Nguá»“n: AI Vietnam\n\n\n\n\n\nBatch Norm trÃªn dá»¯ liá»‡u 2 chiá»u. Mean vÃ  variance Ä‘Æ°á»£c trÃ­nh theo tá»«ng channel cá»§a input (tÆ°Æ¡ng á»©ng vá»›i sá»‘ filters cá»§a lá»›p Convolution ngay trÆ°á»›c Ä‘Ã³) | Nguá»“n: AI Vietnam\n\ná» bÆ°á»›c cuá»‘i, BN tÃ­nh output áº(i) báº±ng cÃ¡ch Ã¡p dá»¥ng má»™t biáº¿n Ä‘á»•i tuyá»ƒn tÃ­nh (linear transformation) vá»›i hai tham sá»‘ huáº¥n luyá»‡n lÃ  ğ›¾ vÃ  ğ›½ (4). BÆ°á»›c nÃ y cho phÃ©p mÃ´ hÃ¬nh chá»n Ä‘Æ°á»£c phÃ¢n phá»‘i tá»‘i Æ°u cho tá»«ng lá»›p áº©n khi thay Ä‘á»•i hai tham sá»‘:\n\nğ›¾ giÃºp Ä‘iá»u chá»‰nh phÆ°Æ¡ng sai phÃ¢n phá»‘i\nğ›½ giÃºp Ä‘iá»u chá»‰nh bias, dá»‹ch chuyá»ƒn phÃ¢n phá»‘i sang trÃ¡i hay pháº£i\n\n\n\n\n\nLá»£i Ã­ch cá»§a tham sá»‘ ğ›¾ vÃ  ğ›½: Thay Ä‘á»•i phÃ¢n phá»‘i (hÃ¬nh trÃªn) giÃºp chÃºng ta sá»­ dá»¥ng cÃ¡c hÃ¬nh thÃ¡i khÃ¡c nhau cá»§a hÃ m phi tuyáº¿n tÃ­nh (hÃ¬nh dÆ°á»›i) | Nguá»“n : author - Design : Lou HD\n\n\nLÆ°u Ã½: Nhá»¯ng lÃ½ do giáº£i thÃ­ch cho sá»± hiá»‡u quáº£ cá»§a BN cÃ³ thá»ƒ bá»‹ hiá»ƒu sai hoáº·c máº¯c lá»—i (ngay cáº£ trong bÃ i bÃ¡o gá»‘c). Má»™t bÃ i bÃ¡o gáº§n Ä‘Ã¢y [2] phá»§ Ä‘á»‹nh vÃ i giáº£ thiáº¿t sai vÃ  giÃºp cá»™ng Ä‘á»“ng hiá»ƒu tá»‘t hÆ¡n vá» BN. ChÃºng ta sáº½ nÃ³i rÃµ hÆ¡n trong pháº§n â€œTáº¡i sao BN hiá»‡u quáº£?â€\n\nVá»›i má»—i láº§n láº·p, network sáº½ tÃ­nh toÃ¡n trung bÃ¬nh ğœ‡ vÃ  phÆ°Æ¡ng sai ÏƒÂ² cho batch hiá»‡n táº¡i. Sau Ä‘Ã³ nÃ³ huáº¥n luyá»‡n ğ›¾ vÃ  ğ›½ báº±ng gradient descent, sá»­ dá»¥ng ÄÆ°á»ng trung bÃ¬nh Ä‘á»™ng hÃ m mÅ© (Exponential Moving Average/EMA) giÃºp Æ°u tiÃªn hÆ¡n cho nhá»¯ng iterations gáº§n nháº¥t.\n\n\n2.2.2 ÄÃ¡nh giÃ¡\nKhÃ¡c vá»›i khi huáº¥n luyá»‡n, chÃºng ta cÃ³ thá»ƒ khÃ´ng cÃ³ batch Ä‘áº§y Ä‘á»§ Ä‘á»ƒ Ä‘Æ°a vÃ o mÃ´ hÃ¬nh.\nÄá»ƒ giáº£i quyáº¿t váº¥n Ä‘á» nÃ y, chÃºng ta tÃ­nh (ğœ‡_pop , Ïƒ_pop) vá»›i:\n\nğœ‡_pop : Æ°á»›c lÆ°á»£ng giÃ¡ trá»‹ trung bÃ¬nh cho toÃ n bá»™ quáº§n thá»ƒ (population) Ä‘Æ°á»£c nghiÃªn cá»©u\nÏƒ_pop : Æ°á»›c lÆ°á»£ng giÃ¡ trá»‹ Ä‘á»™ lá»‡ch chuáº©n cho toÃ n bá»™ quáº§n thá»ƒ (population) Ä‘Æ°á»£c nghiÃªn cá»©u\n\nHai giÃ¡ trá»‹ nÃ y Ä‘Æ°á»£c tÃ­nh toÃ¡n sá»­ dá»¥ng cÃ¡c giÃ¡ trá»‹ (ğœ‡_batch ,Â Ïƒ_batch) Ä‘Æ°á»£c tÃ­nh trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n, vÃ  input tháº³ng vÃ o cÃ´ng thá»©c (3) á»Ÿ trÃªn, bá» qua bÆ°á»›c (1) vÃ  (2)\n\nLÆ°u Ã½: ChÃºng ta sáº½ tÃ¬m hiá»ƒu ká»¹ hÆ¡n váº¥n Ä‘á» nÃ y á»Ÿ pháº§n â€œChuáº©n hoÃ¡ khi Ä‘Ã¡nh giÃ¡â€\n\n\n\n2.2.3 Thá»±c táº¿\nTrong thá»±c táº¿, chÃºng ta xem BN nhÆ° má»™t lá»›p bÃ¬nh thÆ°á»ng, nhÆ° lÃ  má»™t perceptron, convultional layer, hay má»™t hÃ m kÃ­ch hoáº¡t hoáº·c má»™t lá»›p dropout.\nCÃ¡c ná»n táº£ng thÃ´ng dá»¥ng cÅ©ng Ä‘Ã£ triá»ƒn khai BN nhÆ° má»™t layer. VÃ­ dá»¥:\n\nPytorch: torch.nn.BatchNorm1d, torch.nn.BatchNorm2d, torch.nn.BatchNorm3d\nTensorflow / Keras: tf.nn.batch_normalization, tf.keras.layers.BatchNormalization\n\nTáº¥t cáº£ cÃ¡c cÃ¡ch triá»ƒn khai cá»§a BN Ä‘á»u cho phÃ©p báº¡n cáº¥u hÃ¬nh tham sá»‘ má»™t cÃ¡ch Ä‘á»™c láº­p. Tuy nhiÃªn, kÃ­ch cá»¡ cá»§a vector Ä‘áº§u vÃ o lÃ  quan tá»ng nháº¥t. NÃ³ nÃªn Ä‘Æ°á»£c thiáº¿t láº­p báº±ng:\n\nSá»‘ nÆ¡-ron cá»§a lá»›p áº©n hiá»‡n táº¡i (cho MLP)\nSá»‘ filters cá»§a lá»›p áº©n hiá»‡n táº¡i (cho CNN)\n\nHÃ£y Ä‘á»c tÃ i liá»‡u vá» ná»n táº£ng yÃªu thÃ­ch cá»§a báº¡n vá» BN Ä‘á»ƒ biáº¿t chi tiáº¿t hÆ¡n vá» cÃ¡ch sá»­ dá»¥ng vÃ  triá»ƒn khai.\n\n\n2.2.4 Tá»•ng quan káº¿t quáº£\nKá»ƒ cáº£ khi chÃºng ta chÆ°a hiá»ƒu táº¥t cáº£ cáº¥u táº¡o cá»§a Batch Normalization, cÃ³ má»™t thá»© mÃ  ai cÅ©ng pháº£i cÃ´ng nháº­n: NÃ³ ráº¥t hiá»‡u quáº£!\nÄá»ƒ hiá»ƒu thÃªm, hÃ£y xem káº¿t quáº£ cá»§a bÃ i bÃ¡o gá»‘c [1]:\n\n\n\n\náº¢nh 1: Hiá»‡u quáº£ cá»§a BN. Äá»™ chÃ­nh xÃ¡c trÃªn táº­p Ä‘Ã¡nh giÃ¡ cá»§a ImageNet(2012) theo sá»‘ láº§n huáº¥n luyá»‡n. NÄƒm networks Ä‘Æ°á»£c so sÃ¡nh: â€œInceptionâ€ lÃ  network Inception gá»‘c [3], â€œBN-Xâ€ lÃ  Inception network thÃªm BN (vá»›i 3 learning rates: x1, x5, x30 láº§n Inception tá»‘i Æ°u), â€œBN-X-Sigmoidâ€ lÃ  Inception network thÃªm BN, nhÆ°ng thay ReLU báº±ng Sigmoid\n\nKáº¿t quáº£ ráº¥t rÃµ rÃ ng: CÃ¡c lá»›p BN tÄƒng tá»‘c quÃ¡ trÃ¬nh luáº¥n luyá»‡n, há»— trá»£ tá»‘t nhiá»u giÃ¡ trá»‹ learning rate nhÆ°ng láº¡i khÃ´ng hi sinh kháº£ nÄƒng há»™i tá»¥ cá»§a mÃ´ hÃ¬nh.\n\nLÆ°u Ã½: Äá»c Ä‘áº¿n Ä‘Ã¢y lÃ  Ä‘á»§ cho báº¡n Ä‘á»ƒ á»©ng dá»¥ng BN rá»“i. Tuy nhiÃªn, Ä‘á»ƒ táº­n dá»¥ng tá»‘i Ä‘a Ä‘Æ°á»£c BN thÃ¬ chÃºng ta cáº§n Ä‘Ã o sÃ¢u hÆ¡n ná»¯a.\n\n\n\n\n\nBatch Normalization liÃªn quan gÃ¬ Ä‘áº¿n hÃ¬nh áº£nh nÃ y nhá»‰? | Nguá»“n : author - Design : Danilo Alvesd"
  },
  {
    "objectID": "articles/translate-batch-normalization-in-3-levels-of-understanding.html#triá»ƒn-khai",
    "href": "articles/translate-batch-normalization-in-3-levels-of-understanding.html#triá»ƒn-khai",
    "title": "3 Cáº¥p Ä‘á»™ hiá»ƒu vá» Batch Normalization (BÃ i dá»‹ch)",
    "section": "3.1 Triá»ƒn khai",
    "text": "3.1 Triá»ƒn khai\nTÃ¡c giáº£ Ä‘Ã£ triá»ƒn khai cÃ¡c lá»›p BN báº±ng Pytorch Ä‘á»ƒ tÃ¡i táº¡o káº¿t quáº£ tá»« bÃ i bÃ¡o gá»‘c. MÃ£ nguá»“n á»Ÿ trÃªn repo nÃ y\nCÃ¡c báº¡n cÅ©ng nÃªn tham tháº£o thÃªm cÃ¡c cÃ¡ch triá»ƒn khai BN khÃ¡c nhau, nÃ³ sáº½ ráº¥t cÃ³ Ã­ch khi báº¡n tháº¥y Ä‘Æ°á»£c cÃ¡ch cÃ¡c ná»n táº£ng DL láº­p trÃ¬nh BN nhÆ° tháº¿ nÃ o.\n\n3.1.1 CÃ¡c lá»›p BN trong thá»±c táº¿\nTrÆ°á»›c khi Ä‘i vÃ o lÃ½ thuyáº¿t, chÃºng ta sáº½ tÃ³m láº¡i vÃ i Ä‘iá»u vá» BN:\n\nBN áº£nh hÆ°á»Ÿng tháº¿ nÃ o Ä‘áº¿n hiá»‡u nÄƒng huáº¥n luyá»‡n? Táº¡i sao BN láº¡i quan trá»ng nhÆ° váº­y trong Deep Learning?\nBN cÃ³ nhá»¯ng tÃ¡c dá»¥ng phá»¥ nÃ o mÃ  chÃºng ta cáº§n lÆ°u tÃ¢m?\nKhi nÃ o cáº§n dÃ¹ng BN vÃ  dÃ¹ng nhÆ° tháº¿ nÃ o?\n\n\n\n3.1.2 Káº¿t quáº£ tá»« bÃ i bÃ¡o gá»‘c\nNhÆ° Ä‘Ã£ nÃ³i á»Ÿ trÃªn, BN Ä‘Æ°á»£c sá»­ dá»¥ng rá»™ng rÃ£i vÃ¬ háº§u nhÆ° lÃºc nÃ o nÃ³ cÅ©ng cáº£i thiá»‡n hiá»‡u nÄƒng cá»§a cÃ¡c mÃ´ hÃ¬nh há»c sÃ¢u.\nBÃ i bÃ¡o gá»‘c thá»±c hiá»‡n 3 thÃ­ nghiá»‡m Ä‘á»ƒ minh hoáº¡ tháº¥y ráº±ng phÆ°Æ¡ng phÃ¡p cá»§a há» hiá»‡u quáº£ tháº¿ nÃ o.\nÄáº§u tiÃªn, há» hÃ¢uns luyá»‡n má»™t mÃ´ hÃ¬nh phÃ¢n loáº¡i trÃªn tá»‡p dá»¯ liá»‡u MNIST (chá»¯ sá»‘ viáº¿t tay). MÃ´ hÃ¬nh cÃ³ 3 lá»›p fully-connected, má»—i lá»›p gá»“m 100 nÆ¡ ron, cÃ¹ng vá»›i kÃ­ch hoáº¡t sigmoid. Há» huáº¥n luyá»‡n mÃ´ hÃ¬nh nÃ y 2 láº§n (cÃ³ / khÃ´ng thÃªm BN) trong 50,000 láº§n láº·p vá»›i SGG, vÃ  learning rate nhÆ° nhau (0.01). Táº¥t cáº£ cÃ¡c lá»›p BN Ä‘á»u Ä‘áº·t ngay sau hÃ m kÃ­ch hoáº¡t.\nBáº¡n cÃ³ thá»ƒ dá»… dÃ ng tÃ¡i táº¡o káº¿t quáº£ nÃ y mÃ  khÃ´ng cáº§n GPU, Ä‘Ã¢y lÃ  má»™t cÃ¡ch tuyá»‡t vá»i Ä‘á»ƒ lÃ m quen vá»›i khÃ¡i niá»‡m nÃ y.\n\n\n\n\nHÃ¬nh 2: áº¢nh hÆ°á»Ÿng cá»§a BN lÃªn quÃ¡ trÃ¬nh huáº¥n luyá»‡n máº¡ng MLP Ä‘Æ¡n giáº£n | TrÃ¡i: Ä‘á»™ chÃ­nh xÃ¡c khi huáº¥n luyá»‡n | Pháº£i: Ä‘á»™ sai (loss) khi huáº¥n luyá»‡n | Nguá»“n : tÃ¡c gá»‰a\n\nRáº¥t tá»‘t! BN nÃ¢ng cao hiá»‡u nÄƒng cá»§a mÃ´ hÃ¬nh, cáº£ á»Ÿ Ä‘á»™ chÃ­nh xÃ¡c láº«n Ä‘á»™ sai.\nThÃ­ nghiá»‡m thá»© hai lÃ  vá» giÃ¡ trá»‹ kÃ­ch hoáº¡t cá»§a lá»›p áº©n. Sau Ä‘Ã¢y lÃ  Ä‘á»“ thá»‹ giÃ¡ trá»‹ cá»§a lá»›p áº©n cuá»‘i (ngay trÆ°á»›c khi Ã¡p dá»¥ng hÃ m phi tuyáº¿n tÃ­nh nhÆ° ReLU hay sigmoid, v.v):\n\n\n\n\náº¢nh hÆ°á»Ÿng cá»§a BN lÃªn giÃ¡ trá»‹ kÃ­ch hoáº¡t | Nguá»“n : tÃ¡c giáº£\n\nKhi khÃ´ng cÃ³ BN, giÃ¡ trá»‹ kÃ­ch hoáº¡t giao Ä‘á»™ng nhiá»u hÆ¡n vá»›i nhá»¯ng láº§n láº·p Ä‘áº§u tiÃªn. NgÆ°á»£c láº¡i, khi cÃ³ BN, Ä‘Æ°á»ng cong cá»§a giÃ¡ trá»‹ kÃ­ch hoáº¡t mÆ°á»£t hÆ¡n.\n\n\n\n\náº¢nh hÆ°á»Ÿng cá»§a BN lÃªn giÃ¡ trá»‹ kÃ­ch hoáº¡t | MÃ´ hÃ¬nh cÃ³ giÃ¡ trá»‹ kÃ­ch hoáº¡t dao Ä‘á»™ng mÆ°á»£t hÆ¡n khi thÃªm BN | Nguá»“n : tÃ¡c giáº£\n\nTÃ­n hiá»‡u cÅ©ng trá»Ÿ nÃªn Ã­t nhiá»…u hÆ¡n khi thÃªm lá»›p BN. CÃ³ váº» nhÆ° BN lÃ m mÃ´ hÃ¬nh há»™i tá»¥ dá»… hÆ¡n.\nVÃ­ dá»¥ nÃ y váº«n chÆ°a minh hoáº¡ Ä‘Æ°á»£c háº¿t lá»£i Ã­ch cá»§a Batch Normalization.\nBÃ i bÃ¡o gá»‘c thá»±c thiá»‡n thÃªm thÃ­ nghiá»‡m thá»© 3. NhÃ³m tÃ¡c giáº£ muá»‘n so sÃ¡nh hiá»‡u nÄƒng cá»§a mÃ´ hÃ¬nh khi thÃªm BN vá»›i bá»™ dá»¯ liá»‡u lá»›n hÆ¡n: ImageNet(2012). Äá»ƒ lÃ m váº­y, há» huáº¥n luyá»‡n má»™t máº¡ng nÆ¡-ron ráº¥t mÃ£nh máº½ lÃ  Inception. Ban Ä‘áº§u, Inception khÃ´ng sá»­ dá»¥ng báº¥t ká»³ lá»›p BN nÃ o. Há» thÃªm vÃ i lá»›p BN vÃ  huáº¥n luyá»‡n vá»›i nhiá»u má»©c learning rate khÃ¡c nhau (x1, x5, x30 láº§n giÃ¡ trá»‹ tá»‘i Æ°u trÆ°á»›c Ä‘Ã³). Há» cÅ©ng thá»­ nghiá»‡m thay táº¥t cáº£ hÃ m ReLU báº±ng sigmoid trong má»™t mÃ´ hÃ¬nh khÃ¡c. Cuá»‘i cÃ¹ng, há» so sÃ¡nh nhá»¯ng network Ä‘Ã£ Ä‘Æ°á»£c thay Ä‘á»•i vá»›i mÃ´ hÃ¬nh gá»‘c.\n\n\n\n\náº¢nh 1: Hiá»‡u quáº£ cá»§a BN. Äá»™ chÃ­nh xÃ¡c trÃªn táº­p Ä‘Ã¡nh giÃ¡ cá»§a ImageNet(2012) theo sá»‘ láº§n huáº¥n luyá»‡n. NÄƒm networks Ä‘Æ°á»£c so sÃ¡nh: â€œInceptionâ€ lÃ  network Inception gá»‘c [3], â€œBN-Xâ€ lÃ  Inception network thÃªm BN (vá»›i 3 learning rates: x1, x5, x30 láº§n Inception tá»‘i Æ°u), â€œBN-X-Sigmoidâ€ lÃ  Inception network thÃªm BN, nhÆ°ng thay ReLU báº±ng Sigmoid\n\nChÃºng ta cÃ³ thá»ƒ káº¿t luáº­n nhÆ° sau:\n\nThÃªm lá»›p BN giÃºp há»™i tá»¥ nhanh hÆ¡n vÃ  tá»‘t hÆ¡n (~ Ä‘á»™ chÃ­nh xÃ¡c cao hÆ¡n)\n\nVá»›i bá»™ dá»¯ liá»‡u lá»›n, nhá»¯ng cáº£i thiá»‡n nÃ y cÃ ng quan trá»ng hÆ¡n lÃ  khi sá»­ dá»¥ng nhá»¯ng bá»™ dá»¯ liá»‡u nhá» hÆ¡n nhÆ° MNIST.\n\nThÃªm lá»›p BN cho phÃ©p chÃºng ta sá»­ dá»¥ng learning rate lá»›n hÆ¡n nhÆ°ng láº¡i khÃ´ng hi sinh tÃ­nh há»™i tá»¥ cá»§a mÃ´ hÃ¬nh.\n\nNhÃ³m tÃ¡c giáº£ tháº­m chÃ­ cÃ²n huáº¥n luyá»‡n thÃ nh cÃ´ng mÃ´ hÃ¬nh Inception káº¿t há»£p BN vá»›i learning rate lá»›n gáº¥p 30 láº§n so vá»›i mÃ´ hÃ¬nh gá»‘c. Tháº­t áº¥n tÆ°á»£ng, khi chá»‰ vá»›i x5 learning rate thÃ¬ mÃ´ hÃ¬nh gá»‘c Ä‘Ã£ phÃ¢n tÃ¡n rá»“i.\nBáº±ng cÃ¡ch Ä‘Ã³, BN lÃ m cho quÃ¡ trÃ¬nh tÃ¬m learning rate tá»‘t dá»… dÃ ng hÆ¡n khi khoáº£ng cÃ¡ch tá»« underfit Ä‘áº¿n gradient explosion cá»§a learning rate Ä‘Æ°á»£c tÄƒng lÃªn Ä‘Ã¡ng ká»ƒ.\nThÃªm ná»¯a, learning rate cao hÆ¡n giÃºp mÃ´ hÃ¬nh thoÃ¡t há»i giÃ¡ trá»‹ tá»‘i Æ°u cá»¥c bá»™. Nhá» váº­y mÃ  optimizer dá»… dÃ ng tÃ¬m Ä‘Æ°á»£c nghiá»‡m tá»‘t hÆ¡n cho quÃ¡ trÃ¬nh há»™i tá»¥.\n\nMÃ´ hÃ¬nh sá»­ dá»¥ng sigmoid cÃ³ káº¿t quáº£ khÃ¡ cáº¡nh tranh so vá»›i mÃ´ hÃ¬nh sá»­ dá»¥ng ReLU\n\ná» má»™t bá»©c tranh tá»•ng quan hÆ¡n, chÃºng ta tháº¥y ráº±ng mÃ´ hÃ¬nh ReLU cÃ³ hiá»‡u nÄƒng khÃ¡ hÆ¡n má»™t chÃºt so vá»›i mÃ´ hÃ¬nh dÃ¹ng sigmoid. Tuy nhiÃªn Ä‘Ã³ khÃ´ng pháº£i lÃ  Ä‘iá»u quan trá»ng nháº¥t.\nTÃ¡c giáº£ Ian Goodfellow (tÃ¡c giáº£ cá»§a GAN) tá»«ng nÃ³i vá» BN:\n\nTrÆ°á»›c BN, chÃºng ta nghÄ© ráº±ng huáº¥n luyá»‡n mÃ´ hÃ¬nh há»c sÃ¢u vá»›i hÃ m kÃ­ch hoáº¡t sigmoid á»Ÿ cÃ¡c táº§ng áº©n lÃ  khÃ´ng thá»ƒ. ChÃºng ta xem xÃ©t má»™t vÃ i hÆ°á»›ng tiáº¿p cáº­n Ä‘á»ƒ xá»­ lÃ½ sá»± báº¥t á»•n Ä‘á»‹nh khi huáº¥n luyá»‡n, vÃ­ dá»¥ nhÆ° cÃ¡c phÆ°Æ¡ng phÃ¡p khá»Ÿi táº¡o tham sá»‘. Nhá»¯ng máº£nh ghÃ©p nÃ y dá»±a nhiá»u vÃ o kinh nghiá»‡m cÅ©ng nhÆ° ráº¥t khÃ³ Ä‘á»ƒ cho ra káº¿t quáº£ thoáº£ mÃ£n. Batch Normalization cho phÃ©p chÃºng ta huáº¥n luyá»‡n Ä‘Æ°á»£c cáº£ nhá»¯ng mÃ´ hÃ¬nh báº¥t á»•n Ä‘á»‹nh. ÄÃ³ chÃ­nh lÃ  nhá»¯ng gÃ¬ chÃºng ta tháº¥y Ä‘Æ°á»£c tá»« vÃ­ dá»¥ nÃ y. - Ian Goodfellows (tÃ¡c giáº£ viáº¿t láº¡i theo nguá»“n: https://www.youtube.com/watch?v=Xogn6veSyxA)\n\nÄáº¿n Ä‘Ã¢y, chÃºng ta Ä‘Ã£ hiá»ƒu Ä‘Æ°á»£c BN cÃ³ Ã½ nghÄ©a quan trá»ng nhÆ° tháº¿ nÃ o trong lÄ©nh vá»±c Há»c sÃ¢u.\nNhá»¯ng káº¿t quáº£ trÃªn táº¡o nÃªn má»™t bá»©c tranh tá»•ng quan vá» lá»£i Ã­ch cá»§a BN khi huáº¥n luyá»‡n mÃ´ hÃ¬nh. Tuy nhiÃªn, BN cÃ³ nhá»¯ng tÃ¡c dá»¥ng phá»¥ mÃ  chÃºng ta cáº§n lÆ°u tÃ¢m Ä‘á»ƒ táº­n dá»¥ng Ä‘Æ°á»£c nÃ³.\n\n\n3.1.3 TÃ¡c dá»¥ng phá»¥ cá»§a BN: Regularization\nBN phá»¥ thuá»™c vÃ o trung bÃ¬nh ğœ‡ vÃ  phÆ°Æ¡ng sai ÏƒÂ² Ä‘á»ƒ chuáº©n hoÃ¡ giÃ¡ trá»‹ kÃ­ch hoáº¡t. VÃ¬ tháº¿ mÃ  káº¿t quáº£ Ä‘áº§u ra cá»§a BN sáº½ bá»‹ áº£nh hÆ°á»Ÿng vá»›i thá»‘ng kÃª cá»§a batch hiá»‡n táº¡i. Nhá»¯ng sá»± biáº¿n Ä‘á»•i sáº½ táº¡o thÃªm nhiá»…u, phá»¥ thuá»™c vÃ o dá»¯ liá»‡u Ä‘áº§u vÃ o cá»§a batch hiá»‡n táº¡i.\nViá»‡c thÃªm nhiá»…u cÅ©ng sáº½ giÃºp trÃ¡nh overfittingâ€¦ nghe khÃ¡ giá»‘ng vá»›i regularization, Ä‘Ãºng khÃ´ng nhá»‰?\nTrong thá»±c nghiá»‡m, chÃºng ta sáº½ khÃ´ng dá»±a vÃ o BN Ä‘á»ƒ xá»­ lÃ½ overfitting, vÃ¬ sá»± quan trá»ng cá»§a tÃ­nh trá»±c giao (orthogonality). NÃ³i má»™t cÃ¡ch Ä‘Æ¡n giáº£n, má»—i module nÃªn chá»‰ Ä‘áº£m nhiá»‡m má»™t nhiá»‡m vá»¥. Äiá»u nÃ y giÃºp trÃ¡nh viá»‡c phá»©c táº¡p hoÃ¡ quy trÃ¬nh phÃ¡t triá»ƒn.\nTuy nhiÃªn, viá»‡c biáº¿t Ä‘áº¿n tÃ¡c dá»¥ng phá»¥ nÃ y giÃºp chÃºng ta giáº£i thÃ­ch Ä‘Æ°á»£c nhá»¯ng hÃ nh vi ngoÃ i mong Ä‘á»£i cá»§a mÃ´ hÃ¬nh.\n\nLÆ°u Ã½: Khi kÃ­ch thÆ°á»›c batch cÃ ng lá»›n thÃ¬ tÃ¡c Ä‘á»™ng lÃªn regularization cÃ ng Ã­t Ä‘i (do nhiá»…u áº£nh hÆ°á»Ÿng Ã­t hÆ¡n)\n\n\n\n\n\nLÃ m sao Ä‘á»ƒ deploy Ä‘Æ°á»£c mÃ´ hÃ¬nh cÃ³ BN lÃªn há»‡ thá»‘ng nhÃºng? | Nguá»“n : MarÃ­lia Castelli\n\n\n\n3.1.4 Chuáº©n hoÃ¡ khi Ä‘Ã¡nh giÃ¡\nCÃ³ hai trÆ°á»ng há»£p mÃ  mÃ´ hÃ¬nh Ä‘Æ°á»£c gá»i lÃ  Ä‘ang á»Ÿ cháº¿ Ä‘á»™ Ä‘Ã¡nh giÃ¡ (evaluation mode):\n\nKhi Ä‘ang cross-validation hay test (huáº¥n luyá»‡n vÃ  phÃ¡t triá»ƒn mÃ´ hÃ¬nh)\nKhi Ä‘ang deploy mÃ´ hÃ¬nh\n\ná» trÆ°á»ng há»£p Ä‘áº§u, chÃºng ta cÃ³ thá»ƒ Ã¡p dá»¥ng Batch Normalization vá»›i thá»‘ng kÃª tá»« batch Ä‘ang xá»­ lÃ½. Tuy nhiÃªn, á»Ÿ trÆ°á»ng há»£p sau thÃ¬ cÃ¡ch nÃ y khÃ´ng Ã¡p dá»¥ng Ä‘Æ°á»£c, vÃ¬ chÃºng ta khÃ´ng cÃ³ Ä‘á»§ má»™t batch Ä‘á»ƒ sá»­ dá»¥ng.\nHÃ£y cÃ¹ng xem xÃ©t trÆ°á»ng há»£p má»™t robot vá»›i camera nhÃºng. CÃ³ thá»ƒ chÃºng ta sáº½ sá»­ dá»¥ng mÃ´ hÃ¬nh Ä‘á»ƒ dá»± Ä‘oÃ¡n vá»‹ trÃ­ cá»§a váº­t cáº£n Ä‘Æ°á»ng phÃ­a trÆ°á»›c. ChÃºng ta muá»‘n tÃ­nh toÃ¡n dá»± Ä‘oÃ¡n dá»±a trÃªn 1 khung áº£nh duy nháº¥t cho má»—i iteration. Náº¿u kÃ­ch cá»¡ batch lÃ  N, thÃ¬ N-1 inputs cÃ²n láº¡i thÃ¬ chÃºng ta nÃªn chá»n nhÆ° tháº¿ nÃ o Ä‘á»ƒ tÃ­nh toÃ¡n forward propagation?\nNhá»› ráº±ng vá»›i má»—i lá»›p BN, (ğ›½, ğ›¾) Ä‘Æ°á»£c huáº¥n luyá»‡n báº±ng tÃ­n hiá»‡u Ä‘Ã£ chuáº©n hoÃ¡. Tháº¿ nÃªn chÃºng ta cáº§n xÃ¡c Ä‘á»‹nh (ğœ‡, Ïƒ) Ä‘á»ƒ táº¡o ra káº¿t quáº£ cÃ³ Ã½ nghÄ©a.\nMá»™t giáº£i phÃ¡p lÃ  chá»n giÃ¡ trá»‹ ngáº«u nhiÃªn Ä‘á»ƒ Ä‘iá»n cho Ä‘á»§ batch. Khi Ä‘Æ°a batch Ä‘áº§u tiÃªn vÃ o, chÃºng ta sáº½ cÃ³ má»™t káº¿t quáº£ cho áº£nh mÃ  chÃºng ta quan tÃ¢m. Náº¿u chung ta táº¡o thÃªm batch thá»© hai vá»›i giÃ¡ trá»‹ ngáº«u nhiÃªn khÃ¡c, mÃ´ hÃ¬nh sáº½ cho dá»± Ä‘oÃ¡n khÃ¡c trÃªn cÃ¹ng má»™t áº£nh. ÄÃ¢y khÃ´ng pháº£i lÃ  hÃ nh vi mong muá»‘n vÃ¬ mÃ´ hÃ¬nh cÃ³ thá»ƒ dá»± Ä‘oÃ¡n khÃ¡c nhau cho cÃ¹ng má»™t input.\nCÃ¡ch giáº£i quyáº¿t tá»‘t hÆ¡n lÃ  xÃ¡c Ä‘á»‹nh (ğœ‡_pop , Ïƒ_pop) - Æ°á»›c lÆ°á»£ng trung bÃ¬nh vÃ  Ä‘á»™ lá»‡ch chuáº©n cá»§a quáº§n thá»ƒ mÃ  ta nháº¯m Ä‘áº¿n. ThÃ´ng sá»‘ nÃ y Ä‘Æ°á»£c tÃ­nh báº±ng trung bÃ¬nh cá»§a (ğœ‡_batch, Ïƒ_batch) trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n.\n\nCÃ¡ch nÃ y cÃ³ thá»ƒ dáº«n Ä‘áº¿n sá»± báº¥t á»•n Ä‘á»‹nh trong quÃ¡ trÃ¬nh Ä‘Ã¡nh giÃ¡: ChÃºng ta sáº½ tháº£o luáº­n Ä‘iá»u nÃ y á»Ÿ pháº§n tiáº¿p theo\n\n\n\n3.1.5 TÃ­nh á»•n Ä‘á»‹nh cá»§a BN layer\nTuy BN khÃ¡ hiá»‡u quáº£, nÃ³ cÅ©ng cÃ³ thá»ƒ Ä‘Ã´i khi gÃ¢y ra váº¥n Ä‘á» vá» á»•n Ä‘á»‹nh. CÃ³ trÆ°á»ng há»£p BN lÃ m giÃ¡ trá»‹ kÃ­ch hoáº¡t bá»‹ explode khi Ä‘Ã¡nh giÃ¡ (khiáº¿n loss=NaN).\nNhÆ° á»Ÿ trÃªn cÃ³ Ä‘á» cáº­p, (ğœ‡_pop , Ïƒ_pop) trong quÃ¡ trÃ¬nh Ä‘Ã¡nh giÃ¡ Ä‘Æ°á»£c dá»±a trÃªn (ğœ‡_batch, Ïƒ_batch) khi huáº¥n luyá»‡n.\nThá»­ tÆ°á»Ÿng tÆ°á»£ng má»™t mÃ´ hÃ¬nh chá»‰ Ä‘Æ°á»£c huáº¥n luyá»‡n báº±ng tá»‡p áº£nh giÃ y thá»ƒ thao nhÆ°ng khi test láº¡i báº±ng táº­p giÃ y derby (kiá»ƒu giÃ y TÃ¢y)?\n\n\n\n\nNáº¿u phÃ¢n phá»‘i cá»§a Ä‘áº§u vÃ o quÃ¡ khÃ¡c giá»¯a khi huáº¥n luyá»‡n vÃ  Ä‘Ã¡nh giÃ¡, mÃ´ hÃ¬nh cÃ³ thá»ƒ pháº£n á»©ng thÃ¡i quÃ¡ vá»›i má»™t vÃ i tÃ­n hiá»‡u, dáº«n Ä‘áº¿n sá»± phÃ¢n tÃ¡n cá»§a giÃ¡ trá»‹ kÃ­ch hoáº¡t | Nguá»“n : Grailify & Jia Ye\n\nGiáº£ sá»­ giÃ¡ trá»‹ kÃ­ch hoáº¡t á»Ÿ lá»›p áº©n cÃ³ phÃ¢n phá»‘i quÃ¡ khÃ¡c biá»‡t giá»¯a khi huáº¥n luyá»‡n vÃ  khi Ä‘Ã¡nh giÃ¡, (ğœ‡_pop, Ïƒ_pop) sáº½ khÃ´ng Æ°á»›c lÆ°á»£ng Ä‘Ãºng Ä‘Æ°á»£c trung bÃ¬nh vÃ  Ä‘á»™ lá»‡ch chuáº©n cá»§a quáº§n thá»ƒ. Sá»­ dá»¥ng bá»™ giÃ¡ trá»‹ nÃ y sáº½ Ä‘áº©y lá»‡ch giÃ¡ trá»‹ kÃ­ch hoáº¡t xa khá»i phÃ¢n phá»‘i chuáº©n (ğœ‡ = 0, Ïƒ = 1) -> Ä‘Ã¡nh giÃ¡ sai giÃ¡ trá»‹ kÃ­ch hoáº¡t.\n\nHiá»‡n tÆ°á»£ng nÃ y gá»i lÃ  â€œcovariate shiftâ€, sáº½ nÃ³i á»Ÿ pháº§n sau\n\nHiá»‡u á»©ng trÃªn cÃ²n Ä‘Æ°á»£c tÄƒng cÆ°á»ng bá»Ÿi má»™t thuá»™c tÃ­nh cá»§a BN: Trong khi huáº¥n luyá»‡n, giÃ¡ trá»‹ kÃ­ch hoáº¡t Ä‘Æ°á»£c chuáº©n hoÃ¡ bá»Ÿi chÃ­nh giÃ¡ trá»‹ cá»§a nÃ³. CÃ²n khi inference, thÃ¬ tÃ­n hiá»‡u láº¡i cÅ©ng Ä‘Æ°á»£c sá»­ dá»¥ng giÃ¡ trá»‹ (ğœ‡_pop, Ïƒ_pop) Ä‘Æ°á»£c tÃ­nh khi á»Ÿ tranining. Tháº¿ nÃªn, há»‡ sá»‘ cá»§a viá»‡c chuáº©n hoÃ¡ khÃ´ng bao gá»“m nhá»¯ng giÃ¡ trá»‹ kÃ­ch hoáº¡t.\nNÃ³i chung, táº­p huáº¥n luyá»‡n pháº£i â€œÄ‘á»§ giá»‘ngâ€ vá»›i tá»‡p Ä‘Ã¡nh giÃ¡: Náº¿u khÃ´ng, viá»‡c huáº¥n luyá»‡n mÃ´ hÃ¬nh gáº§n nhÆ° lÃ  khÃ´ng thá»ƒ. Trong Ä‘a sá»‘ trÆ°á»ng há»£p, (ğœ‡_pop, Ïƒ_pop) cÅ©ng nÃªn khá»›p vá»›i bá»™ dá»¯ liá»‡u Ä‘Ã¡nh giÃ¡. Náº¿u khÃ´ng Ä‘Æ°á»£c váº­y thÃ¬ chÃºng ta sáº½ káº¿t luáº­n lÃ  táº­p huáº¥n luyá»‡n khÃ´ng Ä‘á»§ lá»›n, hoáº·c cháº¥t lÆ°á»£ng cá»§a dá»¯ liá»‡u khÃ´ng Ä‘á»§ tá»‘t cho tÃ¡c vá»¥ má»¥c tiÃªu.\nTÃ¡c giáº£ cÅ©ng tá»«ng gáº·p trÆ°á»ng há»£p nÃ y trong cuá»™c thi Pulmonary Fibrosis Progression Kaggle competition. Táº­p huáº¥n luyá»‡n bao gá»“m metadata, 3D scans phá»•i cá»§a tá»«ng bá»‡nh nhÃ¢n. Ná»™i dung cá»§a nhá»¯ng báº£n scans nÃ y phá»©c táº¡p vÃ  phong phÃº, tuy nhiÃªn nÃ³ chá»‰ tá»« gáº§n 100 bá»‡nh nhÃ¢n Ä‘á»ƒ chia thÃ nh train vÃ  validation. Káº¿t quáº£ lÃ  CNN dÃ¹ng Ä‘á»ƒ trÃ­ch xuáº¥t Ä‘áº·c trÆ°ng mÃ  tÃ¡c giáº£ dÃ¹ng chá»‰ tráº£ vá» NaN khi mÃ´ hÃ¬nh chuyá»ƒn tá»« huáº¥n luyá»‡n sang Ä‘Ã¡nh giÃ¡.\nKhi báº¡n khÃ´ng thá»ƒ láº¥y thÃªm dá»¯ liá»‡u Ä‘á»ƒ bá»• sung cho huáº¥n luyá»‡n, báº¡n cáº§n pháº£i tÃ¬m cÃ¡ch khÃ¡c. Trong trÆ°á»ng há»£p nÃ y, tÃ¡c giáº£ â€œÃ©pâ€ cÃ¡c lá»›p BN pháº£i tÃ­nh láº¡i (ğœ‡_batch, Ïƒ_batch) khi Ä‘Ã¡nh giÃ¡. (tÃ¡c giáº£ tá»± nháº­n Ä‘Ã¢y lÃ  cÃ¡ch hÆ¡i â€œxáº¥u xÃ­â€, nhÆ°ng anh ta khÃ´ng cÃ²n thá»i gian).\nThÃªm cÃ¡c lá»›p BN vÃ o mÃ´ hÃ¬nh mÃ  tá»± giáº£ Ä‘á»‹nh ráº±ng nÃ³ khÃ´ng cÃ³ áº£nh hÆ°á»Ÿng xáº¥u khÃ´ng pháº£i lÃºc nÃ o cÅ©ng lÃ  má»™t Ä‘iá»u tá»‘t.\n\n\n3.1.6 Recurrent Network vÃ  Layer Normalization\nTrong thá»±c nghiá»‡m, nhiá»u ngÆ°á»i nháº­n Ä‘á»‹nh ráº±ng:\n\nTrong CNN: Batch Normalization (BN) phÃ¹ há»£p hÆ¡n\nTrong RNN: Layer Normalization (LN) phÃ¹ há»£p hÆ¡n\n\nTrong khi BN dÃ¹ng batch hiá»‡n táº¡i Ä‘á»ƒ chuáº©n hoÃ¡ tá»«ng giÃ¡ trá»‹, LN thÃ¬ dÃ¹ng táº¥t cáº£ cÃ¡c layers hiá»‡n táº¡i. NÃ³i cÃ¡ch khÃ¡c, LN chuáº©n hoÃ¡ trÃªn toÃ n bá»™ cÃ¡c Ä‘áº·c trÆ°ng cá»§a dá»¯ liá»‡u thay vÃ¬ theo tá»«ng Ä‘áº·c trÆ°ng nhÆ° BN. Äiá»u nÃ y lÃ m LN hiá»‡u quáº£ hÆ¡n vá»›i RNN. Viá»‡c Ä‘Æ°a má»™t phÆ°Æ¡ng phÃ¡p nháº¥t quÃ¡n cho RNN khÃ¡ lÃ  khÃ³, vÃ¬ RNN sá»­ dá»¥ng phÃ©p nhÃ¢n láº·p Ä‘i láº·p láº¡i vá»›i cÃ¹ng má»™t bá»™ trá»ng sá»‘. Váº­y chÃºng ta nÃªn chuáº©n hoÃ¡ theo tá»«ng step má»™t cÃ¡ch Ä‘á»™c láº­p? Hay lÃ  nÃªn tÃ­nh mean trÃªn toÃ n steps, hay lÃ  chuáº©n hoÃ¡ theo quy há»“i? (Tham kháº£o: YouTube)\nCÃ¢u há»i nÃ y náº±m ngoÃ i pháº¡m vi cá»§a bÃ i viáº¿t nÃ y.\n\n\n3.1.7 TrÆ°á»›c hay sau phi tuyáº¿n tÃ­nh?\nTá»« trÆ°á»›c Ä‘áº¿n nay, lá»›p BN thÆ°á»ng Ä‘áº·t ngay sau hÃ m phi tuyáº¿n tÃ­nh, Ä‘Ã¢y lÃ  cÃ¡ch lÃ m Ä‘Ãºng theo má»¥c tiÃªu vÃ  giáº£ thiáº¿t cá»§a tÃ¡c giáº£ gá»‘c:\n\nâ€œChÃºng tÃ´i muá»‘n Ä‘áº£m báº£o ráº±ng, vá»›i má»i giÃ¡ trá»‹ tham sá»‘, máº¡ng lÆ°á»›i lÃºc nÃ o cÅ©ng cho ra giÃ¡ trá»‹ kÃ­ch hoáº¡t vá»›i phÃ¢n phá»‘i mong muá»‘nâ€ â€” Sergey Ioffe & Christian Szegedy (source : [1])\n\nMá»™t vÃ i thÃ­ nghiá»‡m cho tháº¥y viá»‡c Ä‘áº·t lá»›p BN Ä‘áº±ng sau hÃ m phi tuyáº¿n tinh cho káº¿t quáº£ tá»‘t hÆ¡n. VÃ­ dá»¥\nFranÃ§ois Chollet, cha Ä‘áº» cá»§a Keras vÃ  hiá»‡n táº¡i lÃ  ká»¹ sÆ° cá»§a Google, cho ráº±ng:\n\nTÃ´i chÆ°a xem láº¡i nhá»¯ng khuyáº¿n cÃ¡o trong paper gá»‘c, nhÆ°ng tÃ´i Ä‘áº£m báº£o ráº±ng mÃ£ nguá»“n gáº§n Ä‘Ã¢y viáº¿t bá»Ÿi Christian [Szegedy] Ä‘áº·t relu trÆ°á»›c BN. Tuy nhiÃªn, váº¥n Ä‘á» nÃ y cÅ©ng hay Ä‘Æ°á»£c tranh luáº­n.\n\nVáº«n cÃ³ nhiá»u kiáº¿n trÃºc thÆ°á»ng dÃ¹ng trong transfer learning Ä‘áº·t BN trÆ°á»›c hÃ m phi tuyáº¿n tÃ­nh nhÆ° ResNet, mobilenet-v2, v.v\nNÃªn nhá»› ráº±ng trong bÃ i bÃ¡o [2], khi thÃ¡ch thá»©c nhá»¯ng giáº£ thiáº¿t cá»§a bÃ i bÃ¡o gá»‘c Ä‘á»ƒ giáº£i thÃ­ch sá»± hiá»‡u quáº£ cá»§a BN thÃ¬ tÃ¡c giáº£ láº¡i Ä‘á»ƒ lá»›p BN trÆ°á»›c hÃ m kÃ­ch hoáº¡t. Tuy nhiÃªn, tÃ¡c giáº£ láº¡i khÃ´ng Ä‘Æ°a ra nguyÃªn nhÃ¢n thuyáº¿t phá»¥c cho viá»‡c nÃ y.\nCho Ä‘áº¿n nay thÃ¬ váº¥n Ä‘á» nÃ y váº«n Ä‘Æ°á»£c tháº£o luáº­n vÃ  tranh cÃ£i. TrÃªn reddit cÅ©ng cÃ³ má»™t thread nÃ³i vá» Ä‘iá»u nÃ y."
  },
  {
    "objectID": "articles/translate-batch-normalization-in-3-levels-of-understanding.html#táº¡i-sao-bn-láº¡i-hiá»‡u-quáº£",
    "href": "articles/translate-batch-normalization-in-3-levels-of-understanding.html#táº¡i-sao-bn-láº¡i-hiá»‡u-quáº£",
    "title": "3 Cáº¥p Ä‘á»™ hiá»ƒu vá» Batch Normalization (BÃ i dá»‹ch)",
    "section": "3.2 Táº¡i sao BN láº¡i hiá»‡u quáº£?",
    "text": "3.2 Táº¡i sao BN láº¡i hiá»‡u quáº£?\nTrong Ä‘a sá»‘ trÆ°á»ng há»£p, BN gia tÄƒng hiá»‡u nÄƒng cá»§a mÃ´ hÃ¬nh há»c sÃ¢u. Äiá»u Ä‘Ã³ ráº¥t tá»‘t, nhÆ°ng chÃºng ta cáº§n hiá»ƒu thÃªm vá» nguyÃªn nhÃ¢n gá»‘c rá»… cá»§a nÃ³.\nVáº¥n Ä‘á» lÃ : chÃºng ta váº«n chÆ°a biáº¿t táº¡i sao BN láº¡i hiá»ƒu quáº£ nhÆ° váº­y. Má»™t vÃ i giáº£ thiáº¿t Ä‘Æ°á»£c Ä‘Æ°a ra trong cá»™ng Ä‘á»“ng há»c sÃ¢u vÃ  chÃºng ta sáº½ xem xÃ©t tá»«ng cÃ¡i má»™t.\nTrÆ°á»›c khi tháº£o luáº­n tiáº¿p, Ä‘Ã¢y lÃ  nhá»¯ng thá»© chÃºng ta sáº½ tháº¥y:\n\nBÃ i bÃ¡o gá»‘c [1] giáº£ Ä‘á»‹nh ráº±ng BN hiá»‡u quáº£ vÃ¬ nÃ³ giáº£m Ä‘i thá»© mÃ  há» gá»i lÃ  Internal covariate shift (ICS). Má»™t bÃ i bÃ¡o gáº§n Ä‘Ã¢y [2] Ä‘Ã£ phá»§ Ä‘á»‹nh Ä‘iá»u nÃ y.\nGiáº£i thiáº¿t tiáº¿p theo cho ráº±ng BN giáº£m thiá»ƒu sá»± phá»¥ thuá»™c láº«n nhau giá»¯a cÃ¡c layers khi huáº¥n luyá»‡n.\nGiáº£ thiáº¿t tá»« MIT [2] nháº¥n máº¡nh áº£nh hÆ°á»Ÿng cá»§a BN lÃªn optimization landscape smoothness, khiáº¿n training dá»… dÃ ng hÆ¡n.\n\nViá»‡c khÃ¡m phÃ¡ nhá»¯ng giáº£ thiáº¿t trÃªn sáº½ xÃ¢y dá»±ng cho báº¡n cÃ¡ch hiá»ƒu vá»¯ng cháº¯c hÆ¡n vá» Batch Normalization.\n\n3.2.1 Giáº£ thiáº¿t 1: BN giáº£m internal covariate shift\nCho dÃ¹ áº£nh hÆ°á»Ÿng cá»§a BN lÃ  ráº¥t lá»›n, BN váº«n lÃ  Ä‘iá»u dá»… bá»‹ hiá»ƒu sai. VÃ  Ä‘iá»u nÃ y Ä‘a pháº§n lÃ  do má»™t giáº£ thiáº¿t sai cá»§a bÃ i bÃ¡o gá»‘c [1]:\n\nâ€œChÃºng tÃ´i gá»i Ä‘áº¿n sá»± thay Ä‘á»•i cá»§a phÃ¢n phá»‘i cá»§a nhá»¯ng nodes trong má»™t máº¡ng há»c sÃ¢u khi Ä‘ang huáº¥n luyá»‡n lÃ  Internal Covariate Shift (ICS). [â€¦] ChÃºng tÃ´i Ä‘á» xuáº¥t má»™t phÆ°Æ¡ng phÃ¡p má»›i, gá»i lÃ  Batch Normalization, Ä‘á»ƒ tiáº¿n Ä‘áº¿n giáº£m thiá»ƒu internal covariate shift, vÃ  hÆ¡n ná»¯a lÃ  tÄƒng tá»‘c Ä‘Ã¡ng ká»ƒ quÃ¡ trÃ¬nh huáº¥n luyá»‡n DNNâ€. â€” Sergey Ioffe & Christian Szegedy (nguá»“n : [1])\n\nNÃ³i cÃ¡ch khÃ¡c, BN hiá»‡u quáº£ vÃ¬ nÃ³ giáº£i quyáº¿t (má»™t pháº§n) váº¥n Ä‘á» ICS.\nNháº­n Ä‘á»‹nh nÃ y gáº·p thÃ¡ch thá»©c lá»›n bá»Ÿi bÃ i bÃ¡o [2].\nÄá»ƒ hiá»ƒu Ä‘Æ°á»£c lÃ½ do gÃ¬ láº¡i dáº«n Ä‘áº¿n sá»± ráº¯c rá»‘i nÃ y, chÃºng ta sáº½ tháº£o luáº­n xem covariate shift lÃ  gÃ¬, vÃ  nÃ³ bá»‹ áº£nh hÆ°á»Ÿng tháº¿ nÃ o tá»« normalization.\n\n3.2.1.1 Covariate shift lÃ  gÃ¬?\nTÃ¡c giáº£ cá»§a [1] Ä‘á»‹nh nghÄ©a: covariate shift - á»Ÿ gÃ³c nhÃ¬n cá»§a sá»± á»•n Ä‘á»‹nh phÃ¢n phá»‘i - lÃ  sá»± di chuyá»ƒn cá»§a phÃ¢n phá»‘i dá»¯ liá»‡u dáº§u vÃ o cá»§a mÃ´ hÃ¬nh. Má»Ÿ rá»™ng hÆ¡n, internal covariate shift mÃ´ táº£ hiá»‡n tÆ°á»£ng trÃªn khi nÃ³ xáº£y ra giá»¯a cÃ¡c hidden layers (lá»›p áº©n) cá»§a má»™t máº¡ng há»c sÃ¢u.\nHÃ£y xem táº¡i sao Ä‘Ã¢y lÃ  má»™t váº¥n Ä‘Ã¨ thoong qua vÃ­ dá»¥ sau.\nGáº£i sá»­ chÃºng ta muá»‘n huáº¥n luyá»‡n má»™t mÃ´ hÃ¬nh phÃ¢n loáº¡i Ä‘á»ƒ tráº£ lá»i cÃ¢u há»i: ÄÃ¢y cÃ³ pháº£i chiáº¿c xe hÆ¡i khÃ´ng? Náº¿u chÃºng ta muá»‘n trÃ­ch xuáº¥t toÃ n bá»™ hÃ¬nh áº£nh xe hÆ¡i trong má»™t táº­p dá»¯ liá»‡u cá»±c lá»›n, mÃ´ hÃ¬nh nÃ y giÃºp ta tiáº¿p kiá»‡m ráº¥t nhiá»u thá»i gian.\nChÃºng ta sáº½ dÃ¹ng hÃ¬nh RGB lÃ m Ä‘áº§u vÃ o, sau Ä‘Ã³ lÃ  vÃ i lá»›p CNN, vÃ  vÃ i lá»›p fully connected. Output sáº½ lÃ  má»™t giÃ¡ trá»‹ duy nháº¥t, Ä‘Æ°a vÃ o má»™t lÃ m logistic Ä‘á»ƒ cho ra giÃ¡ trá»‹ tá»« 0 Ä‘áº¿n 1 - mÃ´ táº£ xÃ¡c xuáº¥t mÃ  hÃ¬nh input cÃ³ chá»©a xe hÆ¡i.\n\n\n\n\nMá»™t mÃ´ hÃ¬nh phÃ¢n loáº¡i CNN Ä‘Æ¡n giáº£n | Nguá»“n : TÃ¡c gá»‰a - Thiáº¿t káº¿: Lou HD\n\nBÃ¢y giá», hÃ£y xem chÃºng ta chá»‰ cÃ³ xe â€œbÃ¬nh thÆ°á»ngâ€ Ä‘á»ƒ huáº¥n luyá»‡n. Tháº¿ thÃ¬ mÃ´ hÃ¬nh sáº½ hoáº¡t Ä‘á»™ng nhÆ° tháº¿ nÃ o náº¿u chÃºng ta muá»‘n nÃ³ phÃ¢n loáº¡i má»™t chiáº¿c xe cÃ´ng thá»©c 1?\n\n\n\n\nNhÆ° Ä‘Ã£ nÃ³i á»Ÿ trÃªn, covariate shift khiáº¿n giÃ¡ trá»‹ kÃ­ch hoáº¡t bá»‹ phÃ¢n tÃ¡n. Ngay cáº£ khi nÃ³ khÃ´ng lÃ m váº­y, nÃ³ cÅ©ng lÃ m giáº£m hiá»‡u nÄƒng cá»§a mÃ´ hÃ¬nh | Nguá»“n : Dhiva Krishna (TrÃ¡i), Ferhat Deniz Fors (Pháº£i)\n\nTrong vÃ­ dá»¥ nÃ y, cÃ³ sá»± khÃ¡c biá»‡t giá»¯a phÃ¢n phá»‘i cá»§a dá»¯ liá»‡u huáº¥n luyá»‡n vs Ä‘Ã¡nh giáº£. NÃ³i rá»™ng hÆ¡n, sá»± thay Ä‘á»•i vá» hÆ°á»›ng xe, Ã¡nh sÃ¡ng, Ä‘iá»u kiá»‡n thá»i tiáº¿t cÅ©ng Ä‘á»§ áº£nh hÆ°á»›ng Ä‘áº¿n hiá»‡u nÄƒng cá»§a mÃ´ hÃ¬nh. á» Ä‘Ã¢y, mÃ´ hÃ¬nh cá»§a chÃºng ta khÃ´ng tá»•ng quÃ¡t Ä‘á»§ tá»‘t.\nNáº¿u chÃºng ta plot nhá»¯ng Ä‘áº·c trÆ°ng Ä‘Æ°á»£c trÃ­nh xuáº¥t ra tá»« khÃ´ng gian Ä‘áº·c trÆ°ng, chÃºng ta sáº½ cÃ³ hÃ¬nh giá»‘ng nhÆ° sau:\n\n\n\n\nHÃ¬nh 6.a: Táº¡i sao chÃºng ta cáº§n chuáº©n hoÃ¡ giÃ¡ trá»‹ Ä‘áº§u vÃ o cá»§a mÃ´ hÃ¬nh? TrÆ°á»ng há»£p khÃ´ng chuáº©n hoÃ¡: Khi huáº¥n luyá»‡n, giÃ¡ trá»‹ inputs náº±m xa nhau: hÃ m sá»‘ xáº¥p xá»‰ sáº½ ráº¥t chÃ­nh xÃ¡c khi cÃ¡c Ä‘iá»ƒm náº±m cáº­n nhau. NgÆ°á»£c láº¡i, hÃ m nÃ y sáº½ thiáº¿u chÃ­nh xÃ¡c vÃ  bá»‹ ngáº«u nhiÃªu khi máº­t Ä‘á»™ dá»¯ liá»‡u tháº¥p | Nguá»“n : Nguá»“n : TÃ¡c giáº£ - thiáº¿t káº¿ bá»Ÿi: Lou HD\n\nGá»‰a Ä‘á»‹nh ráº±ng kÃ½ tá»± X tÆ°Æ¡ng á»©ng vá»›i hÃ¬nh áº£nh khÃ´ng xe hÆ¡i, vÃ  O lÃ  hÃ¬nh áº£nh cÃ³ xe hÆ¡i. á» Ä‘Ã¢y, chÃºng ta cÃ³ má»™t hÃ m Ä‘á»ƒ chia giá»¯a hai loáº¡i áº£nh. NhÆ°ng hÃ m nÃ y sáº½ cÃ³ Ä‘á»™ chÃ­nh xÃ¡c tháº¥p hÆ¡n á»Ÿ pháº§n trÃªn cÃ¹ng bÃªn pháº£i cá»§a Ä‘á»“ thá»‹ vÃ¬ khÃ´ng cÃ³ Ä‘á»§ dá»¯ liá»‡u Ä‘á»ƒ xÃ¡c Ä‘á»‹nh Ä‘Æ°á»£c hÃ m tá»‘t hÆ¡n. Äiá»u nÃ y cÃ³ thá»ƒ dáº«n Ä‘áº¿n Ä‘á»™ chÃ­nh xÃ¡c tháº¥p hÆ¡n trong quÃ¡ trÃ¬nh Ä‘Ã¡nh giÃ¡.\nÄá»ƒ huáº¥n luyá»‡n mÃ´ hÃ¬nh hiá»‡u quáº£ hÆ¡n, chÃºng ta cáº§n nhiá»u hÃ¬nh áº£nh cÃ³ xe hÆ¡i, vá»›i táº¥t cáº£ cÃ¡c Ä‘iá»u kiá»‡n cÃ³ thá»ƒ tÆ°á»Ÿng tÆ°á»£ng ra. Tuy ráº±ng Ä‘Ã¢y lÃ  chÃºng ta huáº¥n luyá»‡n CNN, chÃºng ta cÅ©ng muá»‘n ráº±ng mÃ´ hÃ¬nh sáº½ tá»•ng quÃ¡t hoÃ¡ tá»‘t chá»‰ vá»›i Ã­t dá»¯ liá»‡u nháº¥t cÃ³ thá»ƒ.\n\nTá»« gÃ³c nhÃ¬n cá»§a mÃ´ hÃ¬nh, hÃ¬nh áº£nh khi huáº¥n luyá»‡n - vá» máº·t thá»‘ng kÃª - thÃ¬ quÃ¡ khÃ¡c biá»‡t vá»›i hÃ¬nh áº£nh khi testing. Tá»©c lÃ  cÃ³ covariate shift\n\nCÃ³ thá»ƒ giáº£i quyáº¿t váº¥n Ä‘á» nÃ y báº±ng nhá»¯ng mÃ´ hÃ¬nh Ä‘Æ¡n giáº£n hÆ¡n. Nhá»¯ng mÃ´ hÃ¬nh logistic regression thÆ°á»ng dá»… tá»‘i Æ°u hÆ¡n khi giÃ¡ trá»‹ input Ä‘Æ°á»£c chuáº©n hoÃ¡ (cÃ³ phÃ¢n phá»‘i gáº§n vá»›i (ğœ‡ = 0, Ïƒ = 1)); ÄÃ¢y lÃ  lÃ½ mo mÃ  dá»¯ liá»‡u Ä‘áº§u vÃ o thÆ°á»ng Ä‘Æ°á»£c chuáº©n hoÃ¡.\n\n\n\n\nHÃ¬nh 6.b: Táº¡i sao chÃºng ta cáº§n chuáº©n hoÃ¡ giÃ¡ trá»‹ Ä‘áº§u vÃ o cá»§a mÃ´ hÃ¬nh? TrÆ°á»ng há»£p cÃ³ chuáº©n hoÃ¡: CÃ¡c dá»¯ liá»‡u Ä‘Æ°á»£c kÃ©o gáº§n láº¡i hÆ¡n trong khÃ´ng gian Ä‘áº·c trÆ°ng -> Dá»… tÃ¬m kiáº¿m hÃ m tá»•ng quÃ¡t tá»‘t hÆ¡n | Nguá»“n : TÃ¡c giáº£ - thiáº¿t káº¿ bá»Ÿi: Lou HD\n\nGiáº£i phÃ¡p nÃ y Ä‘Æ°á»£c biáº¿t Ä‘áº¿n rá»™ng rÃ£i ngay cáº£ trÆ°á»›c khi bÃ i bÃ¡o vá» BN Ä‘Æ°á»£c Ä‘Äƒng. Vá»›i BN, nhÃ³m tÃ¡c giáº£ cá»§a [1] muá»‘n má»Ÿ rá»™ng phÆ°Æ¡ng phÃ¡p nÃ y Ä‘áº¿n vá»›i nhá»¯ng lá»›p áº©n Ä‘á»ƒ cáº£i thiá»‡n quÃ¡ trÃ¬nh huáº¥n luyá»‡n.\n\n\n3.2.1.2 Giáº£i thiáº¿t cá»§a bÃ i bÃ¡o gá»‘c: Internal covariate shift phÃ¡ há»ng quÃ¡ trÃ¬nh huáº¥n luyá»‡n\n\n\n\n\nHÃ¬nh 7: NguyÃªn lÃ½ Internal covariate shift (ICS) trong gÃ³c nhÃ¬n vá» sá»± á»•n Ä‘á»‹nh phÃ¢n phá»‘i | Nguá»“n : TÃ¡c giáº£ - thiáº¿t káº¿ bá»Ÿi: Lou HD\n\nTrong bÃ i toÃ¡n phÃ¢n loáº¡i á»Ÿ trÃªn vá» xe hÆ¡i, cÃ³ thá»ƒ xem nhá»¯ng lá»›p áº©n nhÆ° nhá»¯ng pháº§n tá»« Ä‘Æ°á»£c kÃ­ch hoáº¡t khi nÃ³ phÃ¡t hiá»‡n ra nhá»¯ng Ä‘áº·c trÆ°ng liÃªn quan Ä‘áº¿n xe hÆ¡i: nhÆ° lÃ  bÃ¡nh xe, lá»‘p hoáº·c cá»­a xe. ChÃºng ta cÃ³ thá»ƒ giáº£ Ä‘á»‹nh ráº±ng nhá»¯ng cÃ¡i hiá»‡u á»©ng nÃ³i Ä‘áº¿n á»Ÿ trÃªn cÃ³ thá»ƒ xáº£y ra giá»¯a nhá»¯ng lá»›p áº©n. Má»™t cÃ¡i bÃ¡nh xe vá»›i má»™t hÆ°á»›ng xoay nÃ o Ä‘Ã³ sáº½ kÃ­ch hoáº¡t nÆ¡-ron liÃªn quan Ä‘áº¿n phÃ¢n phá»‘i Ä‘Ã³. Trong trÆ°á»ng há»£p lÃ½ tÆ°á»Ÿng, chÃºng ta muá»‘n nhá»¯ng má»™t vÃ i nÆ¡-ron pháº£n á»©ng vá»›i nhá»¯ng phÃ¢n phá»‘i cá»§a bÃ¡nh xe á»Ÿ báº¥t cá»© hÆ°á»›ng xoay nÃ o, Ä‘á»ƒ mÃ´ hÃ¬nh cÃ³ thá»ƒ xÃ¡c Ä‘á»‹nh Ä‘Æ°á»£c xÃ¡c suáº¥t áº£nh input cÃ³ xe hay khÃ´ng hiá»‡u quáº£ hÆ¡n.\nNáº¿u dá»¯ liá»‡u input cÃ³ covariate shift lá»›n, optimizer sáº½ gáº·p váº¥n Ä‘á» khi tá»•ng quÃ¡t dá»¯ liá»‡u. NgÆ°á»£c láº¡i, náº¿u tÃ­n hiá»‡u Ä‘áº§u vÃ o luÃ´n tuÃ¢n theo phÃ¢n phá»‘i chuáº©n, optimzer sáº½ dá»… dÃ ng tá»•ng quÃ¡t hÆ¡n. Vá»›i nhá»¯ng Ä‘iá»u nÃ y, tÃ¡c giáº£ cá»§a [1] Ä‘Ã£ Ã¡p dá»¥ng chiáº¿n tháº­t chuáº©n hoÃ¡ dá»¯ liá»‡u trong lá»›p áº©n. Há» giáº£ Ä‘á»‹nh ráº±ng Ã©p (ğœ‡ = 0, Ïƒ = 1) vÃ o phÃ¢n phá»‘i cá»§a tÃ­n hiá»‡u trung gian sáº½ tá»•ng quÃ¡t tá»‘t hÆ¡n á»Ÿ táº§ng â€œkhÃ¡i niá»‡mâ€ cá»§a nhá»¯ng Ä‘áº·c trÆ°ng.\nTuy nhiÃªn, chÃºng ta khÃ´ng pháº£i lÃºc nÃ o cÅ©ng muá»‘n phÃ¢n phá»‘i chuáº©n trong nhá»¯ng lá»›p áº©n. VÃ¬ nÃ³ cÃ³ thá»ƒ lÃ m giáº£m tÃ­nh biá»ƒu thá»‹ cá»§a mÃ´ hÃ¬nh:\n\n\n\n\nHÃ¬nh 7: LÃ½ do chÃºng ta khÃ´ng pháº£i lÃºc nÃ o cÅ©ng cáº§n phÃ¢n phá»‘i chuáº©n cho lá»›p áº©n. Trong trÆ°á»ng há»£p nÃ y, hÃ m sigmoid sáº½ chá»‰ hoáº¡t Ä‘Æ°á»£c Ä‘Æ°á»£c pháº§n linear cá»§a nÃ³ | Nguá»“n : TÃ¡c giáº£ - thiáº¿t káº¿ bá»Ÿi: Lou HD\n\nVáº¥n Ä‘á» nÃ y Ä‘Æ°á»£c tÃ¡c giáº£ cá»§a [1] giáº£i quyáº¿t báº±ng cÃ¡ch thÃªm 2 tham sá»‘ huáº¥n luyá»‡n lÃ  ğ›½ and ğ›¾, giÃºp optimizer cÃ³ thá»ƒ chá»n Ä‘Æ°á»£c trung bÃ¬nh tá»‘i Æ°u (dÃ¹ng ğ›½) vÃ  Ä‘á»™ lá»‡ch chuáº©n tá»‘i Æ°u (dÃ¹ng ğ›¾) cho tÃ¡c vá»¥ nháº¥t Ä‘á»‹nh.\n\nCáº£nh bÃ¡o: Nhá»¯ng giáº£ thiáº¿t sau Ä‘Ã£ lá»—i thá»i. Tuy váº­y, váº«n nhiá»u ná»™i dung hay vá» BN váº«n sá»­ dá»¥ng nhá»¯ng giáº£ thiáº¿t Ä‘Ã³ lÃ  lÃ½ do BN hiá»‡u quáº£. Hiá»‡n nay Ä‘Ã£ cÃ³ nhiá»u cÃ´ng trÃ¬nh thÃ¡ch thá»©c nhá»¯ng giáº£ thiáº¿t ban Ä‘áº§u.\n\nTrong vÃ i nÄƒm sau khi [1] phÃ¡t hÃ nh, cá»™ng Ä‘á»“ng há»c sÃ¢u giáº£i thÃ­ch tÃ­nh hiá»‡u quáº£ cá»§a BN nhÆ° sau:\nGiáº£ thiáº¿t 1\n\nBN -> Chuáº©n hoÃ¡ tÃ­n hiá»‡u cá»§a lá»›p áº©n -> ThÃªm hai tham sá»‘ huáº¥n luyá»‡n Ä‘á»ƒ thay Ä‘á»•i phÃ¢n phá»‘i vÃ  táº­n dá»¥ng phi tuyáº¿n tÃ­nh -> Huáº¥n luyá»‡n dá»… hÆ¡n\n\nTáº¡i Ä‘Ã¢y, chuáº©n hoÃ¡ (ğœ‡ = 0, Ïƒ = 1) Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ giáº£i thÃ­ch táº¡i sao BN hiá»‡u quáº£. Giáº£ thiáº¿t nÃ y Ä‘Ã£ bá»‹ thÃ¡ch thá»©c (Ä‘á»c á»Ÿ pháº§n sau) vÃ  Ä‘Æ°á»£c thay tháº¿ bá»Ÿi giáº£ thiáº¿t tiáº¿p heo:\nGiáº£ thiáº¿t 2\n\nBN -> Chuáº©n hoÃ¡ tÃ­n hiá»‡u cá»§a lá»›p áº©n -> Giáº£m sá»± phá»¥ thuá»™c láº«n nhau giá»¯a cÃ¡c lá»›p áº©n (theo gÃ³c nhÃ¬n vá» sá»± á»•n Ä‘á»‹nh cá»§a phÃ¢n phá»‘i) -> Huáº¥n luyá»‡n dá»… hÆ¡n\n\nSá»± khÃ¡c biá»‡t vá»›i giáº£ thiáº¿t 1 lÃ  nhá» nhÆ°ng láº¡i ráº¥t quan trá»ng. á» Ä‘Ã¢y, má»¥c tiÃªu cá»§a chuáº©n hoÃ¡ lÃ  Ä‘á»ƒ giáº£m Ä‘i sá»± phá»¥ thuá»™c láº«n nhau giá»¯a cÃ¡c lá»›p -> optimizer cÃ³ thá»ƒ chá»n Ä‘Æ°á»£c phÃ¢n phá»‘i tá»‘i Æ°u báº±ng cÃ¡ch thay Ä‘á»•i hai tham sá»‘. HÃ£y xem xÃ©t giáº£ thiáº¿t nÃ y ká»¹ hÆ¡n.\n\n\n\n3.2.2 Giáº£ thiáº¿t - BN giáº£m sá»± phá»¥ thuá»™c láº«n nhau trong cÃ¡c lá»›p áº©n khi huáº¥n luyá»‡n.\nVá» pháº§n nÃ y: tÃ¡c giáº£ khÃ´ng thá»ƒ tÃ¬m tháº¥y báº±ng chá»©ng thuyáº¿t phá»¥c vá» giáº£ thiáº¿t nÃ y. Tháº¿ nÃªn, tÃ¡c giáº£ sáº½ dá»±a vÃ o giáº£i thÃ­ch cá»§a Ian Goodfellow: YouTube\nHÃ£y xem xÃ©t vÃ­ dá»¥ sau:\n\n\n\n\nHÃ¬nh 9: Má»™t DNN Ä‘Æ¡n giáº£n, chá»‰ bao gá»“m nhá»¯ng biáº¿n Ä‘á»•i tuyáº¿n tÃ­nh | Láº¥y cáº£m há»©ng tá»« Ian Goodfellow\n\nTrong Ä‘Ã³ (a), (b), (c), (d), (e) lÃ  nhá»¯ng lá»›p tuáº§n tá»± trong DNN. ÄÃ¢y lÃ  má»™t vÃ­ dá»¥ ráº¥t hÆ¡n giáº£n, chá»‰ bao gá»“m cÃ¡c lá»›p Ä‘Æ°á»£c liÃªn káº¿t vá»›i nhau báº±ng biáº¿n Ä‘á»•i tuyáº¿n tÃ­nh. Giáº£ sá»­ chÃºng ta muá»‘n huáº¥n luyá»‡n mÃ´ hÃ¬nh trÃªn báº±ng SGD.\nÄá»ƒ cáº­p nháº­t trá»ng sá»‘ cá»§a lá»›p (a), chÃºng ta cáº§n tÃ­nh giÃ¡ trá»‹ Ä‘áº¡o hÃ m tá»« output cá»§a network nhÆ° sau:\n\n\n\nHÃ£y xem xÃ©t má»™t network khÃ´ng cÃ³ BNN. Tá»« phÆ°Æ¡ng trÃ¬nh á»Ÿ trÃªn, chÃºng ta káº¿t luáº­n ráº±ng náº¿u táº¥t cáº£ giÃ¡ trá»‹ Ä‘áº¡o hÃ m (gradient) lá»›n thÃ¬ Ä‘áº¡o hÃ m cá»§a a (grad(a)) sáº½ ráº¥t lá»›n. NgÆ°á»£c láº¡i, náº¿u táº¥t cáº£ gradient Ä‘á»u nhá» thÃ¬ grad(a) gáº§n nhÆ° báº±ng 0.\nDá»… dÃ ng tháº¥y ráº±ng cÃ¡c lá»›p phá»¥ thuá»™c vÃ o nhau nhÆ° tháº¿ nÃ o báº±ng cÃ¡ch nhÃ¬n vÃ o phÃ¢n phá»‘i tÃ­n hiá»‡u Ä‘áº§u vÃ o cá»§a nhá»¯ng lá»›p áº©n: má»™t sá»± thay Ä‘á»•i trong trá»ng sá»‘ cá»§a (a) sáº½ dáº«n Ä‘áº¿n thay Ä‘á»•i trá»ng sá»‘ cá»§a lá»›p (b) vÃ  tá»« tá»« Ä‘áº¿n (d) vÃ  cuá»‘i cÃ¹ng lÃ  (e). Sá»± phá»¥ thuá»™c láº«n nhau nÃ y gÃ¢y ra váº¥n Ä‘á» vá»›i Ä‘á»™ á»•n Ä‘á»‹nh khi huáº¥n luyá»‡n: Náº¿u ta muá»‘n thay Ä‘á»•i phÃ¢n phá»‘i Ä‘áº§u vÃ o cá»§a má»™t lá»›p áº©n nÃ o Ä‘Ã³, nÃ³ sáº½ dáº«n Ä‘áº¿n sá»± thay Ä‘á»•i cá»§a nhá»¯ng lá»›p theo sau.\nTuy nhiÃªn, SGD chá»‰ quan tÃ¢m Ä‘áº¿n má»‘i liÃªn há»‡ báº­c 1 giá»¯a cÃ¡c lá»›p. NÃªn, nÃ³ khÃ´ng bao quÃ¡t Ä‘Æ°á»£c nhá»¯ng nhá»¯ng má»‘i quan há»‡ báº­c cao hÆ¡n nÃ³i á»Ÿ trÃªn.\n\n\n\n\nHÃ¬nh 10: BN Ä‘iá»u hoÃ¡ dÃ²ng cháº£y cá»§a tÃ­n hiá»‡u, báº±ng cÃ¡ch chuáº©n hoÃ¡ tÃ­n hiá»‡u trong má»—i hidden unit, vÃ  cho phÃ©p Ä‘iá»u chá»‰nh phÃ¢n phá»‘i vá»›i vÃ Â ğ›¾. BN nhÆ° lÃ  má»™t cÃ¡i van khiáº¿n viá»‡c Ä‘iá»u khiá»ƒn dÃ²ng cháº£y dá»… dÃ ng hÆ¡n á»Ÿ vÃ i chá»— mÃ  khÃ´ng lÃ m giáº£m kháº£ nÄƒng phá»©c táº¡p cá»§a mÃ´ hÃ¬nh. Lou HD\n\nThÃªm lá»›p BN giÃºp giáº£m sá»± phá»¥ thuá»™c láº«n nhau giá»¯a cÃ¡c lá»›p (theo cÃ¡ch nhÃ¬n vá» sá»± á»•n Ä‘á»‹nh phÃ¢n phá»‘i) trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n. BN nhÆ° má»™t cÃ¡i van nÆ°á»›c giÃºp giáº£m láº¡i dÃ²ng cháº£y, vá»›i hai tham sá»‘ ğ›½Â vÃ Â ğ›¾. MÃ  vÃ¬ tháº¿ nÃªn khÃ´ng cáº§n xem xÃ©t táº¥t cáº£ tham sá»‘ Ä‘á»ƒ hiá»ƒu vá» phÃ¢n phá»‘i trong cÃ¡c lá»›p áº©n.\n\nLÆ°u Ã½: Do cÃ³ BN, optimizer cÃ³ thá»ƒ thay Ä‘á»•i trá»ng sá»‘ máº¡nh hÆ¡n mÃ  khÃ´ng lÃ m suy thoÃ¡i cÃ¡c tham sá»‘ Ä‘Ã£ Ä‘Æ°á»£c Ä‘iá»u chá»‰nh trÆ°á»›c Ä‘Ã³ cá»§a lá»›p áº©n khÃ¡c. Äiá»u nÃ y khiáº¿n viá»‡c Ä‘iá»u chá»‰nh cÃ¡c siÃªu tham sá»‘ (hyperparameter) dá»… hÆ¡n ráº¥t nhiá»u!\n\nVÃ­ dá»¥ nÃ y bá» qua giáº£ thuyáº¿t cho ráº±ng hiá»‡u quáº£ cá»§a BN lÃ  do sá»± chuáº©n hÃ³a cá»§a phÃ¢n bá»‘ tÃ­n hiá»‡u giá»¯a cÃ¡c lá»›p (Î¼ = 0, Ïƒ = 1). á» Ä‘Ã¢y, BN cÃ³ má»¥c Ä‘Ã­ch lÃ m cho viá»‡c tá»‘i Æ°u hÃ³a tÃ¡c vá»¥ dá»… hÆ¡n, cho phÃ©p nÃ³ Ä‘iá»u chá»‰nh phÃ¢n bá»‘ lá»›p áº©n vá»›i chá»‰ hai tham sá»‘ má»™t lÃºc.\n\nTuy nhiÃªn, hÃ£y nhá»› ráº±ng Ä‘iá»u nÃ y chá»§ yáº¿u lÃ  suy Ä‘oÃ¡n thÃ´i. Nhá»¯ng tháº£o luáº­n nÃ y nÃªn Ä‘Æ°á»£c sá»­ dá»¥ng nhÆ° cÃ¡c kiáº¿n thá»©c Ä‘á»ƒ xÃ¢y dá»±ng sá»± hiá»ƒu biáº¿t vá» BN. ChÃºng ta váº«n khÃ´ng biáº¿t chÃ­nh xÃ¡c táº¡i sao BN hiá»‡u quáº£ trong thá»±c táº¿!\n\nNÄƒm 2019, má»™t nhÃ³m nghiÃªn cá»©u tá»« MIT thá»±c hiá»‡n vÃ i thÃ­ nghiá»‡m thÃº vá»‹ vá» BN [2]. Káº¿t quáº£ cá»§a há» Ä‘e dá»a lá»›n giáº£ thuyáº¿t 1 (váº«n Ä‘Æ°á»£c chia sáº» bá»Ÿi nhiá»u bÃ i viáº¿t blog vÃ  MOOCs!).\nChÃºng ta nÃªn xem qua tÃ i liá»‡u nÃ y náº¿u muá»‘n trÃ¡nh â€œgiáº£ thuyáº¿t Ä‘iá»ƒm tá»‘i thiá»ƒu Ä‘á»‹a phÆ°Æ¡ngâ€ vá» tÃ¡c Ä‘á»™ng cá»§a BN trÃªn huáº¥n luyá»‡nâ€¦ ;)\n\n\n\n\nÄÆ°á»£c rá»“iâ€¦ báº¡n nÃªn khá»Ÿi táº¡o tá»‘t hÆ¡n.\n\n\n\n3.2.3 Giáº£ thuyáº¿t 3 - BN lÃ m cho khÃ´ng gian tá»‘i Æ°u hÃ³a trá»Ÿ nÃªn mÆ°á»£t hÆ¡n\nVá» pháº§n nÃ y: TÃ¡c giáº£ Ä‘Ã£ tÃ³m táº¯t káº¿t quáº£ tá»« [2] mÃ  cÃ³ thá»ƒ giÃºp chÃºng ta xÃ¢y dá»±ng cÃ¡ch hiá»ƒu tá»‘t hÆ¡n vá» BN. TÃ¡c giáº£ khÃ´ng thá»ƒ tÃ³m táº¯t Ä‘áº§y Ä‘á»§, tÃ i liá»‡u nÃ y ráº¥t nhiá»u, tÃ¡c giáº£ Ä‘á» nghá»‹ báº¡n Ä‘á»c ká»¹ náº¿u báº¡n quan tÃ¢m Ä‘áº¿n nhá»¯ng khÃ¡i niá»‡m Ä‘Ã³.\nHÃ£y Ä‘i tá»›i thÃ­ nghiá»‡m thá»© 2 cá»§a [2]. Má»¥c tiÃªu cá»§a há» lÃ  kiá»ƒm tra sá»± tÆ°Æ¡ng quan giá»¯a ICS vÃ  lá»£i Ã­ch cá»§a BN trÃªn hiá»‡u suáº¥t huáº¥n luyá»‡n (giáº£ thuyáº¿t 1).\nKhÃ¡i niá»‡m: ChÃºng ta sáº½ gá»i covariate shift nÃ y lÃ  ICS_distrib.\nÄá»ƒ lÃ m Ä‘iá»u Ä‘Ã³, nhÃ  nghiÃªn cá»©u Ä‘Ã£ huáº¥n luyá»‡n ba máº¡ng VGG (trÃªn CIFAR-10):\n\nMáº¡ng thá»© nháº¥t khÃ´ng cÃ³ báº¥t ká»³ lá»›p BN nÃ o;\nMáº¡ng thá»© hai cÃ³ cÃ¡c lá»›p BN;\nMáº¡ng thá»© ba tÆ°Æ¡ng tá»± nhÆ° máº¡ng thá»© hai, ngoáº¡i trá»« viá»‡c há» Ä‘Ã£ thÃªm má»™t sá»‘ ICS_distrib trong Ä‘Æ¡n vá»‹ áº©n trÆ°á»›c khi kÃ­ch hoáº¡t (báº±ng cÃ¡ch thÃªm khoáº£ng sai vÃ  biáº¿n thá»ƒ ngáº«u nhiÃªn).\n\nHá» Ä‘o lÆ°á»ng Ä‘á»™ chÃ­nh xÃ¡c Ä‘áº¡t Ä‘Æ°á»£c bá»Ÿi má»—i mÃ´ hÃ¬nh vÃ  sá»± thay Ä‘á»•i cá»§a giÃ¡ trá»‹ phÃ¢n bá»‘ theo sá»‘ láº§n láº·p. ÄÃ¢y lÃ  káº¿t quáº£ mÃ  há» Ä‘Ã£ nháº­n Ä‘Æ°á»£c:\n\n\n\n\nBN vá»›i ICS_distrib | CÃ¡c máº¡ng vá»›i BN Ä‘Æ°á»£c huáº¥n luyá»‡n nhanh hÆ¡n so vá»›i máº¡ng tiÃªu chuáº©n; thÃªm rÃµ rÃ ng ICS_distrib trÃªn má»™t máº¡ng Ä‘Æ°á»£c kiá»ƒm soÃ¡t khÃ´ng lÃ m giáº£m lá»£i Ã­ch cá»§a BN. | Nguá»“n: [2]\n\nChÃºng ta cÃ³ thá»ƒ tháº¥y ráº±ng máº¡ng thá»© ba cÃ³ má»™t ICS ráº¥t cao (nhÆ° dá»± Ä‘oÃ¡n). Tuy nhiÃªn, máº¡ng bá»‹ nhiá»…u váº«n Ä‘Æ°á»£c huáº¥n luyá»‡n nhanh hÆ¡n so vá»›i máº¡ng tiÃªu chuáº©n. Káº¿t quáº£ Ä‘áº¡t Ä‘Æ°á»£c cá»§a nÃ³ tÆ°Æ¡ng Ä‘Æ°Æ¡ng vá»›i káº¿t quáº£ Ä‘Æ°á»£c Ä‘áº¡t Ä‘Æ°á»£c vá»›i má»™t máº¡ng BN tiÃªu chuáº©n. Káº¿t quáº£ nÃ y cho tháº¥y ráº±ng hiá»‡u quáº£ cá»§a BN khÃ´ng liÃªn quan Ä‘áº¿n ICS_distrib.\nChÃºng ta khÃ´ng nÃªn loáº¡i bá» giáº£ thuyáº¿t ICS quÃ¡ vá»™i: náº¿u hiá»‡u quáº£ cá»§a BN khÃ´ng xuáº¥t phÃ¡t tá»« ICS_distrib, nÃ³ cÃ³ thá»ƒ liÃªn quan Ä‘áº¿n má»™t Ä‘á»‹nh nghÄ©a khÃ¡c cá»§a ICS. Sau cÃ¹ng, sá»± giáº£ thuyáº¿t sá»‘ 1 cÅ©ng cÃ³ váº» Ä‘Ãºng, pháº£i khÃ´ng?\nVáº¥n Ä‘á» chÃ­nh vá»›i ICS_distrib lÃ  Ä‘á»‹nh nghÄ©a cá»§a nÃ³ liÃªn quan Ä‘áº¿n phÃ¢n bá»‘ Ä‘áº§u vÃ o cá»§a cÃ¡c Ä‘Æ¡n vá»‹ áº©n. VÃ¬ váº­y, khÃ´ng cÃ³ liÃªn káº¿t trá»±c tiáº¿p vá»›i váº¥n Ä‘á» tá»‘i Æ°u hÃ³a cá»§a nÃ³.\nTÃ¡c giáº£ cá»§a [2] Ä‘Ã£ Ä‘Æ°a ra má»™t Ä‘á»‹nh nghÄ©a khÃ¡c cá»§a ICS:\nHÃ£y xem xÃ©t má»™t Ä‘áº§u vÃ o X cá»‘ Ä‘á»‹nh.\n\nChÃºng ta Ä‘á»‹nh nghÄ©a ICS, tá»« má»™t gÃ³c nhÃ¬n cá»§a tá»‘i Æ°u hÃ³a, lÃ  sá»± khÃ¡c biá»‡t giá»¯a Ä‘áº¡o hÃ m tÃ­nh toÃ¡n trÃªn lá»›p áº©n k sau khi phá»¥c há»“i lá»—i \\(L(X)_{it}\\) vÃ  Ä‘áº¡o hÃ m tÃ­nh toÃ¡n trÃªn cÃ¹ng má»™t lá»›p k tá»« máº¥t mÃ¡t \\(L(X)_{it+1}\\) sau láº§n láº·p thá»© \\(it\\)\n\nÄá»‹nh nghÄ©a nÃ y nháº±m táº­p trung vÃ o cÃ¡c giÃ¡ trá»‹ Ä‘áº¡o hÃ m hÆ¡n lÃ  trÃªn phÃ¢n bá»‘ Ä‘áº§u vÃ o cá»§a lá»›p áº©n, giáº£ sá»­ ráº±ng nÃ³ cÃ³ thá»ƒ cho chÃºng ta cÃ¡c gá»£i Ã½ tá»‘t hÆ¡n vá» cÃ¡ch ICS cÃ³ thá»ƒ cÃ³ áº£nh hÆ°á»Ÿng Ä‘áº¿n váº¥n Ä‘á» tá»‘i Æ°u hÃ³a sÃ¢u bÃªn trong.\nKÃ½ hiá»‡u: ICS_opti bÃ¢y giá» Ä‘á» cáº­p Ä‘áº¿n ICS Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a tá»« má»™t gÃ³c Ä‘á»™ tá»‘i Æ°u hÃ³a.\nTrong thÃ­ nghiá»‡m tiáº¿p theo, tÃ¡c giáº£ Ä‘Ã¡nh giÃ¡ tÃ¡c Ä‘á»™ng cá»§a ICS_opti trÃªn hiá»‡u quáº£ huáº¥n luyá»‡n. Äá»ƒ lÃ m nhÆ° váº­y, há» Ä‘o lÆ°á»ng sá»± biáº¿n Ä‘á»•i cá»§a ICS_opti trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n cho má»™t DNN vá»›i cÃ³ vÃ  khÃ´ng cÃ³ BN layers. Äá»ƒ Ä‘á»‹nh lÆ°á»£ng sá»± biáº¿n Ä‘á»•i cá»§a Ä‘áº¡o hÃ m Ä‘Æ°á»£c nÃªu trong Ä‘á»‹nh nghÄ©a ICS_opti, há» tÃ­nh toÃ¡n:\n\nSá»± khÃ¡c biá»‡t L2: cÃ³ Ä‘áº¡o hÃ m cÃ³ norm gáº§n nhau trÆ°á»›c vÃ  sau khi cáº­p nháº­t trá»ng sá»‘ khÃ´ng? LÃ½ tÆ°á»Ÿng: 0\nGÃ³c Cosine: cÃ³ Ä‘áº¡o hÃ m cÃ³ hÆ°á»›ng gáº§n nhau trÆ°á»›c vÃ  sau khi cáº­p nháº­t trá»ng sá»‘ khÃ´ng? LÃ½ tÆ°á»Ÿng: 1\n\n\n\n\n\náº¢nh hÆ°á»Ÿng cá»§a BN lÃªn ICS_opti | Khoáº£ng cÃ¡ch L2 vÃ  gÃ³c cosine gá»£i Ã½ ráº±ng BN khÃ´ng trÃ¡nh Ä‘Æ°á»£c ICS_opti (ngÆ°á»£c láº¡i, nÃ³ cÃ²n lÃ m gia tÄƒng Ä‘iá»u Ä‘Ã³) | Nguá»“n: [2]\n\nKáº¿t quáº£ láº¡i chÃºt báº¥t ngá»: máº¡ng sá»­ dá»¥ng BN cÃ³ váº» cÃ³ ICS_opti cao hÆ¡n so vá»›i máº¡ng tiÃªu chuáº©n. HÃ£y nhá»› ráº±ng máº¡ng vá»›i BN (Ä‘Æ°á»ng xanh) Ä‘Æ°á»£c huáº¥n luyá»‡n nhanh hÆ¡n so vá»›i máº¡ng tiÃªu chuáº©n (Ä‘Æ°á»ng Ä‘á»)!\nICS cÃ³ váº» khÃ´ng liÃªn quan Ä‘áº¿n hiá»‡u quáº£ huáº¥n luyá»‡nâ€¦ Ã­t nháº¥t lÃ  cho Ä‘á»‹nh nghÄ©a ICS_opti.\nMá»™t cÃ¡ch nÃ o Ä‘Ã³, Batch Normalization cÃ³ áº£nh hÆ°á»Ÿng khÃ¡c trÃªn mÃ´ hÃ¬nh, giÃºp cho viá»‡c há»™i tá»¥ dá»… dÃ ng hÆ¡n.\nBÃ¢y giá», hÃ£y xem xÃ©t cÃ¡ch BN áº£nh hÆ°á»Ÿng Ä‘áº¿n cáº£nh quan tá»‘i Æ°u hÃ³a (optimization landscape) Ä‘á»ƒ tÃ¬m theo manh má»‘i nhÃ©.\nTiáº¿p theo lÃ  thÃ­ nghiá»‡m cuá»‘i cÃ¹ng Ä‘Æ°á»£c Ä‘á» cáº­p trong bÃ i viáº¿t nÃ y:\n\n\n\n\nKhÃ¡m phÃ¡ cáº£nh quan tá»‘i Æ°u hoÃ¡ (Optimization landscape exploration) theo hÆ°á»›ng Ä‘áº¡o hÃ m. ThÃ­ nghiá»‡m Ä‘Æ°á»£c thá»±c hiá»‡n trong paper [2] | Cáº£m há»©ng tá»«: Andrew Ilyas - thiáº¿t káº¿ bá»Ÿi: Lou HD\n\nTá»« má»™t giÃ¡ trá»‹ Ä‘áº¡o hÃ m, chÃºng ta cáº­p nháº­t trá»ng sá»‘ vá»›i cÃ¡c bÆ°á»›c tá»‘i Æ°u hÃ³a khÃ¡c nhau (mÃ  hoáº¡t Ä‘á»™ng nhÆ° tá»‘c Ä‘á»™ há»c - learning rate). NÃ³i má»™t cÃ¡ch trá»±c quan, chÃºng ta Ä‘á»‹nh nghÄ©a má»™t hÆ°á»›ng tá»« má»™t Ä‘iá»ƒm nháº¥t Ä‘á»‹nh (tá»©c má»™t cáº¥u hÃ¬nh máº¡ng Ï‰) trong khÃ´ng gian Ä‘áº·c trÆ°ng, sau Ä‘Ã³ khÃ¡m phÃ¡ thÃªm cáº£nh quan tá»‘i Æ°u hÃ³a theo hÆ°á»›ng nÃ y.\nTáº¡i má»—i bÆ°á»›c, chÃºng ta Ä‘o lÆ°á»ng Ä‘áº¡o hÃ m vÃ  máº¥t mÃ¡t (loss). ChÃºng ta cÃ³ thá»ƒ so sÃ¡nh cÃ¡c Ä‘iá»ƒm khÃ¡c nhau cá»§a cáº£nh quan tá»‘i Æ°u hÃ³a vá»›i má»™t Ä‘iá»ƒm báº¯t Ä‘áº§u. Náº¿u chÃºng ta Ä‘o lÆ°á»ng sá»± biáº¿n Ä‘á»•i lá»›n, cáº£nh quan ráº¥t khÃ´ng á»•n Ä‘á»‹nh vÃ  Ä‘áº¡o hÃ m khÃ´ng cháº¯c cháº¯n: cÃ¡c bÆ°á»›c lá»›n cÃ³ thá»ƒ lÃ m xáº¥u viá»‡c tá»‘i Æ°u hÃ³a. NgÆ°á»£c láº¡i, náº¿u sá»± biáº¿n Ä‘á»•i Ä‘o Ä‘Æ°á»£c nhá», cáº£nh quan á»•n Ä‘á»‹nh vÃ  Ä‘áº¡o hÃ m Ä‘Ã¡ng tin cáº­y: chÃºng ta cÃ³ thá»ƒ Ã¡p dá»¥ng cÃ¡c bÆ°á»›c lá»›n hÆ¡n mÃ  khÃ´ng gÃ¢y háº¡i cho tá»‘i Æ°u hÃ³a. NÃ³i cÃ¡ch khÃ¡c, chÃºng ta cÃ³ thá»ƒ sá»­ dá»¥ng má»™t tá»‘c Ä‘á»™ há»c lá»›n hÆ¡n vÃ  lÃ m cho viá»‡c há»™i tá»¥ nhanh hÆ¡n (má»™t tÃ­nh nÄƒng Ä‘Æ°á»£c biáº¿t Ä‘áº¿n cá»§a BN).\nHÃ£y xem káº¿t quáº£:\n\n\n\n\nTÃ¡c Ä‘á»™ng cá»§a BN lÃªn viá»‡c lÃ m mÆ°á»£t cáº£nh quan tá»‘i Æ°u hoÃ¡ (optimization landscape smoothing) | BN lÃ m giáº£m thiá»ƒu Ä‘Ã¡ng káº¿ sá»± biáº¿n Ä‘á»™ng cá»§a Ä‘áº¡o hÃ m | Nguá»“n: [2]\nRÃµ tháº¥y ráº±ng cáº£nh quan tá»‘i Æ°u hoÃ¡ mÆ°á»£t hÆ¡n nhiá»u khi dÃ¹ng cÃ¡c lá»›p BN.\nCuá»‘i chÃ¹ng chÃºng ta cÃ³ káº¿t quáº£ Ä‘á»ƒ dÃ¹ng Ä‘á»ƒ giáº£i thÃ­ch sá»± hiá»‡u quáº£ cá»§a BN: Lá»›p BN lÃ m cho cáº£nh quan tá»‘i Æ°u hoÃ¡ mÆ°á»£t hÆ¡n. Tá»« Ä‘Ã³ thÃ¬ viá»‡c tá»‘i Æ°u hoÃ¡ cÅ©ng dá»… dÃ ng hÆ¡n: chÃºng ta cÃ³ thá»ƒ sá»­ dá»¥ng tá»‘c Ä‘á»™ há»c cao hÆ¡n mÃ  khÃ´ng bá»‹ gradient vanishing hoáº·c gradient explosion.\nGiáº£ thiáº¿t thá»© 3 thÃ¬ Ä‘áº¿n tá»« bÃ i bÃ¡o [2]:\n\n\n3.2.4 Gá»‰a thiáº¿t 3\nBN -> chuáº©n hoÃ¡ tÃ­n hiá»‡u trong cÃ¡c Ä‘Æ¡n vá»‹ áº©n -> lÃ m mÆ°á»£t cáº£nh quan tá»‘i Æ°u hoÃ¡ -> huáº¥n luyá»‡n nhanh vÃ  á»•n Ä‘á»‹nh hÆ¡n.\nNÃ³ Ä‘áº·t ra má»™t cÃ¢u há»i khÃ¡c:** LÃ m sao mÃ  BN láº¡i lÃ m cho cáº£nh quan tá»‘i Æ°u hoÃ¡ mÆ°á»£t hÆ¡n?**\nTÃ¡c giáº£ cá»§a [2] Ä‘Ã£ khÃ¡m nhá»¯ng váº¥n Ä‘á» nÃ y tá»« gÃ³c nhÃ¬n lÃ½ thuyá»‡t. NghiÃªn cá»©u cá»§a há» ráº¥t cÃ³ Ã­ch trong viá»‡c hiá»ƒu Ä‘Æ°á»£c hiá»‡u á»©ng lÃ m mÆ°á»£t cá»§a BN. Chi tiáº¿t hÆ¡n, há» chá»©ng minh ráº±ng BN lÃ m cho cáº£nh quan tá»‘i Æ°u mÆ°á»£t hÆ¡n trong khi váº«n giá»¯ táº¥t cáº£ cÃ¡c cá»±c tiá»ƒu cá»§a cáº£nh quan thÆ°á»ng. NÃ³i cÃ¡ch khÃ¡c, BN thay Ä‘á»•i tham sá»‘ cá»§a bÃ i toÃ¡n tá»‘i Æ°u phÃ­a dÆ°á»›i, lÃ m cho huáº¥n luyá»‡n nhanh vÃ  dá»… hÆ¡n!\nTrong nhá»¯ng nghiÃªn cá»©u bá»• sung, tÃ¡c giáº£ cá»§a [2] quan sÃ¡t ráº±ng hiá»‡u á»©ng nÃ y khÃ´ng chá»‰ cÃ³ á»Ÿ BN. Há» Ä‘áº¡t Ä‘Æ°á»£c hiá»‡u nÄƒng huáº¥n luyá»‡n tÆ°Æ¡ng Ä‘Æ°Æ¡ng vá»›i phÆ°Æ¡ng phÃ¡p tá»‘i Æ°u khÃ¡c nhÆ° L1 hay L2. Nhá»¯ng quan sÃ¡t nÃ y gá»£i Ã½ ráº±ng sá»± hiá»‡u quáº£ cá»§a BN pháº§n lá»›n Ä‘áº¿n tá»« sá»± trÃ¹ng há»£p, do táº­n dá»¥ng má»™t cÆ¡ cháº¿ táº§ng dÆ°á»›i nÃ o Ä‘Ã³ mÃ  chÃºng ta chÆ°a nháº­n dáº¡ng chÃ­nh xÃ¡c Ä‘Æ°á»£c.\nÄá»ƒ káº¿t thÃºc pháº§n nÃ y, bÃ i bÃ¡o nÃ y thÃ¡ch thá»©c cá»±c máº¡nh Ã½ tÆ°á»Ÿng ráº±ng BN hiá»‡u quáº£ vÃ¬ Ä‘Ã³ giáº£m thiá»ƒu ICS (cáº£ trong gÃ³c nhÃ¬n vá» sá»± á»•n Ä‘á»‹nh cá»§a phÃ¢n phá»‘i vÃ  cáº£ vá» tá»‘i Æ°u hoÃ¡). Tuy nhiÃªn, nÃ³ láº¡i nháº¥n máº¡nh vá» áº£nh hÆ°á»Ÿng cá»§a sá»± lÃ m mÆ°á»£t cáº£nh quan tá»‘i Æ°u cá»§a BN.\nTuy ráº±ng bÃ i bÃ¡o nÃ y Ä‘Æ°a ra giáº£ thiáº¿t vá» áº£nh hÆ°á»Ÿng cá»§a BN lÃªn tá»‘c Ä‘á»™ huáº¥n luyá»‡n, nhÆ°ng nÃ³ khÃ´ng tráº£ lá»i táº¡i sao BN láº¡i há»— trá»£ tá»‘t cho quÃ¡ trÃ¬nh tá»•ng quÃ¡t hoÃ¡.\nHá» cÃ³ tháº£o luáº­n nhanh ráº±ng lÃ m cho cáº£nh quan tá»‘i Æ°u hoÃ¡ mÆ°á»£t hÆ¡n cÅ©ng giÃºp mÃ´ hÃ¬nh há»™i tá»¥ á»Ÿ cÃ¡c cá»±c tiá»ƒu pháº³ng -> lÃ m cho kháº£ nÄƒng tá»•ng quÃ¡t hoÃ¡ tá»‘t hÆ¡n. Tuy nhiÃªn, nháº­n Ä‘á»‹nh nÃ y váº«n cáº§n thÃªm nhiá»u giáº£i thÃ­ch hÆ¡n.\nÄÃ³ng gÃ³p chá»§ yáº¿u cá»§a tÃ¡c giáº£ lÃ  thÃ¡ch thá»©c Ã½ tÆ°á»Ÿng vá» sá»± áº£nh hÆ°á»Ÿng cá»§a BN lÃªn ICS - váº­y thÃ´i cÅ©ng Ä‘á»§ quan trá»ng rá»“i!"
  },
  {
    "objectID": "articles/translate-batch-normalization-in-3-levels-of-understanding.html#tá»•ng-há»£p-nhá»¯ng-lÃ½-do-cho-sá»±-hiá»‡u-quáº£-cá»§a-bn-mÃ -chÃºng-ta-biáº¿t",
    "href": "articles/translate-batch-normalization-in-3-levels-of-understanding.html#tá»•ng-há»£p-nhá»¯ng-lÃ½-do-cho-sá»±-hiá»‡u-quáº£-cá»§a-bn-mÃ -chÃºng-ta-biáº¿t",
    "title": "3 Cáº¥p Ä‘á»™ hiá»ƒu vá» Batch Normalization (BÃ i dá»‹ch)",
    "section": "3.3 Tá»•ng há»£p: Nhá»¯ng lÃ½ do cho sá»± hiá»‡u quáº£ cá»§a BN mÃ  chÃºng ta biáº¿t",
    "text": "3.3 Tá»•ng há»£p: Nhá»¯ng lÃ½ do cho sá»± hiá»‡u quáº£ cá»§a BN mÃ  chÃºng ta biáº¿t\n\nGiáº£ thiáº¿t 1: BN lÃ m giáº£m ICS -> Sai: bÃ i bÃ¡o [2] chá»©ng minh ráº±ng khÃ´ng cÃ³ sá»± tÆ°Æ¡ng quan giá»¯a ICS vÃ  hiá»‡u nÄƒng huáº¥n luyá»‡n trong thá»±c táº¿.\nGiáº£ thiáº¿t 2: BN lÃ m cho optimizer tá»‘i Æ°u nhanh hÆ¡n vÃ¬ nÃ³ thay Ä‘á»•i phÃ¢n phá»‘i Ä‘áº§u vÃ o cá»§a Ä‘Æ¡n vá»‹ áº©n chá»‰ báº±ng 2 tham sá»‘ -> CÃ³ thá»ƒ: Giáº£ thiáº¿t nÃ y nháº¥n máº¡nh sá»± liÃªn quan chÃ©o giá»¯a cÃ¡c tham sá»‘ khiáº¿n tá»‘i Æ°u khÃ³ hÆ¡n. Tuy váº­y, nÃ³ váº«n chÆ°a Ä‘á»§ thuyáº¿t phá»¥c.\nGiáº£ thiáº¿t 3: BN thay Ä‘á»•i tham sá»‘ cá»§a bÃ i toÃ¡n tá»‘i Æ°u táº§ng sÃ¢u, lÃ m nÃ³ mÆ°á»£t vÃ  á»•n Ä‘á»‹nh hÆ¡n. -> CÃ³ thá»ƒ: Káº¿t quáº£ cÅ©ng khÃ¡ gáº§n Ä‘Ã¢y. Äáº¿n thá»i Ä‘iá»ƒm cá»§a bÃ i viáº¿t gá»‘c thÃ¬ dÆ°á»ng nhÆ° giáº£ thuyáº¿t nÃ y chÆ°a bá»‹ thÃ¡ch thá»©c. BÃ i bÃ¡o cÅ©ng Ä‘Æ°a ra nhá»¯ng thÃ­ nghiá»‡m thá»±c táº¿ cÅ©ng nhÆ° giáº£i thÃ­ch vá» lÃ½ thuyáº¿t, tuy váº«n chÆ°a tráº£ lá»i má»™t sá»‘ cÃ¢u há»i ná»n táº£ng nhÆ° â€œtáº¡i sao BN láº¡i há»— trá»£ tá»•ng quÃ¡t hoÃ¡?â€\n\nTháº£o luáº­n: Äá»‘i vá»›i tÃ¡c giáº£ cá»§a bÃ i viáº¿t nÃ y, hai giáº£ thiáº¿t cuá»‘i cÃ³ vÃ¨ tÆ°Æ¡ng thÃ­ch. Vá» trá»±c quan thÃ¬ chÃºng ta tháº¥y giáº£ thiáº¿t 2 nhÆ° má»™t phÃ©p chiáº¿u tá»« bÃ i toÃ¡n nhiá»u tham sá»‘ thÃ nh bÃ i toÃ¡n Ã­t tham sá»‘ hÆ¡n; kiá»ƒu nhÆ° bÃ i toÃ¡n giáº£m chiá»u dá»¯ liá»‡u, Ä‘iá»u cÃ³ thá»ƒ há»— trá»£ cho tá»•ng quÃ¡t hoÃ¡. Báº¡n nghÄ© gÃ¬ vá» Ä‘iá»u nÃ y?\nVáº«n cÃ²n nhiá»u nhá»¯ng cÃ¢u há»i má»Ÿ vÃ  BN váº«n lÃ  má»™t chá»§ Ä‘á» nghiÃªn cá»©u ngÃ y nay. Tháº£o luáº­n nhá»¯ng giáº£ thiáº¿t nÃ y váº«n giÃºp ta hiá»ƒu hÆ¡n vá» phÆ°Æ¡ng phÃ¡p thÆ°á»ng dÃ¹ng nÃ y, vÃ  bá» qua nhá»¯ng nháº­n Ä‘á»‹nh sai láº§m trong nhá»¯ng nÄƒm gáº§n Ä‘Ã¢y.\nTuy váº­y, nhá»¯ng cÃ¢u há»i nÃ y khÃ´ng thá»ƒ ngÄƒn cáº£n chÃºng ta táº­n dá»¥ng lá»£i Ã­ch mÃ  BN Ä‘em láº¡i trong thá»±c táº¿!"
  },
  {
    "objectID": "articles/translate-batch-normalization-in-3-levels-of-understanding.html#cÃ¢u-há»i-má»Ÿ",
    "href": "articles/translate-batch-normalization-in-3-levels-of-understanding.html#cÃ¢u-há»i-má»Ÿ",
    "title": "3 Cáº¥p Ä‘á»™ hiá»ƒu vá» Batch Normalization (BÃ i dá»‹ch)",
    "section": "4.1 CÃ¢u há»i má»Ÿ",
    "text": "4.1 CÃ¢u há»i má»Ÿ\nDÃ¹ ráº±ng BN ráº¥t hiá»ƒu qua trong thá»±c nghiá»‡m, váº«n cÃ²n tá»“n táº¡i nhiá»u cÃ¢u há»i vá» cÃ¡ch thá»©c hoáº¡t Ä‘á»™ng phÃ­a sau cá»§a nÃ³ mÃ  chÆ°a cÃ³ lá»i giáº£i.\nSau lÃ  danh sÃ¡ch (khÃ´ng Ä‘áº§y Ä‘á»§):\n\nTáº¡i sao BN láº¡i há»— trá»£ quÃ¡ trÃ¬nh tá»•ng quÃ¡t hoÃ¡?\nBN cÃ³ pháº£i lÃ  phÆ°Æ¡ng phÃ¡p chuáº©n hoÃ¡ tá»‘t nháº¥t cho tá»‘i Æ°u hoÃ¡?\nğ›½ vÃ  ğ›¾ áº£nh hÆ°á»Ÿng nhÆ° tháº¿ nÃ o Ä‘áº¿n sá»± mÆ°á»£t cá»§a cáº£nh quan tá»‘i Æ°u?\nThá»±c nghiá»‡m trong bÃ i [2] vá» cáº£nh quan tá»‘i Æ°u hoÃ¡ táº­p trung vá» tÃ¡c Ä‘á»™ng ngáº¯n háº¡n cá»§a BN lÃªn Ä‘áº¡o hÃ m: há» Ä‘o lÆ°á»ng sá»± biáº¿n Ä‘á»•i cá»§a gradient vÃ  loss trong má»™t láº§n láº·p duy nháº¥t, vá»›i sá»‘ lÆ°á»£ng bÆ°á»›c khÃ¡c nhau. Váº­y BN sáº½ áº£nh hÆ°á»Ÿng lÃªn gradient trong dÃ i háº¡n nhÆ° tháº¿ nÃ o? Liá»‡u sá»± phá»¥ thuá»™c láº«n nhau giá»¯a cÃ¡c trá»ng sá»‘ cÃ³ áº£nh hÆ°á»Ÿng nÃ o Ä‘áº¿n cáº£nh quan tá»‘i Æ°u hoÃ¡?"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Simple MLP model for Guitar Steel vs Nylon strings classification\n\n\n\n\n\n\n\ncode\n\n\nclassification\n\n\nmlp\n\n\ncnn\n\n\naudio\n\n\nprocessing\n\n\nsignal\n\n\nfft\n\n\n\n\n\n\n\n\n\n\n\nNov 26, 2022\n\n\nLuka Nguyen\n\n\n\n\n\n\n  \n\n\n\n\nVietnamese Lyrics Classification\n\n\n\n\n\n\n\ncode\n\n\nclassification\n\n\ndecision tree\n\n\n\n\n\n\n\n\n\n\n\nOct 21, 2022\n\n\nLuka Nguyen\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Luka Blog",
    "section": "",
    "text": "Simple MLP model for Guitar Steel vs Nylon strings classification\n\n\n\n\n\n\n\ncode\n\n\nclassification\n\n\nmlp\n\n\ncnn\n\n\naudio\n\n\nprocessing\n\n\nsignal\n\n\nfft\n\n\n\n\n\n\n\n\n\n\n\nNov 26, 2022\n\n\nLuka Nguyen\n\n\n\n\n\n\n  \n\n\n\n\nVietnamese Lyrics Classification\n\n\n\n\n\n\n\ncode\n\n\nclassification\n\n\ndecision tree\n\n\n\n\n\n\n\n\n\n\n\nOct 21, 2022\n\n\nLuka Nguyen\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About me",
    "section": "",
    "text": "Iâ€™m passionate about Artificial Intelligence and Music."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About me",
    "section": "Education",
    "text": "Education\nUniversity of Science, Ho Chi Minh | Vietnam BSc in Computer Science | Sept 2011 - June 2015"
  },
  {
    "objectID": "articles.html",
    "href": "articles.html",
    "title": "Articles",
    "section": "",
    "text": "3 Cáº¥p Ä‘á»™ hiá»ƒu vá» Batch Normalization (BÃ i dá»‹ch)\n\n\n\n\n\n\n\ntranslate\n\n\ncnn\n\n\nbatch norm\n\n\nvietnamese\n\n\n\n\n\n\n\n\n\n\n\nJan 21, 2023\n\n\nJohann Huber\n\n\n\n\n\n\nNo matching items"
  }
]