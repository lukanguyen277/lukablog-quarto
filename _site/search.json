[
  {
    "objectID": "projects/steel-nylon-classifier.html",
    "href": "projects/steel-nylon-classifier.html",
    "title": "Simple MLP model for Guitar Steel vs Nylon strings classification",
    "section": "",
    "text": "Can a 2-hidden-layer MLP do a good job classifying musical instrument sounds? Let’s find out!"
  },
  {
    "objectID": "projects/steel-nylon-classifier.html#download-audio-from-youtube",
    "href": "projects/steel-nylon-classifier.html#download-audio-from-youtube",
    "title": "Simple MLP model for Guitar Steel vs Nylon strings classification",
    "section": "4.1 Download audio from YouTube",
    "text": "4.1 Download audio from YouTube\nThese are the clips that I handpicked from YouTube. They are solo guitar recordings and were recorded in a professional studio. To watch any of them, just add the youtube url prefix. For example: “foIPN-T7RGo” ➡️ “youtube.com/watch?v=foIPN-T7RGo”\n\nsteel_clips = [\"foIPN-T7RGo\",\"10ATKnZLg9c\",\"IP8vBL5Q8Ac\"]\nnylon_clips = [\"qgb-bdEEI-M\",\"qXwvz-nTiog\",\"6jQ34uTmA9s\"]\n\nNext, we define our function to download and extract audio from a YouTube url:\n\nfrom pytube import YouTube\n\ndef download_youtube_mp3(link, output_dir):\n    \"\"\"\n    Download and extract audio from a clip from youtube \n    \"\"\"\n    yt=YouTube(f\"youtube.com/watch?v={link}\")\n    t=yt.streams.filter(only_audio=True).first().download(output_dir, link + \".mp3\")\n    print(f\"Downloaded YouTube Audio from: {link}\")\n\nEach clip is over 60 minutes long, which could take a long time to download. To accelerate, we will create a downloading thread for each clip and download all clips simultaneously.\n\ndownload_thread_list = []\n\nfor link in steel_clips:\n  new_thread = threading.Thread(target=download_youtube_mp3, args=(link, RAW_CLIP_PATH + \"steel\"))\n  download_thread_list.append(new_thread)\n\nfor link in nylon_clips:\n  new_thread = threading.Thread(target=download_youtube_mp3, args=(link, RAW_CLIP_PATH + \"nylon\"))\n  download_thread_list.append(new_thread)\n\n\nprint(\"Download Raw Clips starting...\")\n# start each thread\nfor thread in download_thread_list:\n  thread.start()\n\n# wait for all to finish\nfor thread in download_thread_list:\n  thread.join()\n\n# successfully excecuted\nprint(\"Download Raw Clips finished!\")\n\nDownload Raw Clips starting...\nDownloaded YouTube Audio from: foIPN-T7RGo\nDownloaded YouTube Audio from: 6jQ34uTmA9s\nDownloaded YouTube Audio from: qgb-bdEEI-M\nDownloaded YouTube Audio from: qXwvz-nTiog\nDownloaded YouTube Audio from: 10ATKnZLg9c\nDownloaded YouTube Audio from: IP8vBL5Q8Ac\nDownload Raw Clips finished!"
  },
  {
    "objectID": "projects/steel-nylon-classifier.html#segmentize-into-5-second-clips",
    "href": "projects/steel-nylon-classifier.html#segmentize-into-5-second-clips",
    "title": "Simple MLP model for Guitar Steel vs Nylon strings classification",
    "section": "4.2 Segmentize into 5-second clips",
    "text": "4.2 Segmentize into 5-second clips\nNow, let’s create some function to segment each audio clip into segments of 5 second long.\n\ndef segmentize_signal(signal, sr, dur):\n    \"\"\"\n    Segmentize the 1-d signal (mono) to a list of clips with custom duration (dur).\n    \"\"\"\n    seg_len = dur * sr\n\n    # calculate number of segments\n    no_segs = len(signal) // seg_len\n\n\n    # truncate input signal to have length divisiable by seg_len\n    trunc_len = int(no_segs * seg_len)\n\n    # split equally\n    return np.split(signal[:trunc_len], no_segs)\n\ndef save_audio(signal, sr, output_dir, filename):\n    output_path = os.path.join(output_dir, filename)\n    # torchaudio.save(output_path, signal, sr)\n    # print(output_path, sr)\n    sf.write(output_path, signal, sr)\n\ndef segment_audio_file(audio_path, output_dir,  target_sr=TARGET_SR, segment_duration=SEGMENT_DURATION):\n    print(f\"Processing raw clip: {audio_path}\")\n    signal, _ = librosa.load(audio_path, sr=target_sr, mono=True)\n    # signal, target_sr = librosa.load(audio_path,sr=None,  mono=True)\n    print(f\"\\tLoaded clip from disk\")\n    segments_list = segmentize_signal(signal, target_sr, segment_duration)\n    print(f\"\\tSegmented clip into {len(segments_list)} segments\")\n    for seg_idx, seg in enumerate(segments_list):\n        seg_name = f\"{audio_path.split('/')[-1][:-4]}_{seg_idx}.wav\"\n        save_audio(seg, target_sr, output_dir, seg_name)\n    print(f\"\\tSegments are saved completely\")\n\nNext, we use threading to segmentize all clips at the same time. Beware that if your system has less than 32GB of RAM, this could cause the system to freeze and run out of memory. In such case, please modify the code before do it sequentially (i.e. without threading)\n\nthread_list = []\n\nfor cls in CLASSES:\n# get all raw files from subfolders\n    raw_audio_paths = glob(f\"{RAW_CLIP_PATH}{cls}/*mp3\")\n    for audio_path in raw_audio_paths:\n        output_dir = f\"{SEGMENT_DIR}{cls}\"\n        new_thread = threading.Thread(target=segment_audio_file, args=(audio_path, output_dir))\n        thread_list.append(new_thread)\n        \nprint(\"Segmentation starting...\")\n# start each thread\nfor thread in thread_list:\n  thread.start()\n\n# wait for all to finish\nfor thread in thread_list:\n  thread.join()\n\n# successfully excecuted\nprint(\"Segmentation finished!\")\n\nSegmentation starting...\nProcessing raw clip: /workspace/data/raw/nylon/qXwvz-nTiog.mp3\nProcessing raw clip: /workspace/data/raw/nylon/6jQ34uTmA9s.mp3\nProcessing raw clip: /workspace/data/raw/nylon/qgb-bdEEI-M.mp3\nProcessing raw clip: /workspace/data/raw/steel/IP8vBL5Q8Ac.mp3\nProcessing raw clip: /workspace/data/raw/steel/foIPN-T7RGo.mp3\n\n\n/opt/conda/lib/python3.7/site-packages/librosa/util/decorators.py:88: UserWarning: PySoundFile failed. Trying audioread instead.\n  return f(*args, **kwargs)\n/opt/conda/lib/python3.7/site-packages/librosa/util/decorators.py:88: UserWarning: PySoundFile failed. Trying audioread instead.\n  return f(*args, **kwargs)\n\n\nProcessing raw clip: /workspace/data/raw/steel/10ATKnZLg9c.mp3\n\n\n/opt/conda/lib/python3.7/site-packages/librosa/util/decorators.py:88: UserWarning: PySoundFile failed. Trying audioread instead.\n  return f(*args, **kwargs)\n\n\n    Loaded clip from disk\n    Segmented clip into 648 segments\n    Segments are saved completely\n    Loaded clip from disk\n    Segmented clip into 742 segments\n    Segments are saved completely\n    Loaded clip from disk\n    Segmented clip into 1230 segments\n    Segments are saved completely\n    Loaded clip from disk\n    Segmented clip into 1251 segments\n    Segments are saved completely\n    Loaded clip from disk\n    Segmented clip into 1427 segments\n    Segments are saved completely\n    Loaded clip from disk\n    Segmented clip into 2647 segments\n    Segments are saved completely\nSegmentation finished!"
  },
  {
    "objectID": "projects/steel-nylon-classifier.html#create-annotations",
    "href": "projects/steel-nylon-classifier.html#create-annotations",
    "title": "Simple MLP model for Guitar Steel vs Nylon strings classification",
    "section": "5.1 Create annotations",
    "text": "5.1 Create annotations\nBefore creating our own dataset class, we need to have a csv file to describe our training / val / test sets.\nThis annotation dataframe stores the paths to each audio sample and its label:\n\nannotation_dict = {\"audio_path\": [], \"label\": []}\n\n\nfor label, cls in enumerate(CLASSES):\n  wav_dirs = f\"{SEGMENT_DIR}{cls}/*wav\"\n  audio_path_list = glob(wav_dirs)\n  count_audio_files = len(audio_path_list)\n  label_list = [label] * count_audio_files\n\n  annotation_dict[\"audio_path\"] += audio_path_list\n  annotation_dict[\"label\"]      += label_list\n\n\nannotation_df = pd.DataFrame.from_dict(annotation_dict)\nannotation_df.tail()\n\n\n\n\n\n  \n    \n      \n      audio_path\n      label\n    \n  \n  \n    \n      7940\n      ./data/segments/steel/foIPN-T7RGo_575.wav\n      1\n    \n    \n      7941\n      ./data/segments/steel/10ATKnZLg9c_545.wav\n      1\n    \n    \n      7942\n      ./data/segments/steel/10ATKnZLg9c_1035.wav\n      1\n    \n    \n      7943\n      ./data/segments/steel/10ATKnZLg9c_602.wav\n      1\n    \n    \n      7944\n      ./data/segments/steel/IP8vBL5Q8Ac_1146.wav\n      1\n    \n  \n\n\n\n\nThe data is quite enormously for an average system. That’s why I seperated the training data set to full, half, quarter, and one eighth. This allows me to build and test model fast (by using a smaller training dataset). When I find something that works well, I can then use a larger training dataset to improve the training.\n\ntrain_df_full = annotation_df.sample(frac=TRAIN_SIZE, random_state=RANDOM_SEED)\nval_df = annotation_df.drop(train_df_full.index, axis=0)\n\n# make smaller train datasets for quick experimentations\ntrain_df_half = train_df_full.sample(frac=1/2, random_state=RANDOM_SEED)\ntrain_df_quarter = train_df_full.sample(frac=1/4, random_state=RANDOM_SEED)\ntrain_df_1eight = train_df_full.sample(frac=1/8, random_state=RANDOM_SEED)\n\nWe have 4816 samples of NYLON, and 3129 of STEEL\n\nannotation_df[\"label\"].value_counts()\n\n0    4816\n1    3129\nName: label, dtype: int64\n\n\nFinally, let’s write them to CSV files for later use.\n\ndf_list = [train_df_full, train_df_half, train_df_quarter, train_df_1eight, val_df]\n\ndf_names = [\"train_df_full\", \"train_df_half\", \"train_df_quarter\", \"train_df_1eight\", \"val_df\"]\n\nfor df_name, df_content in zip(df_names, df_list):\n    df_content.to_csv(f\"{DATA_DIR}{df_name}.csv\", index=False)"
  },
  {
    "objectID": "projects/steel-nylon-classifier.html#dataset-class",
    "href": "projects/steel-nylon-classifier.html#dataset-class",
    "title": "Simple MLP model for Guitar Steel vs Nylon strings classification",
    "section": "5.2 Dataset class",
    "text": "5.2 Dataset class\nWe create GuitarSoundDataset which inherets Dataset from PyTorch. This class holds the annotation that we created earlier and helps us access and preprocess each individual input and label.\nTo create this class, I took inspiration from this awesome Deep Learning for Audio channel: https://www.youtube.com/watch?v=iCwMQJnKk2c&t=1s&ab_channel=ValerioVelardo-TheSoundofAI\n\nfrom torch.utils.data import Dataset\n\nclass GuitarSoundDataset(Dataset):\n\n    def __init__(self,\n                 annotations_file,\n                 transformation,\n                 target_sample_rate,\n                 num_samples,\n                 device,\n                 audio_col=\"audio_path\",\n                 label_col=\"label\"):\n        self.annotations = pd.read_csv(annotations_file)\n        self.device = device\n        if transformation:\n          self.transformation = transformation.to(self.device)\n        else:\n          self.transformation = None\n        self.target_sample_rate = target_sample_rate\n        self.num_samples = num_samples\n        self.audio_col = audio_col\n        self.label_col = label_col\n\n    def __len__(self):\n        return len(self.annotations)\n\n    def __getitem__(self, index):\n        audio_sample_path = self.__get_audio_sample_path(index)\n        label = self.__get_audio_sample_label(index)\n        signal, sr = torchaudio.load(audio_sample_path)\n        if signal.dim() < 2:\n          signal = signal[None, :]\n        signal = signal.to(self.device)\n        signal, sr = self.preprocess_signal(signal, sr)\n        if self.transformation:\n          signal = self.transformation(signal)\n        return signal, label\n\n    def preprocess_signal(self, signal, sr):\n        signal = self.__resample_if_necessary(signal, sr)\n        signal = self.__mix_down_if_necessary(signal)\n        signal = self.__cut_if_necessary(signal)\n        signal = self.__right_pad_if_necessary(signal)\n        return signal, sr\n\n    def __cut_if_necessary(self, signal):\n        if signal.shape[1] > self.num_samples:\n            signal = signal[:, :self.num_samples]\n        return signal\n\n    def __right_pad_if_necessary(self, signal):\n        length_signal = signal.shape[1]\n        if length_signal < self.num_samples:\n            num_missing_samples = self.num_samples - length_signal\n            last_dim_padding = (0, num_missing_samples)\n            signal = torch.nn.functional.pad(signal, last_dim_padding)\n        return signal\n\n    def __resample_if_necessary(self, signal, sr):\n        if sr != self.target_sample_rate:\n            resampler = torchaudio.transforms.Resample(sr, self.target_sample_rate).to(self.device)\n            signal = resampler(signal)\n        return signal\n\n    def __mix_down_if_necessary(self, signal):\n        if signal.shape[0] > 1:\n            signal = torch.mean(signal, dim=0, keepdim=True)\n        return signal\n\n    def __get_audio_sample_path(self, index):\n        path = self.annotations.iloc[index, :][self.audio_col]\n        return path\n\n    def __get_audio_sample_label(self, index):\n        label =  self.annotations.iloc[index, :][self.label_col]\n        return torch.tensor(label, dtype=torch.float)"
  },
  {
    "objectID": "projects/steel-nylon-classifier.html#dataloader",
    "href": "projects/steel-nylon-classifier.html#dataloader",
    "title": "Simple MLP model for Guitar Steel vs Nylon strings classification",
    "section": "5.3 DataLoader",
    "text": "5.3 DataLoader\n\nfrom torch.utils.data import DataLoader\n\ndef create_data_loader(dataset, batch_size):\n    dataset_loader = DataLoader(dataset, batch_size=batch_size)\n    return dataset_loader\n\nMel Spectrogram transforms our signal from time-domain into frequency-domain, which helps not only human but also computers to understand the characteristic of sound input better. Thus, we need to transform each audio input into mel spec before feeding it into the neural network.\n\nmel_spectrogram = torchaudio.transforms.MelSpectrogram(\n      sample_rate=TARGET_SR,\n      n_fft=1024,\n      hop_length=512,\n      n_mels=64\n  )\n\ntrain_dataset = GuitarSoundDataset(\n                      annotations_file =f\"{DATA_DIR}train_df_half.csv\",\n                      transformation = mel_spectrogram,\n                      target_sample_rate = TARGET_SR,\n                      num_samples = TARGET_SR * SEGMENT_DURATION,\n                      device = device)\nprint(f\"There are {len(train_dataset)} samples in the TRAIN dataset.\")\n\nval_dataset = GuitarSoundDataset(f\"{DATA_DIR}val_df.csv\",\n                      transformation = mel_spectrogram,\n                      target_sample_rate = TARGET_SR,\n                      num_samples = TARGET_SR * SEGMENT_DURATION,\n                      device = device)\nprint(f\"There are {len(val_dataset)} samples in the VAL dataset.\")\n\nThere are 3774 samples in the TRAIN dataset.\nThere are 397 samples in the VAL dataset.\n\n\nWe will take one sample out to find out the exact input shape for our neural network\n\nsignal_sample, _ = val_dataset[0]\nsignal_sample.shape\n\ntorch.Size([1, 64, 79])"
  },
  {
    "objectID": "projects/steel-nylon-classifier.html#training-loop",
    "href": "projects/steel-nylon-classifier.html#training-loop",
    "title": "Simple MLP model for Guitar Steel vs Nylon strings classification",
    "section": "6.1 Training Loop",
    "text": "6.1 Training Loop\nBecause the training and validating loops are pretty basic, I don’t delve into these code too much. The official tutorial is where I took inspiration from: https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n\ndef compute_accuracy(preds, target):\n  _preds = preds.detach().cpu().numpy()\n  _target = target.detach().cpu().numpy()\n  return np.mean(_preds.squeeze().round() == _target.squeeze())\n\ndef train_single_epoch(model, data_loader, loss_fn, optimiser, device):\n  size = len(data_loader.dataset)\n  train_losses = []\n  train_accs = []\n\n  model.train(True)\n  for batch, (input, target) in enumerate(data_loader):\n      input, target = input.to(device), target.to(device)\n\n      # calculate loss\n      preds = model(input)\n      loss = loss_fn(preds.squeeze(), target.squeeze())\n      train_losses.append(loss.item())\n\n\n      # backpropagate error and update weights\n      optimiser.zero_grad()\n      loss.backward()\n      optimiser.step()\n\n      # calculate accuracy\n      acc = compute_accuracy(preds, target)\n      train_accs.append(acc)\n\n  return np.mean(train_losses), np.mean(train_accs)\n\ndef validate(model, data_loader, loss_fn, device):\n  # model.train(False)\n  val_losses = []\n  val_accs = []\n  with torch.inference_mode():\n    for input, target in data_loader:\n      input, target = input.to(device), target.to(device)\n\n      # calculate loss\n      preds = model(input)\n      loss = loss_fn(preds.squeeze(), target.squeeze())\n      val_losses.append(loss.item())\n\n      # calculate acc\n      acc = compute_accuracy(preds, target)\n      val_accs.append(acc)\n\n    return np.mean(val_losses), np.mean(val_accs)\n\ndef save_model(model, model_dir):\n  torch.save(model.state_dict(), model_dir)\n\ndef train(model, train_dataloader, test_dataloader, loss_fn, optimiser, device, epochs, save_best=True, model_dir=\"bestmodel.pth\"):\n  train_losses = []\n  train_accs = []\n  val_losses = []\n  val_accs = []\n  for i in range(epochs):\n      # training\n      train_loss, train_acc = train_single_epoch(model, train_dataloader, loss_fn, optimiser, device)\n      # val\n      val_loss, val_acc = validate(model, test_dataloader, loss_fn, device)\n      print(f\"Epoch {i+1} | train loss: {train_loss:.5f}, train acc: {train_acc:.3%} | val loss: {val_loss:.5f}, val acc: {val_acc:.3%}\")\n\n      # save best val acc\n      if save_best and len(val_losses) > 0 and val_acc > np.max(val_accs):\n        # save model\n        print(\"-> Best Model found! Saving to disk...\")\n        save_model(model, model_dir)\n\n      # update losses\n      train_losses.append(train_loss)\n      val_losses.append(val_loss)\n      train_accs.append(train_acc)\n      val_accs.append(val_acc)\n  print(\"Finished training\")\n  return train_losses, train_accs, val_losses, val_accs\n\n\ndef plot_model(model_history):\n    train_losses, train_accs, val_losses, val_accs = model_history\n    # Plot Loss\n    plt.plot(range(len(train_losses)), train_losses, label='Training Loss')\n    plt.plot(range(len(train_losses)), val_losses, label='Validation Loss')\n    \n    # Add in a title and axes labels\n    plt.title('Training and Validation Loss')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend(loc=\"upper left\")\n    plt.show()    \n    \n    # Plot Acc\n    plt.plot(range(len(train_accs)), train_accs, label='Training Acc')\n    plt.plot(range(len(train_accs)), val_accs, label='Validation Acc')\n    \n    # Add in a title and axes labels\n    plt.title('Training and Validation Acc')\n    plt.xlabel('Epochs')\n    plt.ylabel('Accuracy')\n    plt.legend(loc=\"upper left\")\n    plt.show()\n    \ndef describe_model_stats(model_history):\n    train_losses, train_accs, val_losses, val_accs = model_history\n    history = {\"train_losses\": train_losses, \"train_accs\": train_accs, \"val_losses\": val_losses, \"val_accs\": val_accs}\n    print(pd.DataFrame.from_dict(history).describe())"
  },
  {
    "objectID": "projects/steel-nylon-classifier.html#mlp-model-building-2-hidden-layers-with-relu-activation",
    "href": "projects/steel-nylon-classifier.html#mlp-model-building-2-hidden-layers-with-relu-activation",
    "title": "Simple MLP model for Guitar Steel vs Nylon strings classification",
    "section": "6.2 MLP Model Building: 2 hidden layers with ReLu Activation",
    "text": "6.2 MLP Model Building: 2 hidden layers with ReLu Activation\nI define a simple MLP with 2 hidden fully connected layers with relu activation. The final output is then taken by sigmoid to produce probabily prediction.\n\nfrom torch import nn\nfrom torchsummary import summary\n\n\nclass MLPNetwork(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        self.flatten = nn.Flatten()\n        self.linear = nn.Sequential(\n            nn.Linear(1 * 64 * 79, 256), # I got the number (1 * 64 * 79) as input size from the code above\n            nn.ReLU(),\n            nn.Linear(256, 128),\n            nn.ReLU(),\n            nn.Linear(128, 1),\n        )\n\n    def forward(self, input_data):\n        x = self.flatten(input_data)\n        logits = self.linear(x)\n        predictions = torch.sigmoid(logits)\n        return predictions\n        # return x\n\n\nif __name__ == \"__main__\":\n    model2 = MLPNetwork()\n    summary(model2.to(device), (1, 64, 79))\n\n----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n           Flatten-1                 [-1, 5056]               0\n            Linear-2                  [-1, 256]       1,294,592\n              ReLU-3                  [-1, 256]               0\n            Linear-4                  [-1, 128]          32,896\n              ReLU-5                  [-1, 128]               0\n            Linear-6                    [-1, 1]             129\n================================================================\nTotal params: 1,327,617\nTrainable params: 1,327,617\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.02\nForward/backward pass size (MB): 0.04\nParams size (MB): 5.06\nEstimated Total Size (MB): 5.13\n----------------------------------------------------------------\n\n\nAudio input is complex, with an audio sample of 5-second long at 8000 Hz sampling rate, we have an input of 5056 already.\nAnd, this simple MLP model already has 1.3+ millions params.\nNow, let’s create a folder to store our trained params.\n\nMODEL_DIR = f\"{ROOT_DIR}weights/\"\n\nif not os.path.exists(MODEL_DIR):\n    os.makedirs(MODEL_DIR)\n\nThen, define some hyper params for training and create dataloader for each training and validation dataset\n\nBATCH_SIZE = 128\nEPOCHS = 15\nLEARNING_RATE = 0.001\n    \ntrain_dataloader = create_data_loader(train_dataset, BATCH_SIZE)\nval_dataloader = create_data_loader(val_dataset, BATCH_SIZE)\n\nNow, let’s train our model!\n\nMODEL_SAVE_PATH = f\"{MODEL_DIR}model_mlp1.pth\"\nprint(f\"Best models will saved to: {MODEL_DIR} (based on val acc)\")\n\nmodel1 = MLPNetwork()\n\n\nif os.path.exists(MODEL_SAVE_PATH):\n  model1.load_state_dict(torch.load(MODEL_SAVE_PATH, map_location=torch.device(device)))\n\nmodel1 = model1.to(device)\n\n# initialise loss funtion + optimiser\nloss_fn = nn.BCELoss()\n\noptimiser = torch.optim.Adam(model1.parameters(),\n                                lr=LEARNING_RATE)\n\n# train model\nhistory_model1 = train(model1, train_dataloader, val_dataloader, loss_fn, optimiser, device, EPOCHS, save_best=True, model_dir=MODEL_SAVE_PATH)\n\nBest models will saved to: ./weights/ (based on val acc)\nEpoch 1 | train loss: 18.60054, train acc: 74.353% | val loss: 18.41207, val acc: 79.943%\nEpoch 2 | train loss: 16.17105, train acc: 80.607% | val loss: 14.15756, val acc: 82.287%\n-> Best Model found! Saving to disk...\nEpoch 3 | train loss: 16.04773, train acc: 79.716% | val loss: 22.44626, val acc: 72.251%\nEpoch 4 | train loss: 15.91658, train acc: 80.841% | val loss: 24.57104, val acc: 72.446%\nEpoch 5 | train loss: 15.68584, train acc: 81.720% | val loss: 26.59615, val acc: 71.274%\nEpoch 6 | train loss: 17.09794, train acc: 80.188% | val loss: 15.84533, val acc: 81.671%\nEpoch 7 | train loss: 15.53885, train acc: 82.014% | val loss: 15.38519, val acc: 80.364%\nEpoch 8 | train loss: 12.86597, train acc: 84.409% | val loss: 19.71215, val acc: 75.931%\nEpoch 9 | train loss: 12.15642, train acc: 85.247% | val loss: 13.57315, val acc: 81.926%\nEpoch 10 | train loss: 11.81812, train acc: 84.438% | val loss: 12.96833, val acc: 81.145%\nEpoch 11 | train loss: 10.44457, train acc: 86.577% | val loss: 10.41160, val acc: 82.677%\n-> Best Model found! Saving to disk...\nEpoch 12 | train loss: 8.54932, train acc: 88.063% | val loss: 7.47481, val acc: 84.826%\n-> Best Model found! Saving to disk...\nEpoch 13 | train loss: 7.18438, train acc: 88.478% | val loss: 6.82821, val acc: 83.263%\nEpoch 14 | train loss: 5.54964, train acc: 89.264% | val loss: 4.69765, val acc: 84.405%\nEpoch 15 | train loss: 2.19403, train acc: 89.026% | val loss: 1.49113, val acc: 84.075%\nFinished training\n\n\n\ndescribe_model_stats(history_model1)\nplot_model(history_model1)\n\n       train_losses  train_accs  val_losses   val_accs\ncount     15.000000   15.000000   15.000000  15.000000\nmean      12.388064    0.836627   14.304709   0.798988\nstd        4.770562    0.042507    7.314085   0.046287\nmin        2.194033    0.743532    1.491129   0.712740\n25%        9.496942    0.807237    8.943201   0.779372\n50%       12.865967    0.844086   14.157557   0.816707\n75%       15.982155    0.873198   19.062112   0.829703\nmax       18.600537    0.892641   26.596150   0.848257"
  },
  {
    "objectID": "projects/vietnamese-lyrics-classifier.html",
    "href": "projects/vietnamese-lyrics-classifier.html",
    "title": "Vietnamese Lyrics Classification",
    "section": "",
    "text": "Liệu rằng phần lời của một bài hát có đầy đủ thông tin để chúng ta phân loại chủ đề bài hát đó? Trong bài viết này, chúng ta sẽ dùng machine learning để trả lời câu hỏi này với các phương pháp Logistic Regression (PyTorch) / Naive Bayes / Genetic Algorithm / Decision Tree nhé!"
  },
  {
    "objectID": "projects/vietnamese-lyrics-classifier.html#trích-xuất-dữ-liệu",
    "href": "projects/vietnamese-lyrics-classifier.html#trích-xuất-dữ-liệu",
    "title": "Vietnamese Lyrics Classification",
    "section": "2.1 Trích xuất dữ liệu",
    "text": "2.1 Trích xuất dữ liệu\nCảm ơn loibaihathot.com đã có một kho lời bài hát khá nhiều và dễ trích xuất. Tất cả những gì chúng ta cần chỉ là thư viện requests để lấy nội dung html của trang web. Và, BeautifulSoup để parse nội dung html ra element cho dễ trích xuất.\n\n# Import thư viện cần thiết\n\nimport requests\nimport time\nimport csv\nimport re\nfrom bs4 import BeautifulSoup\n\nXác định 1 số chủ đề có sẵn trên trang web để tài về. Base URL sẽ là url gốc, từ đây chúng ta replace {genre} để tải lời cho bài hát thuộc chủ đề tương ứng.\nLưu ý: Ở đây mình dùng từ genre không sát nghĩa “chủ đề” đâu nghen.\n\ngenre_list = ['cach-mang', 'que-huong', 'thieu-nhi', 'tre']\n\nbase_url = \"https://loibaihathot.com/{genre}\"\n\nTiếp theo, chúng ta tải hết tất cả các URL của các bài hát, phân loại theo chủ đề\n\nlyric_url_list = {}\n\nfor genre in genre_list:\n  url = base_url.format(genre=genre)\n  html_text = requests.get(url).text\n  soup = BeautifulSoup(html_text, 'html.parser')\n  all_links = list(map(lambda e: e['href'], soup.find_all('a')))\n  lyric_url_list[genre] = list(set(filter(lambda url: '/20' in url, all_links)))\n\nTạo thư mục data để lưu trữ dữ liệu, tránh việc phải chạy Trích xuất lại lần nữa:\n\nimport os\n\nDATA_FOLDER = 'data/'\n\ndef create_path_if_nonexist(path):\n  if not os.path.exists(path):\n    os.mkdir(path)\n\ncreate_path_if_nonexist(DATA_FOLDER)\n\nViết hàm để tải nội dung lời bài hát cho một bài hát, với input là đường dẫn đến bài hát đó:\n\nimport re\n\ndef download_lyric(song_url):\n  html_text = requests.get(song_url).text\n  soup = BeautifulSoup(html_text, 'html.parser')\n  lyric = soup.find('div', class_=\"entry-content content mt-6\").get_text(separator = '\\n', strip = True)\n  lyric = re.sub('\\n.+','',lyric, count=3).strip()\n  return lyric\n\nCòn đây là hàm để tải lời nhiều bài cùng 1 lúc, hàm này sẽ dùng để phân luồng threading -> tăng tốc download:\n\ndef download_songs(song_urls, genre):\n  full_path = os.path.join(DATA_FOLDER, genre)\n  create_path_if_nonexist(full_path)\n  for url in song_urls:\n    lyric = download_lyric(url)\n    if len(lyric) < 10:\n      continue\n    file_name =re.findall('/[^\\/]+$', url)[0][1:-5]\n    with open(f'{full_path}/{file_name}.txt', 'w') as f:\n      f.write(lyric)\n\nVới mỗi chủ đề, chúng ta sẽ dùng threading để tải hết lời bài hát về. Để tải nhanh hơn, chúng ta có thể chia thêm nhiều threads con nữa. Nhưng mình muốn mọi thứ đơn giản trước đã:\n\nimport threading\n\nthread_list = []\n\n# create list of threads\nfor genre in genre_list:\n  thread = threading.Thread(target=download_songs, args=(lyric_url_list[genre],genre))\n  thread_list.append(thread)\n\nprint(\"Download starting...\")\n# start each thread\nfor thread in thread_list:\n  thread.start()\n\n# wait for all to finish\nfor thread in thread_list:\n  thread.join()\n\n# successfully excecuted\nprint(\"Download finished!\")\n\nDownload starting...\nDownload finished!"
  },
  {
    "objectID": "projects/vietnamese-lyrics-classifier.html#tiền-xử-lý-pre-processing",
    "href": "projects/vietnamese-lyrics-classifier.html#tiền-xử-lý-pre-processing",
    "title": "Vietnamese Lyrics Classification",
    "section": "2.2 Tiền xử lý (Pre-processing)",
    "text": "2.2 Tiền xử lý (Pre-processing)\nSau khi mình và train thử cho bộ dữ liệu ban đầu thì thấy độ chính xác đều dưới 20%. Kiểm tra lại thì thấy có nhiều bài download về vào sai thư mục chủ đề. Điều này có thể gây nhiễu dữ liệu, nên chúng ta sẽ xóa thủ xông những files sau để tăng độ chính xác khi huấn luyện:\n\n# Clean-up data\n\nto_delete = {\n    \"thieu-nhi\":[\n        \"cam-on-nguoi-da-roi-xa-toi.txt\",\n        \"chac-ai-do-se-ve.txt\",\n        \"dung-tin-em-manh-me.txt\",\n        \"hay-ra-khoi-nguoi-do-di.txt\",\n        \"khuon-mat-dang-thuong.txt\",\n    ],\n    \n    \"cach-mang\":[\n        \"cam-on-nguoi-da-roi-xa-toi.txt\",\n        \"chac-ai-do-se-ve.txt\",\n        \"dung-tin-em-manh-me.txt\",\n        \"hay-ra-khoi-nguoi-do-di.txt\",\n        \"khuon-mat-dang-thuong.txt\",\n    ],\n    \n    \"que-huong\":[\n        \"cam-on-nguoi-da-roi-xa-toi.txt\",\n        \"chac-ai-do-se-ve.txt\",\n        \"dung-tin-em-manh-me.txt\",\n        \"hay-ra-khoi-nguoi-do-di.txt\",\n        \"khuon-mat-dang-thuong.txt\",\n    ],\n\n}\n\nfor subfolder, filenames in to_delete.items():\n    for filename in filenames:\n      try:\n        filepath = f\"{DATA_FOLDER}{subfolder}/{filename}\"\n        os.remove(filepath)\n      except Exception as e:\n        print(e)\n\nCài thêm thư viện underthesea để hỗ trợ tokenize ngôn ngữ Việt:\n\n!pip install -q underthesea==1.3.5a3\n\nTiền xử lý lời bài hát, chủ yếu là xóa ký tự đặc biệt:\n\ndef preprocess_lyric(text):\n  # remove special characters\n  import re\n  text = re.sub('[^\\w\\s]','', text).lower()\n  return text\n\npreprocess_lyric('Ngày mai??!! 13 em đi!')\n\n'ngày mai 13 em đi'"
  },
  {
    "objectID": "projects/vietnamese-lyrics-classifier.html#xây-dựng-mô-hình",
    "href": "projects/vietnamese-lyrics-classifier.html#xây-dựng-mô-hình",
    "title": "Vietnamese Lyrics Classification",
    "section": "3.1 Xây dựng mô hình",
    "text": "3.1 Xây dựng mô hình\nChúng ta import những library cần thiết của PyTorch\n\nimport torch\nimport matplotlib.pyplot as plt\nfrom torch import nn\nfrom torch import optim\nfrom torchvision import datasets, transforms\n\nseed = 69\ntorch.manual_seed(seed)\n\n<torch._C.Generator at 0x7f4a9c335130>\n\n\nKhai báo mô hình gồm 2 lớp: 1. Linear: Nhận input là tensor của bài hát được tokenize thành tensor có n_features và output là tensor có số chiều là n_labels tương ứng với số lượng chủ đề cần phân loại 2. LogSoftMax: Đây là dạng activation function cho bài toán phân loại đa lớp\n\nmodel = nn.Sequential(nn.Linear(n_features, n_labels),nn.LogSoftmax(dim=1))\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(), lr=0.001)\n\nHàm tính độ chính xác của mô hình:\n\ndef calculate_accuracy(model, X, labels):\n  y_hat = model(torch.as_tensor(X).float())\n  preds = y_hat.max(axis=1, keepdim=True)[1].numpy().squeeze()\n  correct = np.sum(preds == labels)\n  accuracy = correct / len(labels)\n  return accuracy\n\nTiến hành train model\n\nepochs = 300\n\nlosses = []\naccs = []\nfor e in range(epochs+1):\n  optimizer.zero_grad()\n  # tính y_hat\n  output = model(X_train_tensor)\n\n  # tính loss\n  loss = criterion(output, y_train_onehot_tensor)\n\n  # tính gradient\n  loss.backward()\n\n  # tối ưu gradient\n  optimizer.step()\n\n  # cập nhật loss\n  running_loss = loss.item()\n  losses.append(running_loss)\n\n  # tính accuracy\n  acc = calculate_accuracy(model, X_test, y_test)\n  accs.append(acc)\n\n  # in thông số training\n  if e % 100 == 0:\n    print(f\"Training epoch {e} : loss: {running_loss:.3f}; accuracy: {acc:.2%}\")\n\nTraining epoch 0 : loss: 9201.797; accuracy: 40.00%\nTraining epoch 100 : loss: 16386.709; accuracy: 76.67%\nTraining epoch 200 : loss: 6421.723; accuracy: 56.67%\nTraining epoch 300 : loss: 906.633; accuracy: 86.67%\n\n\nMô hình chúng ta sau khi train 300 epoch có accuracy 86.67%.\n\nplt.plot(losses)\nplt.xlabel('epoch')\nplt.ylabel('Loss')\nplt.title('Model losses over time (epoch)')\nplt.show()\n\n\n\n\n\nplt.plot(accs)\nplt.xlabel('epochs')\nplt.ylabel('Loss')\nplt.title('Model accuracy over time (epoch)')\nplt.show()\n\n\n\n\nHàm sau dùng để phân loại lời bài hát bất kỳ.\n\ndef predict_label(model, lyric):\n  x = lyric_to_features(lyric, n_labels)\n  y_hat = model(torch.as_tensor(np.array([x])).float())\n  pred = y_hat.max(axis=1, keepdim=True)[1].numpy().squeeze()\n  return genre_list[pred]"
  },
  {
    "objectID": "projects/vietnamese-lyrics-classifier.html#predict",
    "href": "projects/vietnamese-lyrics-classifier.html#predict",
    "title": "Vietnamese Lyrics Classification",
    "section": "3.2 Predict",
    "text": "3.2 Predict\nỞ đây chúng ta sẽ phân loại thử một số bài hát một ách cách thủ công nhé.\nĐây là bài “Hát về anh”, thuộc thể loại cách mạng. Mô hình đã phân loại chính xác trường hợp này.\n\nlyric = '''Hát Về Anh\nMột ba lô, cây súng trên vai,\nNgười chiến sĩ quen với gian lao,\nNgày dài đêm thâu vẫn có những người lính trẻ,\nNặng tình quê hương canh giữ trên miền đất mẹ.\nRừng âm u, mây núi mênh mông\nNgày nắng cháy, đêm giá lạnh đầy.\nRừng mờ sương khuya bóng tối quân thù trước mặt,\nNặng tình non sông anh dâng trọn tuổi đời thanh xuân.\nCho em thơ ngủ ngon và vui bước sớm hôm đến trường\nCho yên vui mùa xuân đôi lứa còn hẹn hò ước mơ\nÐã có những hy sinh khó nói hết bằng lời\nNên đọng lại trong tôi những nghĩ suy.\nCho tôi bài ca về người chiến sĩ nơi tuyến đầu.\nNơi biên cường rừng sâu, anh âm thầm chịu đựng gió sương.\nĐã có những gian lao, đã có những nhọc nhằn\nMang trong trái tim anh trọn niềm tin.\nXin hát mãi về anh người chiến sĩ biên cương\nXin hát mãi về anh người chiến sĩ biên cương\nNghe lời bài hát Hát Về Anh\nHát Về Anh '''\n\npredict_label(model, lyric)\n\n'cach-mang'\n\n\nBài hát Dây Đủng Đỉnh Buồn được phân loại Quê Hương trong dữ liệu. Đây cũng là một dự đoán chính xác.\n\nlyric = '''Dây Đủng Đỉnh Buồn (Remix)\nEm đi theo chồng xa thôn làng cách biệt dòng sông.\nEm đi theo chồng anh nơi này mỏi mon` đợi trông\nNhư dây đủng đỉnh nuôi trái tình bào tháng ngày qua.\nTình đã trọng xanh rồi người nỡ đem đi hái cho đành.\nAi xuôi chỉ mình ôm nỗi buồn cho người ta vui.\nAi xuôi chỉ mình xây duyên tình giờ đây lẻ loi.\nNhìn con nước chảy theo con thuyền lạc bến đời nhau.\nLời ước hẹn xưa giờ thì cũng xa xa cuối chân trời.\nĐK:\nĐau thương thui thủi đêm trương, gió lạnh từng đêm lẻ bóng đơn côi.\nBuồn miên man thầm trách cho đời lơ lửng chi rồi bỏ bạn mình ên.\nYêu thương xin trả cho người nuốt lệ nhìn theo đám cưới người ta.\nĐể bên đây đủng đỉnh u buồn, sao mang ân tình trao tặng người ta.\nNghe lời bài hát Dây Đủng Đỉnh Buồn (Remix)\n\n \nDây Đủng Đỉnh Buồn (Remix)'''\n\npredict_label(model, lyric)\n\n'que-huong'\n\n\nBài hát thiếu nhi này cũng được phân loại đúng:\n\nlyric = '''Ai yêu bác Hồ Chí Minh hơn thiếu nhi Việt Nam\nBác chúng em dáng cao cao, người thanh thanh\nBác chúng em mắt như sao, râu hơi dài\nBác chúng em nước da nâu vì sương gió\nBác chúng em thề cương quyết trả thù nhà\nHồ Chí Minh kính yêu, chúng em kính yêu Bác Hồ Chí Minh trọn một đời\nHồ Chí Minh kính yêu Bác đã bao phen bôn ba nước ngoài vì giống nòi\nBác nay tuy đã già rồi\nGià rồi nhưng vẫn vui tươi\nNgày ngày chúng cháu ước mơ\nMong sao Bác sống muôn đời để dẫn dắt nhi đồng thành người và kiến thiết nước nhà bằng Người\nHồ Chí Minh kính yêu, chúng em kính yêu Bác Hồ Chí Minh trọn một đời\nHồ Chí Minh kính yêu, chúng em ước sao Bác Hồ Chí Minh sống muôn đời\nAi Yêu Bác Hồ Chí Minh Hơn Chúng Em Nhi Đồng'''\n\npredict_label(model, lyric)\n\n'thieu-nhi'\n\n\nBài hát “Ánh nắng của anh” được phân loại chính xác cho thể loại “Trẻ”\n\nlyric = '''Những phút giâу trôi qua tầm taу\nϹhờ một ai đó đến bên anh\nLặng nghe những tâm tư nàу\nLà tia nắng ấm\nLà em đến bên anh cho vơi đi ưu phiền ngàу hôm qua\nNhẹ nhàng xóa đi bao mâу đen vâу quanh cuộc đời nơi anh\nPhút giâу anh mong đến tình уêu ấу\nGiờ đâу là em, người anh mơ ước bao đêm\nЅẽ luôn thật gần bên em\nЅẽ luôn là vòng taу ấm êm\nЅẽ luôn là người уêu em\nϹùng em đi đến chân trời\nLắng nghe từng nhịp tim anh\nLắng nghe từng lời anh muốn nói\nVì em luôn đẹp nhất khi em cười'''\n\npredict_label(model, lyric)\n\n'tre'\n\n\nBây giờ mình sẽ lấy 2 bài hát không trong tập dữ liệu\nBài thứ nhất là một bài nhạc trẻ mới nổi gần đây “Anh Chưa Thương Em Đến Vậy Đâu”:\n\nlyric = '''Sao mình không gạt bỏ đi hết những lời nói ngoài kia\nVà sao mình không gạt bỏ đi hết những định kiến ngoài kia\n\nGiữa ngân hà em biết đâu là\nBiết đâu là thế gian này mà\nMình bên nhau, được yêu nhau, được trao nhau, tình yêu sâu trái tim đậm sâu\n\nGiữa ngân hà em biết đâu là\nBiết đâu một sớm mai khi mà\nCần bao lâu, chờ bao lâu, đợi bao lâu, tình trao nhau mãi thôi đậm sâu\n\nGiữa ngân hà, giữa ngân hà, giữa ngân hà\nBiết đâu là, biết đâu là, biết đâu là\nHành tinh của hai chúng ta\nMột nơi của riêng chúng ta\n\nGiữa ngân hà, giữa ngân hà, giữa ngân hà\nBiết đâu là, biết đâu là, biết đâu là\nHành tinh của hai chúng ta\nỞ 1 thế giới còn rất xa'''\n\npredict_label(model, lyric)\n\n'tre'\n\n\nVới bài hát “Bước qua mùa cô đơn”:\n\nlyric = '''Mùa thu rơi vào em, vào trong giấc mơ hôm qua\nMùa thu ôm mình em, chạy xa vòng tay vội vã\nLời em nói ngày xưa đâu đây\nVẫn âm thầm chìm vào trong mây\nĐến bao giờ, dặn lòng anh không mong nhớ\nMùa thu rơi vào em, vào trong chiếc hôn ngây thơ\nMùa thu không cần anh, vì em giờ đây còn mãi hững hờ\nNgày mai kia nếu có phút giây vô tình thấy nhau sẽ nói câu gì...\nHay ta chỉ nhìn\nLặng lẽ\nĐi qua\nChào cơn mưa\nLàm sao cứ kéo ta quay lại\nNhững rung động con tim\nLần đầu hai ta gặp gỡ'''\n\npredict_label(model, lyric)\n\n'tre'"
  },
  {
    "objectID": "articles/translate-batch-normalization-in-3-levels-of-understanding.html",
    "href": "articles/translate-batch-normalization-in-3-levels-of-understanding.html",
    "title": "3 Cấp độ hiểu về Batch Normalization (Bài dịch)",
    "section": "",
    "text": "Trong series bài dịch này, mình sưu tầm những bài trên nguồn như Medium và dịch lại với mục đích:\nTìm đọc bài gốc ở đây"
  },
  {
    "objectID": "articles/translate-batch-normalization-in-3-levels-of-understanding.html#cách-hiểu-trong-30-giây",
    "href": "articles/translate-batch-normalization-in-3-levels-of-understanding.html#cách-hiểu-trong-30-giây",
    "title": "3 Cấp độ hiểu về Batch Normalization (Bài dịch)",
    "section": "2.1 Cách hiểu trong 30 giây",
    "text": "2.1 Cách hiểu trong 30 giây\nBatch-Normalization (BN) là phương pháp khiến cho việc huấn luyện mạng nơ rông sâu (Deep Nearon Network, DNN) nhanh và ổn định hơn.\nNó bao gồm chuẩn hoá các vectors của lớp ẩn (hidden layers) sử dụng trung bình và phương sai (mean và variance) của batch hiện tại. Bước chuẩn hoá có thể được áp dụng ngay trước hoặc ngay sau một hàm phi tuyến tính.\n\n\n\n\nMultilayer Perceptron (MLP) không batch normalization (BN) | Nguồn : author - Design : Lou HD\n\n\n\n\n\nMultilayer Perceptron (MLP) có batch normalization (BN) | Nguồn : author - Design : Lou HD\n\nTất cả các nền tảng học sâu đều đã hỗ trợ Batch Normalization. Thường bạn sẽ sử dụng BN như một lớp trong mạng DNN.\nVới những ai thích đọc code hơn chữ thì tác giả có triển khai BN dạng Jupyter Notebook ở đây."
  },
  {
    "objectID": "articles/translate-batch-normalization-in-3-levels-of-understanding.html#cách-hiểu-trong-3-phút",
    "href": "articles/translate-batch-normalization-in-3-levels-of-understanding.html#cách-hiểu-trong-3-phút",
    "title": "3 Cấp độ hiểu về Batch Normalization (Bài dịch)",
    "section": "2.2 Cách hiểu trong 3 phút",
    "text": "2.2 Cách hiểu trong 3 phút\nCách tính toán BN là khác nhau cho traing vs testing.\n\n2.2.1 Training\nVới mỗi lớp ẩn (hidden layer), BN chuyển đổi tín hiệu như sau:\n\n\n\nLớp BN đầu tiên xác định trung bình 𝜇 và phương sai σ² của các kích hoạt (activation) trong batch, sử dụng công thức (1) và (2). Tiếp theo, nó chuẩn hoá vector kích hoạt \\(Z^{(i)}\\) với công thức (3). Thế là, tất cả các output đều được tuân theo phân phối chuẩn trong batch đó. (𝜀 là một hằng số giúp duy trì numerical stablity)\n\n\n\n\nBước đầu của Batch Norm. Ví dụ một hidden layer với 3 neurons, kích cỡ batch là b. Mỗi neuron sau đó đều tuân theo phân phối chuẩn | Nguồn : author - Design : Lou HD\n\n\n\n\n\nBatch Norm trên dữ liệu 1 chiều. Mean và variance được trính trên toàn bộ các features của batch | Nguồn: AI Vietnam\n\n\n\n\n\nBatch Norm trên dữ liệu 2 chiều. Mean và variance được trính theo từng channel của input (tương ứng với số filters của lớp Convolution ngay trước đó) | Nguồn: AI Vietnam\n\nỞ bước cuối, BN tính output Ẑ(i) bằng cách áp dụng một biến đổi tuyển tính (linear transformation) với hai tham số huấn luyện là 𝛾 và 𝛽 (4). Bước này cho phép mô hình chọn được phân phối tối ưu cho từng lớp ẩn khi thay đổi hai tham số:\n\n𝛾 giúp điều chỉnh phương sai phân phối\n𝛽 giúp điều chỉnh bias, dịch chuyển phân phối sang trái hay phải\n\n\n\n\n\nLợi ích của tham số 𝛾 và 𝛽: Thay đổi phân phối (hình trên) giúp chúng ta sử dụng các hình thái khác nhau của hàm phi tuyến tính (hình dưới) | Nguồn : author - Design : Lou HD\n\n\nLưu ý: Những lý do giải thích cho sự hiệu quả của BN có thể bị hiểu sai hoặc mắc lỗi (ngay cả trong bài báo gốc). Một bài báo gần đây [2] phủ định vài giả thiết sai và giúp cộng đồng hiểu tốt hơn về BN. Chúng ta sẽ nói rõ hơn trong phần “Tại sao BN hiệu quả?”\n\nVới mỗi lần lặp, network sẽ tính toán trung bình 𝜇 và phương sai σ² cho batch hiện tại. Sau đó nó huấn luyện 𝛾 và 𝛽 bằng gradient descent, sử dụng Đường trung bình động hàm mũ (Exponential Moving Average/EMA) giúp ưu tiên hơn cho những iterations gần nhất.\n\n\n2.2.2 Đánh giá\nKhác với khi huấn luyện, chúng ta có thể không có batch đầy đủ để đưa vào mô hình.\nĐể giải quyết vấn đề này, chúng ta tính (𝜇_pop , σ_pop) với:\n\n𝜇_pop : ước lượng giá trị trung bình cho toàn bộ quần thể (population) được nghiên cứu\nσ_pop : ước lượng giá trị độ lệch chuẩn cho toàn bộ quần thể (population) được nghiên cứu\n\nHai giá trị này được tính toán sử dụng các giá trị (𝜇_batch , σ_batch) được tính trong quá trình huấn luyện, và input thẳng vào công thức (3) ở trên, bỏ qua bước (1) và (2)\n\nLưu ý: Chúng ta sẽ tìm hiểu kỹ hơn vấn đề này ở phần “Chuẩn hoá khi đánh giá”\n\n\n\n2.2.3 Thực tế\nTrong thực tế, chúng ta xem BN như một lớp bình thường, như là một perceptron, convultional layer, hay một hàm kích hoạt hoặc một lớp dropout.\nCác nền tảng thông dụng cũng đã triển khai BN như một layer. Ví dụ:\n\nPytorch: torch.nn.BatchNorm1d, torch.nn.BatchNorm2d, torch.nn.BatchNorm3d\nTensorflow / Keras: tf.nn.batch_normalization, tf.keras.layers.BatchNormalization\n\nTất cả các cách triển khai của BN đều cho phép bạn cấu hình tham số một cách độc lập. Tuy nhiên, kích cỡ của vector đầu vào là quan tọng nhất. Nó nên được thiết lập bằng:\n\nSố nơ-ron của lớp ẩn hiện tại (cho MLP)\nSố filters của lớp ẩn hiện tại (cho CNN)\n\nHãy đọc tài liệu về nền tảng yêu thích của bạn về BN để biết chi tiết hơn về cách sử dụng và triển khai.\n\n\n2.2.4 Tổng quan kết quả\nKể cả khi chúng ta chưa hiểu tất cả cấu tạo của Batch Normalization, có một thứ mà ai cũng phải công nhận: Nó rất hiệu quả!\nĐể hiểu thêm, hãy xem kết quả của bài báo gốc [1]:\n\n\n\n\nẢnh 1: Hiệu quả của BN. Độ chính xác trên tập đánh giá của ImageNet(2012) theo số lần huấn luyện. Năm networks được so sánh: “Inception” là network Inception gốc [3], “BN-X” là Inception network thêm BN (với 3 learning rates: x1, x5, x30 lần Inception tối ưu), “BN-X-Sigmoid” là Inception network thêm BN, nhưng thay ReLU bằng Sigmoid\n\nKết quả rất rõ ràng: Các lớp BN tăng tốc quá trình luấn luyện, hỗ trợ tốt nhiều giá trị learning rate nhưng lại không hi sinh khả năng hội tụ của mô hình.\n\nLưu ý: Đọc đến đây là đủ cho bạn để ứng dụng BN rồi. Tuy nhiên, để tận dụng tối đa được BN thì chúng ta cần đào sâu hơn nữa.\n\n\n\n\n\nBatch Normalization liên quan gì đến hình ảnh này nhỉ? | Nguồn : author - Design : Danilo Alvesd"
  },
  {
    "objectID": "articles/translate-batch-normalization-in-3-levels-of-understanding.html#triển-khai",
    "href": "articles/translate-batch-normalization-in-3-levels-of-understanding.html#triển-khai",
    "title": "3 Cấp độ hiểu về Batch Normalization (Bài dịch)",
    "section": "3.1 Triển khai",
    "text": "3.1 Triển khai\nTác giả đã triển khai các lớp BN bằng Pytorch để tái tạo kết quả từ bài báo gốc. Mã nguồn ở trên repo này\nCác bạn cũng nên tham thảo thêm các cách triển khai BN khác nhau, nó sẽ rất có ích khi bạn thấy được cách các nền tảng DL lập trình BN như thế nào.\n\n3.1.1 Các lớp BN trong thực tế\nTrước khi đi vào lý thuyết, chúng ta sẽ tóm lại vài điều về BN:\n\nBN ảnh hưởng thế nào đến hiệu năng huấn luyện? Tại sao BN lại quan trọng như vậy trong Deep Learning?\nBN có những tác dụng phụ nào mà chúng ta cần lưu tâm?\nKhi nào cần dùng BN và dùng như thế nào?\n\n\n\n3.1.2 Kết quả từ bài báo gốc\nNhư đã nói ở trên, BN được sử dụng rộng rãi vì hầu như lúc nào nó cũng cải thiện hiệu năng của các mô hình học sâu.\nBài báo gốc thực hiện 3 thí nghiệm để minh hoạ thấy rằng phương pháp của họ hiệu quả thế nào.\nĐầu tiên, họ hâuns luyện một mô hình phân loại trên tệp dữ liệu MNIST (chữ số viết tay). Mô hình có 3 lớp fully-connected, mỗi lớp gồm 100 nơ ron, cùng với kích hoạt sigmoid. Họ huấn luyện mô hình này 2 lần (có / không thêm BN) trong 50,000 lần lặp với SGG, và learning rate như nhau (0.01). Tất cả các lớp BN đều đặt ngay sau hàm kích hoạt.\nBạn có thể dễ dàng tái tạo kết quả này mà không cần GPU, đây là một cách tuyệt vời để làm quen với khái niệm này.\n\n\n\n\nHình 2: Ảnh hưởng của BN lên quá trình huấn luyện mạng MLP đơn giản | Trái: độ chính xác khi huấn luyện | Phải: độ sai (loss) khi huấn luyện | Nguồn : tác gỉa\n\nRất tốt! BN nâng cao hiệu năng của mô hình, cả ở độ chính xác lẫn độ sai.\nThí nghiệm thứ hai là về giá trị kích hoạt của lớp ẩn. Sau đây là đồ thị giá trị của lớp ẩn cuối (ngay trước khi áp dụng hàm phi tuyến tính như ReLU hay sigmoid, v.v):\n\n\n\n\nẢnh hưởng của BN lên giá trị kích hoạt | Nguồn : tác giả\n\nKhi không có BN, giá trị kích hoạt giao động nhiều hơn với những lần lặp đầu tiên. Ngược lại, khi có BN, đường cong của giá trị kích hoạt mượt hơn.\n\n\n\n\nẢnh hưởng của BN lên giá trị kích hoạt | Mô hình có giá trị kích hoạt dao động mượt hơn khi thêm BN | Nguồn : tác giả\n\nTín hiệu cũng trở nên ít nhiễu hơn khi thêm lớp BN. Có vẻ như BN làm mô hình hội tụ dễ hơn.\nVí dụ này vẫn chưa minh hoạ được hết lợi ích của Batch Normalization.\nBài báo gốc thực thiện thêm thí nghiệm thứ 3. Nhóm tác giả muốn so sánh hiệu năng của mô hình khi thêm BN với bộ dữ liệu lớn hơn: ImageNet(2012). Để làm vậy, họ huấn luyện một mạng nơ-ron rất mãnh mẽ là Inception. Ban đầu, Inception không sử dụng bất kỳ lớp BN nào. Họ thêm vài lớp BN và huấn luyện với nhiều mức learning rate khác nhau (x1, x5, x30 lần giá trị tối ưu trước đó). Họ cũng thử nghiệm thay tất cả hàm ReLU bằng sigmoid trong một mô hình khác. Cuối cùng, họ so sánh những network đã được thay đổi với mô hình gốc.\n\n\n\n\nẢnh 1: Hiệu quả của BN. Độ chính xác trên tập đánh giá của ImageNet(2012) theo số lần huấn luyện. Năm networks được so sánh: “Inception” là network Inception gốc [3], “BN-X” là Inception network thêm BN (với 3 learning rates: x1, x5, x30 lần Inception tối ưu), “BN-X-Sigmoid” là Inception network thêm BN, nhưng thay ReLU bằng Sigmoid\n\nChúng ta có thể kết luận như sau:\n\nThêm lớp BN giúp hội tụ nhanh hơn và tốt hơn (~ độ chính xác cao hơn)\n\nVới bộ dữ liệu lớn, những cải thiện này càng quan trọng hơn là khi sử dụng những bộ dữ liệu nhỏ hơn như MNIST.\n\nThêm lớp BN cho phép chúng ta sử dụng learning rate lớn hơn nhưng lại không hi sinh tính hội tụ của mô hình.\n\nNhóm tác giả thậm chí còn huấn luyện thành công mô hình Inception kết hợp BN với learning rate lớn gấp 30 lần so với mô hình gốc. Thật ấn tượng, khi chỉ với x5 learning rate thì mô hình gốc đã phân tán rồi.\nBằng cách đó, BN làm cho quá trình tìm learning rate tốt dễ dàng hơn khi khoảng cách từ underfit đến gradient explosion của learning rate được tăng lên đáng kể.\nThêm nữa, learning rate cao hơn giúp mô hình thoát hỏi giá trị tối ưu cục bộ. Nhờ vậy mà optimizer dễ dàng tìm được nghiệm tốt hơn cho quá trình hội tụ.\n\nMô hình sử dụng sigmoid có kết quả khá cạnh tranh so với mô hình sử dụng ReLU\n\nỞ một bức tranh tổng quan hơn, chúng ta thấy rằng mô hình ReLU có hiệu năng khá hơn một chút so với mô hình dùng sigmoid. Tuy nhiên đó không phải là điều quan trọng nhất.\nTác giả Ian Goodfellow (tác giả của GAN) từng nói về BN:\n\nTrước BN, chúng ta nghĩ rằng huấn luyện mô hình học sâu với hàm kích hoạt sigmoid ở các tầng ẩn là không thể. Chúng ta xem xét một vài hướng tiếp cận để xử lý sự bất ổn định khi huấn luyện, ví dụ như các phương pháp khởi tạo tham số. Những mảnh ghép này dựa nhiều vào kinh nghiệm cũng như rất khó để cho ra kết quả thoả mãn. Batch Normalization cho phép chúng ta huấn luyện được cả những mô hình bất ổn định. Đó chính là những gì chúng ta thấy được từ ví dụ này. - Ian Goodfellows (tác giả viết lại theo nguồn: https://www.youtube.com/watch?v=Xogn6veSyxA)\n\nĐến đây, chúng ta đã hiểu được BN có ý nghĩa quan trọng như thế nào trong lĩnh vực Học sâu.\nNhững kết quả trên tạo nên một bức tranh tổng quan về lợi ích của BN khi huấn luyện mô hình. Tuy nhiên, BN có những tác dụng phụ mà chúng ta cần lưu tâm để tận dụng được nó.\n\n\n3.1.3 Tác dụng phụ của BN: Regularization\nBN phụ thuộc vào trung bình 𝜇 và phương sai σ² để chuẩn hoá giá trị kích hoạt. Vì thế mà kết quả đầu ra của BN sẽ bị ảnh hưởng với thống kê của batch hiện tại. Những sự biến đổi sẽ tạo thêm nhiễu, phụ thuộc vào dữ liệu đầu vào của batch hiện tại.\nViệc thêm nhiễu cũng sẽ giúp tránh overfitting… nghe khá giống với regularization, đúng không nhỉ?\nTrong thực nghiệm, chúng ta sẽ không dựa vào BN để xử lý overfitting, vì sự quan trọng của tính trực giao (orthogonality). Nói một cách đơn giản, mỗi module nên chỉ đảm nhiệm một nhiệm vụ. Điều này giúp tránh việc phức tạp hoá quy trình phát triển.\nTuy nhiên, việc biết đến tác dụng phụ này giúp chúng ta giải thích được những hành vi ngoài mong đợi của mô hình.\n\nLưu ý: Khi kích thước batch càng lớn thì tác động lên regularization càng ít đi (do nhiễu ảnh hưởng ít hơn)\n\n\n\n\n\nLàm sao để deploy được mô hình có BN lên hệ thống nhúng? | Nguồn : Marília Castelli\n\n\n\n3.1.4 Chuẩn hoá khi đánh giá\nCó hai trường hợp mà mô hình được gọi là đang ở chế độ đánh giá (evaluation mode):\n\nKhi đang cross-validation hay test (huấn luyện và phát triển mô hình)\nKhi đang deploy mô hình\n\nỞ trường hợp đầu, chúng ta có thể áp dụng Batch Normalization với thống kê từ batch đang xử lý. Tuy nhiên, ở trường hợp sau thì cách này không áp dụng được, vì chúng ta không có đủ một batch để sử dụng.\nHãy cùng xem xét trường hợp một robot với camera nhúng. Có thể chúng ta sẽ sử dụng mô hình để dự đoán vị trí của vật cản đường phía trước. Chúng ta muốn tính toán dự đoán dựa trên 1 khung ảnh duy nhất cho mỗi iteration. Nếu kích cỡ batch là N, thì N-1 inputs còn lại thì chúng ta nên chọn như thế nào để tính toán forward propagation?\nNhớ rằng với mỗi lớp BN, (𝛽, 𝛾) được huấn luyện bằng tín hiệu đã chuẩn hoá. Thế nên chúng ta cần xác định (𝜇, σ) để tạo ra kết quả có ý nghĩa.\nMột giải pháp là chọn giá trị ngẫu nhiên để điền cho đủ batch. Khi đưa batch đầu tiên vào, chúng ta sẽ có một kết quả cho ảnh mà chúng ta quan tâm. Nếu chung ta tạo thêm batch thứ hai với giá trị ngẫu nhiên khác, mô hình sẽ cho dự đoán khác trên cùng một ảnh. Đây không phải là hành vi mong muốn vì mô hình có thể dự đoán khác nhau cho cùng một input.\nCách giải quyết tốt hơn là xác định (𝜇_pop , σ_pop) - ước lượng trung bình và độ lệch chuẩn của quần thể mà ta nhắm đến. Thông số này được tính bằng trung bình của (𝜇_batch, σ_batch) trong quá trình huấn luyện.\n\nCách này có thể dẫn đến sự bất ổn định trong quá trình đánh giá: Chúng ta sẽ thảo luận điều này ở phần tiếp theo\n\n\n\n3.1.5 Tính ổn định của BN layer\nTuy BN khá hiệu quả, nó cũng có thể đôi khi gây ra vấn đề về ổn định. Có trường hợp BN làm giá trị kích hoạt bị explode khi đánh giá (khiến loss=NaN).\nNhư ở trên có đề cập, (𝜇_pop , σ_pop) trong quá trình đánh giá được dựa trên (𝜇_batch, σ_batch) khi huấn luyện.\nThử tưởng tượng một mô hình chỉ được huấn luyện bằng tệp ảnh giày thể thao nhưng khi test lại bằng tập giày derby (kiểu giày Tây)?\n\n\n\n\nNếu phân phối của đầu vào quá khác giữa khi huấn luyện và đánh giá, mô hình có thể phản ứng thái quá với một vài tín hiệu, dẫn đến sự phân tán của giá trị kích hoạt | Nguồn : Grailify & Jia Ye\n\nGiả sử giá trị kích hoạt ở lớp ẩn có phân phối quá khác biệt giữa khi huấn luyện và khi đánh giá, (𝜇_pop, σ_pop) sẽ không ước lượng đúng được trung bình và độ lệch chuẩn của quần thể. Sử dụng bộ giá trị này sẽ đẩy lệch giá trị kích hoạt xa khỏi phân phối chuẩn (𝜇 = 0, σ = 1) -> đánh giá sai giá trị kích hoạt.\n\nHiện tượng này gọi là “covariate shift”, sẽ nói ở phần sau\n\nHiệu ứng trên còn được tăng cường bởi một thuộc tính của BN: Trong khi huấn luyện, giá trị kích hoạt được chuẩn hoá bởi chính giá trị của nó. Còn khi inference, thì tín hiệu lại cũng được sử dụng giá trị (𝜇_pop, σ_pop) được tính khi ở tranining. Thế nên, hệ số của việc chuẩn hoá không bao gồm những giá trị kích hoạt.\nNói chung, tập huấn luyện phải “đủ giống” với tệp đánh giá: Nếu không, việc huấn luyện mô hình gần như là không thể. Trong đa số trường hợp, (𝜇_pop, σ_pop) cũng nên khớp với bộ dữ liệu đánh giá. Nếu không được vậy thì chúng ta sẽ kết luận là tập huấn luyện không đủ lớn, hoặc chất lượng của dữ liệu không đủ tốt cho tác vụ mục tiêu.\nTác giả cũng từng gặp trường hợp này trong cuộc thi Pulmonary Fibrosis Progression Kaggle competition. Tập huấn luyện bao gồm metadata, 3D scans phổi của từng bệnh nhân. Nội dung của những bản scans này phức tạp và phong phú, tuy nhiên nó chỉ từ gần 100 bệnh nhân để chia thành train và validation. Kết quả là CNN dùng để trích xuất đặc trưng mà tác giả dùng chỉ trả về NaN khi mô hình chuyển từ huấn luyện sang đánh giá.\nKhi bạn không thể lấy thêm dữ liệu để bổ sung cho huấn luyện, bạn cần phải tìm cách khác. Trong trường hợp này, tác giả “ép” các lớp BN phải tính lại (𝜇_batch, σ_batch) khi đánh giá. (tác giả tự nhận đây là cách hơi “xấu xí”, nhưng anh ta không còn thời gian).\nThêm các lớp BN vào mô hình mà tự giả định rằng nó không có ảnh hưởng xấu không phải lúc nào cũng là một điều tốt.\n\n\n3.1.6 Recurrent Network và Layer Normalization\nTrong thực nghiệm, nhiều người nhận định rằng:\n\nTrong CNN: Batch Normalization (BN) phù hợp hơn\nTrong RNN: Layer Normalization (LN) phù hợp hơn\n\nTrong khi BN dùng batch hiện tại để chuẩn hoá từng giá trị, LN thì dùng tất cả các layers hiện tại. Nói cách khác, LN chuẩn hoá trên toàn bộ các đặc trưng của dữ liệu thay vì theo từng đặc trưng như BN. Điều này làm LN hiệu quả hơn với RNN. Việc đưa một phương pháp nhất quán cho RNN khá là khó, vì RNN sử dụng phép nhân lặp đi lặp lại với cùng một bộ trọng số. Vậy chúng ta nên chuẩn hoá theo từng step một cách độc lập? Hay là nên tính mean trên toàn steps, hay là chuẩn hoá theo quy hồi? (Tham khảo: YouTube)\nCâu hỏi này nằm ngoài phạm vi của bài viết này.\n\n\n3.1.7 Trước hay sau phi tuyến tính?\nTừ trước đến nay, lớp BN thường đặt ngay sau hàm phi tuyến tính, đây là cách làm đúng theo mục tiêu và giả thiết của tác giả gốc:\n\n“Chúng tôi muốn đảm bảo rằng, với mọi giá trị tham số, mạng lưới lúc nào cũng cho ra giá trị kích hoạt với phân phối mong muốn” — Sergey Ioffe & Christian Szegedy (source : [1])\n\nMột vài thí nghiệm cho thấy việc đặt lớp BN đằng sau hàm phi tuyến tinh cho kết quả tốt hơn. Ví dụ\nFrançois Chollet, cha đẻ của Keras và hiện tại là kỹ sư của Google, cho rằng:\n\nTôi chưa xem lại những khuyến cáo trong paper gốc, nhưng tôi đảm bảo rằng mã nguồn gần đây viết bởi Christian [Szegedy] đặt relu trước BN. Tuy nhiên, vấn đề này cũng hay được tranh luận.\n\nVẫn có nhiều kiến trúc thường dùng trong transfer learning đặt BN trước hàm phi tuyến tính như ResNet, mobilenet-v2, v.v\nNên nhớ rằng trong bài báo [2], khi thách thức những giả thiết của bài báo gốc để giải thích sự hiệu quả của BN thì tác giả lại để lớp BN trước hàm kích hoạt. Tuy nhiên, tác giả lại không đưa ra nguyên nhân thuyết phục cho việc này.\nCho đến nay thì vấn đề này vẫn được thảo luận và tranh cãi. Trên reddit cũng có một thread nói về điều này."
  },
  {
    "objectID": "articles/translate-batch-normalization-in-3-levels-of-understanding.html#tại-sao-bn-lại-hiệu-quả",
    "href": "articles/translate-batch-normalization-in-3-levels-of-understanding.html#tại-sao-bn-lại-hiệu-quả",
    "title": "3 Cấp độ hiểu về Batch Normalization (Bài dịch)",
    "section": "3.2 Tại sao BN lại hiệu quả?",
    "text": "3.2 Tại sao BN lại hiệu quả?\nTrong đa số trường hợp, BN gia tăng hiệu năng của mô hình học sâu. Điều đó rất tốt, nhưng chúng ta cần hiểu thêm về nguyên nhân gốc rễ của nó.\nVấn đề là: chúng ta vẫn chưa biết tại sao BN lại hiểu quả như vậy. Một vài giả thiết được đưa ra trong cộng đồng học sâu và chúng ta sẽ xem xét từng cái một.\nTrước khi thảo luận tiếp, đây là những thứ chúng ta sẽ thấy:\n\nBài báo gốc [1] giả định rằng BN hiệu quả vì nó giảm đi thứ mà họ gọi là Internal covariate shift (ICS). Một bài báo gần đây [2] đã phủ định điều này.\nGiải thiết tiếp theo cho rằng BN giảm thiểu sự phụ thuộc lẫn nhau giữa các layers khi huấn luyện.\nGiả thiết từ MIT [2] nhấn mạnh ảnh hưởng của BN lên optimization landscape smoothness, khiến training dễ dàng hơn.\n\nViệc khám phá những giả thiết trên sẽ xây dựng cho bạn cách hiểu vững chắc hơn về Batch Normalization.\n\n3.2.1 Giả thiết 1: BN giảm internal covariate shift\nCho dù ảnh hưởng của BN là rất lớn, BN vẫn là điều dễ bị hiểu sai. Và điều này đa phần là do một giả thiết sai của bài báo gốc [1]:\n\n“Chúng tôi gọi đến sự thay đổi của phân phối của những nodes trong một mạng học sâu khi đang huấn luyện là Internal Covariate Shift (ICS). […] Chúng tôi đề xuất một phương pháp mới, gọi là Batch Normalization, để tiến đến giảm thiểu internal covariate shift, và hơn nữa là tăng tốc đáng kể quá trình huấn luyện DNN”. — Sergey Ioffe & Christian Szegedy (nguồn : [1])\n\nNói cách khác, BN hiệu quả vì nó giải quyết (một phần) vấn đề ICS.\nNhận định này gặp thách thức lớn bởi bài báo [2].\nĐể hiểu được lý do gì lại dẫn đến sự rắc rối này, chúng ta sẽ thảo luận xem covariate shift là gì, và nó bị ảnh hưởng thế nào từ normalization.\n\n3.2.1.1 Covariate shift là gì?\nTác giả của [1] định nghĩa: covariate shift - ở góc nhìn của sự ổn định phân phối - là sự di chuyển của phân phối dữ liệu dầu vào của mô hình. Mở rộng hơn, internal covariate shift mô tả hiện tượng trên khi nó xảy ra giữa các hidden layers (lớp ẩn) của một mạng học sâu.\nHãy xem tại sao đây là một vấn đè thoong qua ví dụ sau.\nGải sử chúng ta muốn huấn luyện một mô hình phân loại để trả lời câu hỏi: Đây có phải chiếc xe hơi không? Nếu chúng ta muốn trích xuất toàn bộ hình ảnh xe hơi trong một tập dữ liệu cực lớn, mô hình này giúp ta tiếp kiệm rất nhiều thời gian.\nChúng ta sẽ dùng hình RGB làm đầu vào, sau đó là vài lớp CNN, và vài lớp fully connected. Output sẽ là một giá trị duy nhất, đưa vào một làm logistic để cho ra giá trị từ 0 đến 1 - mô tả xác xuất mà hình input có chứa xe hơi.\n\n\n\n\nMột mô hình phân loại CNN đơn giản | Nguồn : Tác gỉa - Thiết kế: Lou HD\n\nBây giờ, hãy xem chúng ta chỉ có xe “bình thường” để huấn luyện. Thế thì mô hình sẽ hoạt động như thế nào nếu chúng ta muốn nó phân loại một chiếc xe công thức 1?\n\n\n\n\nNhư đã nói ở trên, covariate shift khiến giá trị kích hoạt bị phân tán. Ngay cả khi nó không làm vậy, nó cũng làm giảm hiệu năng của mô hình | Nguồn : Dhiva Krishna (Trái), Ferhat Deniz Fors (Phải)\n\nTrong ví dụ này, có sự khác biệt giữa phân phối của dữ liệu huấn luyện vs đánh giả. Nói rộng hơn, sự thay đổi về hướng xe, ánh sáng, điều kiện thời tiết cũng đủ ảnh hướng đến hiệu năng của mô hình. Ở đây, mô hình của chúng ta không tổng quát đủ tốt.\nNếu chúng ta plot những đặc trưng được trính xuất ra từ không gian đặc trưng, chúng ta sẽ có hình giống như sau:\n\n\n\n\nHình 6.a: Tại sao chúng ta cần chuẩn hoá giá trị đầu vào của mô hình? Trường hợp không chuẩn hoá: Khi huấn luyện, giá trị inputs nằm xa nhau: hàm số xấp xỉ sẽ rất chính xác khi các điểm nằm cận nhau. Ngược lại, hàm này sẽ thiếu chính xác và bị ngẫu nhiêu khi mật độ dữ liệu thấp | Nguồn : Nguồn : Tác giả - thiết kế bởi: Lou HD\n\nGỉa định rằng ký tự X tương ứng với hình ảnh không xe hơi, và O là hình ảnh có xe hơi. Ở đây, chúng ta có một hàm để chia giữa hai loại ảnh. Nhưng hàm này sẽ có độ chính xác thấp hơn ở phần trên cùng bên phải của đồ thị vì không có đủ dữ liệu để xác định được hàm tốt hơn. Điều này có thể dẫn đến độ chính xác thấp hơn trong quá trình đánh giá.\nĐể huấn luyện mô hình hiệu quả hơn, chúng ta cần nhiều hình ảnh có xe hơi, với tất cả các điều kiện có thể tưởng tượng ra. Tuy rằng đây là chúng ta huấn luyện CNN, chúng ta cũng muốn rằng mô hình sẽ tổng quát hoá tốt chỉ với ít dữ liệu nhất có thể.\n\nTừ góc nhìn của mô hình, hình ảnh khi huấn luyện - về mặt thống kê - thì quá khác biệt với hình ảnh khi testing. Tức là có covariate shift\n\nCó thể giải quyết vấn đề này bằng những mô hình đơn giản hơn. Những mô hình logistic regression thường dễ tối ưu hơn khi giá trị input được chuẩn hoá (có phân phối gần với (𝜇 = 0, σ = 1)); Đây là lý mo mà dữ liệu đầu vào thường được chuẩn hoá.\n\n\n\n\nHình 6.b: Tại sao chúng ta cần chuẩn hoá giá trị đầu vào của mô hình? Trường hợp có chuẩn hoá: Các dữ liệu được kéo gần lại hơn trong không gian đặc trưng -> Dễ tìm kiếm hàm tổng quát tốt hơn | Nguồn : Tác giả - thiết kế bởi: Lou HD\n\nGiải pháp này được biết đến rộng rãi ngay cả trước khi bài báo về BN được đăng. Với BN, nhóm tác giả của [1] muốn mở rộng phương pháp này đến với những lớp ẩn để cải thiện quá trình huấn luyện.\n\n\n3.2.1.2 Giải thiết của bài báo gốc: Internal covariate shift phá hỏng quá trình huấn luyện\n\n\n\n\nHình 7: Nguyên lý Internal covariate shift (ICS) trong góc nhìn về sự ổn định phân phối | Nguồn : Tác giả - thiết kế bởi: Lou HD\n\nTrong bài toán phân loại ở trên về xe hơi, có thể xem những lớp ẩn như những phần từ được kích hoạt khi nó phát hiện ra những đặc trưng liên quan đến xe hơi: như là bánh xe, lốp hoặc cửa xe. Chúng ta có thể giả định rằng những cái hiệu ứng nói đến ở trên có thể xảy ra giữa những lớp ẩn. Một cái bánh xe với một hướng xoay nào đó sẽ kích hoạt nơ-ron liên quan đến phân phối đó. Trong trường hợp lý tưởng, chúng ta muốn những một vài nơ-ron phản ứng với những phân phối của bánh xe ở bất cứ hướng xoay nào, để mô hình có thể xác định được xác suất ảnh input có xe hay không hiệu quả hơn.\nNếu dữ liệu input có covariate shift lớn, optimizer sẽ gặp vấn đề khi tổng quát dữ liệu. Ngược lại, nếu tín hiệu đầu vào luôn tuân theo phân phối chuẩn, optimzer sẽ dễ dàng tổng quát hơn. Với những điều này, tác giả của [1] đã áp dụng chiến thật chuẩn hoá dữ liệu trong lớp ẩn. Họ giả định rằng ép (𝜇 = 0, σ = 1) vào phân phối của tín hiệu trung gian sẽ tổng quát tốt hơn ở tầng “khái niệm” của những đặc trưng.\nTuy nhiên, chúng ta không phải lúc nào cũng muốn phân phối chuẩn trong những lớp ẩn. Vì nó có thể làm giảm tính biểu thị của mô hình:\n\n\n\n\nHình 7: Lý do chúng ta không phải lúc nào cũng cần phân phối chuẩn cho lớp ẩn. Trong trường hợp này, hàm sigmoid sẽ chỉ hoạt được được phần linear của nó | Nguồn : Tác giả - thiết kế bởi: Lou HD\n\nVấn đề này được tác giả của [1] giải quyết bằng cách thêm 2 tham số huấn luyện là 𝛽 and 𝛾, giúp optimizer có thể chọn được trung bình tối ưu (dùng 𝛽) và độ lệch chuẩn tối ưu (dùng 𝛾) cho tác vụ nhất định.\n\nCảnh báo: Những giả thiết sau đã lỗi thời. Tuy vậy, vẫn nhiều nội dung hay về BN vẫn sử dụng những giả thiết đó là lý do BN hiệu quả. Hiện nay đã có nhiều công trình thách thức những giả thiết ban đầu.\n\nTrong vài năm sau khi [1] phát hành, cộng đồng học sâu giải thích tính hiệu quả của BN như sau:\nGiả thiết 1\n\nBN -> Chuẩn hoá tín hiệu của lớp ẩn -> Thêm hai tham số huấn luyện để thay đổi phân phối và tận dụng phi tuyến tính -> Huấn luyện dễ hơn\n\nTại đây, chuẩn hoá (𝜇 = 0, σ = 1) được sử dụng để giải thích tại sao BN hiệu quả. Giả thiết này đã bị thách thức (đọc ở phần sau) và được thay thế bởi giả thiết tiếp heo:\nGiả thiết 2\n\nBN -> Chuẩn hoá tín hiệu của lớp ẩn -> Giảm sự phụ thuộc lẫn nhau giữa các lớp ẩn (theo góc nhìn về sự ổn định của phân phối) -> Huấn luyện dễ hơn\n\nSự khác biệt với giả thiết 1 là nhỏ nhưng lại rất quan trọng. Ở đây, mục tiêu của chuẩn hoá là để giảm đi sự phụ thuộc lẫn nhau giữa các lớp -> optimizer có thể chọn được phân phối tối ưu bằng cách thay đổi hai tham số. Hãy xem xét giả thiết này kỹ hơn.\n\n\n\n3.2.2 Giả thiết - BN giảm sự phụ thuộc lẫn nhau trong các lớp ẩn khi huấn luyện.\nVề phần này: tác giả không thể tìm thấy bằng chứng thuyết phục về giả thiết này. Thế nên, tác giả sẽ dựa vào giải thích của Ian Goodfellow: YouTube\nHãy xem xét ví dụ sau:\n\n\n\n\nHình 9: Một DNN đơn giản, chỉ bao gồm những biến đổi tuyến tính | Lấy cảm hứng từ Ian Goodfellow\n\nTrong đó (a), (b), (c), (d), (e) là những lớp tuần tự trong DNN. Đây là một ví dụ rất hơn giản, chỉ bao gồm các lớp được liên kết với nhau bằng biến đổi tuyến tính. Giả sử chúng ta muốn huấn luyện mô hình trên bằng SGD.\nĐể cập nhật trọng số của lớp (a), chúng ta cần tính giá trị đạo hàm từ output của network như sau:\n\n\n\nHãy xem xét một network không có BNN. Từ phương trình ở trên, chúng ta kết luận rằng nếu tất cả giá trị đạo hàm (gradient) lớn thì đạo hàm của a (grad(a)) sẽ rất lớn. Ngược lại, nếu tất cả gradient đều nhỏ thì grad(a) gần như bằng 0.\nDễ dàng thấy rằng các lớp phụ thuộc vào nhau như thế nào bằng cách nhìn vào phân phối tín hiệu đầu vào của những lớp ẩn: một sự thay đổi trong trọng số của (a) sẽ dẫn đến thay đổi trọng số của lớp (b) và từ từ đến (d) và cuối cùng là (e). Sự phụ thuộc lẫn nhau này gây ra vấn đề với độ ổn định khi huấn luyện: Nếu ta muốn thay đổi phân phối đầu vào của một lớp ẩn nào đó, nó sẽ dẫn đến sự thay đổi của những lớp theo sau.\nTuy nhiên, SGD chỉ quan tâm đến mối liên hệ bậc 1 giữa các lớp. Nên, nó không bao quát được những những mối quan hệ bậc cao hơn nói ở trên.\n\n\n\n\nHình 10: BN điều hoá dòng chảy của tín hiệu, bằng cách chuẩn hoá tín hiệu trong mỗi hidden unit, và cho phép điều chỉnh phân phối với và 𝛾. BN như là một cái van khiến việc điều khiển dòng chảy dễ dàng hơn ở vài chỗ mà không làm giảm khả năng phức tạp của mô hình. Lou HD\n\nThêm lớp BN giúp giảm sự phụ thuộc lẫn nhau giữa các lớp (theo cách nhìn về sự ổn định phân phối) trong quá trình huấn luyện. BN như một cái van nước giúp giảm lại dòng chảy, với hai tham số 𝛽 và 𝛾. Mà vì thế nên không cần xem xét tất cả tham số để hiểu về phân phối trong các lớp ẩn.\n\nLưu ý: Do có BN, optimizer có thể thay đổi trọng số mạnh hơn mà không làm suy thoái các tham số đã được điều chỉnh trước đó của lớp ẩn khác. Điều này khiến việc điều chỉnh các siêu tham số (hyperparameter) dễ hơn rất nhiều!\n\nVí dụ này bỏ qua giả thuyết cho rằng hiệu quả của BN là do sự chuẩn hóa của phân bố tín hiệu giữa các lớp (μ = 0, σ = 1). Ở đây, BN có mục đích làm cho việc tối ưu hóa tác vụ dễ hơn, cho phép nó điều chỉnh phân bố lớp ẩn với chỉ hai tham số một lúc.\n\nTuy nhiên, hãy nhớ rằng điều này chủ yếu là suy đoán thôi. Những thảo luận này nên được sử dụng như các kiến thức để xây dựng sự hiểu biết về BN. Chúng ta vẫn không biết chính xác tại sao BN hiệu quả trong thực tế!\n\nNăm 2019, một nhóm nghiên cứu từ MIT thực hiện vài thí nghiệm thú vị về BN [2]. Kết quả của họ đe dọa lớn giả thuyết 1 (vẫn được chia sẻ bởi nhiều bài viết blog và MOOCs!).\nChúng ta nên xem qua tài liệu này nếu muốn tránh “giả thuyết điểm tối thiểu địa phương” về tác động của BN trên huấn luyện… ;)\n\n\n\n\nĐược rồi… bạn nên khởi tạo tốt hơn.\n\n\n\n3.2.3 Giả thuyết 3 - BN làm cho không gian tối ưu hóa trở nên mượt hơn\nVề phần này: Tác giả đã tóm tắt kết quả từ [2] mà có thể giúp chúng ta xây dựng cách hiểu tốt hơn về BN. Tác giả không thể tóm tắt đầy đủ, tài liệu này rất nhiều, tác giả đề nghị bạn đọc kỹ nếu bạn quan tâm đến những khái niệm đó.\nHãy đi tới thí nghiệm thứ 2 của [2]. Mục tiêu của họ là kiểm tra sự tương quan giữa ICS và lợi ích của BN trên hiệu suất huấn luyện (giả thuyết 1).\nKhái niệm: Chúng ta sẽ gọi covariate shift này là ICS_distrib.\nĐể làm điều đó, nhà nghiên cứu đã huấn luyện ba mạng VGG (trên CIFAR-10):\n\nMạng thứ nhất không có bất kỳ lớp BN nào;\nMạng thứ hai có các lớp BN;\nMạng thứ ba tương tự như mạng thứ hai, ngoại trừ việc họ đã thêm một số ICS_distrib trong đơn vị ẩn trước khi kích hoạt (bằng cách thêm khoảng sai và biến thể ngẫu nhiên).\n\nHọ đo lường độ chính xác đạt được bởi mỗi mô hình và sự thay đổi của giá trị phân bố theo số lần lặp. Đây là kết quả mà họ đã nhận được:\n\n\n\n\nBN với ICS_distrib | Các mạng với BN được huấn luyện nhanh hơn so với mạng tiêu chuẩn; thêm rõ ràng ICS_distrib trên một mạng được kiểm soát không làm giảm lợi ích của BN. | Nguồn: [2]\n\nChúng ta có thể thấy rằng mạng thứ ba có một ICS rất cao (như dự đoán). Tuy nhiên, mạng bị nhiễu vẫn được huấn luyện nhanh hơn so với mạng tiêu chuẩn. Kết quả đạt được của nó tương đương với kết quả được đạt được với một mạng BN tiêu chuẩn. Kết quả này cho thấy rằng hiệu quả của BN không liên quan đến ICS_distrib.\nChúng ta không nên loại bỏ giả thuyết ICS quá vội: nếu hiệu quả của BN không xuất phát từ ICS_distrib, nó có thể liên quan đến một định nghĩa khác của ICS. Sau cùng, sự giả thuyết số 1 cũng có vẻ đúng, phải không?\nVấn đề chính với ICS_distrib là định nghĩa của nó liên quan đến phân bố đầu vào của các đơn vị ẩn. Vì vậy, không có liên kết trực tiếp với vấn đề tối ưu hóa của nó.\nTác giả của [2] đã đưa ra một định nghĩa khác của ICS:\nHãy xem xét một đầu vào X cố định.\n\nChúng ta định nghĩa ICS, từ một góc nhìn của tối ưu hóa, là sự khác biệt giữa đạo hàm tính toán trên lớp ẩn k sau khi phục hồi lỗi \\(L(X)_{it}\\) và đạo hàm tính toán trên cùng một lớp k từ mất mát \\(L(X)_{it+1}\\) sau lần lặp thứ \\(it\\)\n\nĐịnh nghĩa này nhằm tập trung vào các giá trị đạo hàm hơn là trên phân bố đầu vào của lớp ẩn, giả sử rằng nó có thể cho chúng ta các gợi ý tốt hơn về cách ICS có thể có ảnh hưởng đến vấn đề tối ưu hóa sâu bên trong.\nKý hiệu: ICS_opti bây giờ đề cập đến ICS được định nghĩa từ một góc độ tối ưu hóa.\nTrong thí nghiệm tiếp theo, tác giả đánh giá tác động của ICS_opti trên hiệu quả huấn luyện. Để làm như vậy, họ đo lường sự biến đổi của ICS_opti trong quá trình huấn luyện cho một DNN với có và không có BN layers. Để định lượng sự biến đổi của đạo hàm được nêu trong định nghĩa ICS_opti, họ tính toán:\n\nSự khác biệt L2: có đạo hàm có norm gần nhau trước và sau khi cập nhật trọng số không? Lý tưởng: 0\nGóc Cosine: có đạo hàm có hướng gần nhau trước và sau khi cập nhật trọng số không? Lý tưởng: 1\n\n\n\n\n\nẢnh hưởng của BN lên ICS_opti | Khoảng cách L2 và góc cosine gợi ý rằng BN không tránh được ICS_opti (ngược lại, nó còn làm gia tăng điều đó) | Nguồn: [2]\n\nKết quả lại chút bất ngờ: mạng sử dụng BN có vẻ có ICS_opti cao hơn so với mạng tiêu chuẩn. Hãy nhớ rằng mạng với BN (đường xanh) được huấn luyện nhanh hơn so với mạng tiêu chuẩn (đường đỏ)!\nICS có vẻ không liên quan đến hiệu quả huấn luyện… ít nhất là cho định nghĩa ICS_opti.\nMột cách nào đó, Batch Normalization có ảnh hưởng khác trên mô hình, giúp cho việc hội tụ dễ dàng hơn.\nBây giờ, hãy xem xét cách BN ảnh hưởng đến cảnh quan tối ưu hóa (optimization landscape) để tìm theo manh mối nhé.\nTiếp theo là thí nghiệm cuối cùng được đề cập trong bài viết này:\n\n\n\n\nKhám phá cảnh quan tối ưu hoá (Optimization landscape exploration) theo hướng đạo hàm. Thí nghiệm được thực hiện trong paper [2] | Cảm hứng từ: Andrew Ilyas - thiết kế bởi: Lou HD\n\nTừ một giá trị đạo hàm, chúng ta cập nhật trọng số với các bước tối ưu hóa khác nhau (mà hoạt động như tốc độ học - learning rate). Nói một cách trực quan, chúng ta định nghĩa một hướng từ một điểm nhất định (tức một cấu hình mạng ω) trong không gian đặc trưng, sau đó khám phá thêm cảnh quan tối ưu hóa theo hướng này.\nTại mỗi bước, chúng ta đo lường đạo hàm và mất mát (loss). Chúng ta có thể so sánh các điểm khác nhau của cảnh quan tối ưu hóa với một điểm bắt đầu. Nếu chúng ta đo lường sự biến đổi lớn, cảnh quan rất không ổn định và đạo hàm không chắc chắn: các bước lớn có thể làm xấu việc tối ưu hóa. Ngược lại, nếu sự biến đổi đo được nhỏ, cảnh quan ổn định và đạo hàm đáng tin cậy: chúng ta có thể áp dụng các bước lớn hơn mà không gây hại cho tối ưu hóa. Nói cách khác, chúng ta có thể sử dụng một tốc độ học lớn hơn và làm cho việc hội tụ nhanh hơn (một tính năng được biết đến của BN).\nHãy xem kết quả:\n\n\n\n\nTác động của BN lên việc làm mượt cảnh quan tối ưu hoá (optimization landscape smoothing) | BN làm giảm thiểu đáng kế sự biến động của đạo hàm | Nguồn: [2]\nRõ thấy rằng cảnh quan tối ưu hoá mượt hơn nhiều khi dùng các lớp BN.\nCuối chùng chúng ta có kết quả để dùng để giải thích sự hiệu quả của BN: Lớp BN làm cho cảnh quan tối ưu hoá mượt hơn. Từ đó thì việc tối ưu hoá cũng dễ dàng hơn: chúng ta có thể sử dụng tốc độ học cao hơn mà không bị gradient vanishing hoặc gradient explosion.\nGiả thiết thứ 3 thì đến từ bài báo [2]:\n\n\n3.2.4 Gỉa thiết 3\nBN -> chuẩn hoá tín hiệu trong các đơn vị ẩn -> làm mượt cảnh quan tối ưu hoá -> huấn luyện nhanh và ổn định hơn.\nNó đặt ra một câu hỏi khác:** Làm sao mà BN lại làm cho cảnh quan tối ưu hoá mượt hơn?**\nTác giả của [2] đã khám những vấn đề này từ góc nhìn lý thuyệt. Nghiên cứu của họ rất có ích trong việc hiểu được hiệu ứng làm mượt của BN. Chi tiết hơn, họ chứng minh rằng BN làm cho cảnh quan tối ưu mượt hơn trong khi vẫn giữ tất cả các cực tiểu của cảnh quan thường. Nói cách khác, BN thay đổi tham số của bài toán tối ưu phía dưới, làm cho huấn luyện nhanh và dễ hơn!\nTrong những nghiên cứu bổ sung, tác giả của [2] quan sát rằng hiệu ứng này không chỉ có ở BN. Họ đạt được hiệu năng huấn luyện tương đương với phương pháp tối ưu khác như L1 hay L2. Những quan sát này gợi ý rằng sự hiệu quả của BN phần lớn đến từ sự trùng hợp, do tận dụng một cơ chế tầng dưới nào đó mà chúng ta chưa nhận dạng chính xác được.\nĐể kết thúc phần này, bài báo này thách thức cực mạnh ý tưởng rằng BN hiệu quả vì đó giảm thiểu ICS (cả trong góc nhìn về sự ổn định của phân phối và cả về tối ưu hoá). Tuy nhiên, nó lại nhấn mạnh về ảnh hưởng của sự làm mượt cảnh quan tối ưu của BN.\nTuy rằng bài báo này đưa ra giả thiết về ảnh hưởng của BN lên tốc độ huấn luyện, nhưng nó không trả lời tại sao BN lại hỗ trợ tốt cho quá trình tổng quát hoá.\nHọ có thảo luận nhanh rằng làm cho cảnh quan tối ưu hoá mượt hơn cũng giúp mô hình hội tụ ở các cực tiểu phẳng -> làm cho khả năng tổng quát hoá tốt hơn. Tuy nhiên, nhận định này vẫn cần thêm nhiều giải thích hơn.\nĐóng góp chủ yếu của tác giả là thách thức ý tưởng về sự ảnh hưởng của BN lên ICS - vậy thôi cũng đủ quan trọng rồi!"
  },
  {
    "objectID": "articles/translate-batch-normalization-in-3-levels-of-understanding.html#tổng-hợp-những-lý-do-cho-sự-hiệu-quả-của-bn-mà-chúng-ta-biết",
    "href": "articles/translate-batch-normalization-in-3-levels-of-understanding.html#tổng-hợp-những-lý-do-cho-sự-hiệu-quả-của-bn-mà-chúng-ta-biết",
    "title": "3 Cấp độ hiểu về Batch Normalization (Bài dịch)",
    "section": "3.3 Tổng hợp: Những lý do cho sự hiệu quả của BN mà chúng ta biết",
    "text": "3.3 Tổng hợp: Những lý do cho sự hiệu quả của BN mà chúng ta biết\n\nGiả thiết 1: BN làm giảm ICS -> Sai: bài báo [2] chứng minh rằng không có sự tương quan giữa ICS và hiệu năng huấn luyện trong thực tế.\nGiả thiết 2: BN làm cho optimizer tối ưu nhanh hơn vì nó thay đổi phân phối đầu vào của đơn vị ẩn chỉ bằng 2 tham số -> Có thể: Giả thiết này nhấn mạnh sự liên quan chéo giữa các tham số khiến tối ưu khó hơn. Tuy vậy, nó vẫn chưa đủ thuyết phục.\nGiả thiết 3: BN thay đổi tham số của bài toán tối ưu tầng sâu, làm nó mượt và ổn định hơn. -> Có thể: Kết quả cũng khá gần đây. Đến thời điểm của bài viết gốc thì dường như giả thuyết này chưa bị thách thức. Bài báo cũng đưa ra những thí nghiệm thực tế cũng như giải thích về lý thuyết, tuy vẫn chưa trả lời một số câu hỏi nền tảng như “tại sao BN lại hỗ trợ tổng quát hoá?”\n\nThảo luận: Đối với tác giả của bài viết này, hai giả thiết cuối có vè tương thích. Về trực quan thì chúng ta thấy giả thiết 2 như một phép chiếu từ bài toán nhiều tham số thành bài toán ít tham số hơn; kiểu như bài toán giảm chiều dữ liệu, điều có thể hỗ trợ cho tổng quát hoá. Bạn nghĩ gì về điều này?\nVẫn còn nhiều những câu hỏi mở và BN vẫn là một chủ đề nghiên cứu ngày nay. Thảo luận những giả thiết này vẫn giúp ta hiểu hơn về phương pháp thường dùng này, và bỏ qua những nhận định sai lầm trong những năm gần đây.\nTuy vậy, những câu hỏi này không thể ngăn cản chúng ta tận dụng lợi ích mà BN đem lại trong thực tế!"
  },
  {
    "objectID": "articles/translate-batch-normalization-in-3-levels-of-understanding.html#câu-hỏi-mở",
    "href": "articles/translate-batch-normalization-in-3-levels-of-understanding.html#câu-hỏi-mở",
    "title": "3 Cấp độ hiểu về Batch Normalization (Bài dịch)",
    "section": "4.1 Câu hỏi mở",
    "text": "4.1 Câu hỏi mở\nDù rằng BN rất hiểu qua trong thực nghiệm, vẫn còn tồn tại nhiều câu hỏi về cách thức hoạt động phía sau của nó mà chưa có lời giải.\nSau là danh sách (không đầy đủ):\n\nTại sao BN lại hỗ trợ quá trình tổng quát hoá?\nBN có phải là phương pháp chuẩn hoá tốt nhất cho tối ưu hoá?\n𝛽 và 𝛾 ảnh hưởng như thế nào đến sự mượt của cảnh quan tối ưu?\nThực nghiệm trong bài [2] về cảnh quan tối ưu hoá tập trung về tác động ngắn hạn của BN lên đạo hàm: họ đo lường sự biến đổi của gradient và loss trong một lần lặp duy nhất, với số lượng bước khác nhau. Vậy BN sẽ ảnh hưởng lên gradient trong dài hạn như thế nào? Liệu sự phụ thuộc lẫn nhau giữa các trọng số có ảnh hưởng nào đến cảnh quan tối ưu hoá?"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Simple MLP model for Guitar Steel vs Nylon strings classification\n\n\n\n\n\n\n\ncode\n\n\nclassification\n\n\nmlp\n\n\ncnn\n\n\naudio\n\n\nprocessing\n\n\nsignal\n\n\nfft\n\n\n\n\n\n\n\n\n\n\n\nNov 26, 2022\n\n\nLuka Nguyen\n\n\n\n\n\n\n  \n\n\n\n\nVietnamese Lyrics Classification\n\n\n\n\n\n\n\ncode\n\n\nclassification\n\n\ndecision tree\n\n\n\n\n\n\n\n\n\n\n\nOct 21, 2022\n\n\nLuka Nguyen\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Luka Blog",
    "section": "",
    "text": "Simple MLP model for Guitar Steel vs Nylon strings classification\n\n\n\n\n\n\n\ncode\n\n\nclassification\n\n\nmlp\n\n\ncnn\n\n\naudio\n\n\nprocessing\n\n\nsignal\n\n\nfft\n\n\n\n\n\n\n\n\n\n\n\nNov 26, 2022\n\n\nLuka Nguyen\n\n\n\n\n\n\n  \n\n\n\n\nVietnamese Lyrics Classification\n\n\n\n\n\n\n\ncode\n\n\nclassification\n\n\ndecision tree\n\n\n\n\n\n\n\n\n\n\n\nOct 21, 2022\n\n\nLuka Nguyen\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About me",
    "section": "",
    "text": "I’m passionate about Artificial Intelligence and Music."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About me",
    "section": "Education",
    "text": "Education\nUniversity of Science, Ho Chi Minh | Vietnam BSc in Computer Science | Sept 2011 - June 2015"
  },
  {
    "objectID": "articles.html",
    "href": "articles.html",
    "title": "Articles",
    "section": "",
    "text": "3 Cấp độ hiểu về Batch Normalization (Bài dịch)\n\n\n\n\n\n\n\ntranslate\n\n\ncnn\n\n\nbatch norm\n\n\nvietnamese\n\n\n\n\n\n\n\n\n\n\n\nJan 21, 2023\n\n\nJohann Huber\n\n\n\n\n\n\nNo matching items"
  }
]